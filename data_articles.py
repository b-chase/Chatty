article1 = """
Addressing homelessness with data analytics

While traditional approaches have helped policymakers reduce homelessness to an extent, technology and data analytics 
can help them go a step further in dealing with the country’s homelessness problem.

EVERY year in January, an army of volunteers braves the cold winter night to count America’s unsheltered homeless 
population. These volunteers fan out across the nation, looking for people in parks, under bridges and highway 
overpasses, in train stations, and any other places with possible encampments. This mammoth exercise is the federal 
Department of Housing and Urban Development’s (HUD’s) annual “Point-in-Time” homeless count, the largest attempt to 
quantify homelessness in America.1

Learn more
Read the related article, The homelessness paradox

Read more about smart cities

Subscribe to receive related content from Deloitte Insights

Download the Deloitte Insights and Dow Jones app

Although the annual homeless count has been criticized for possibly underestimating the scale of homelessness and 
disregarding large numbers of Americans on the verge of homelessness,2 the HUD data can be useful for analyzing 
trends. It reveals three key insights: First, despite a strong economy, homelessness inched up in 2017 and 2018 (see 
figure 1), reversing a previous downward trend. Second, homelessness in the United States primarily affects urban 
communities, with about 82 percent of the homeless population in urban areas. Finally, homelessness isn’t distributed 
evenly (see figure 2): The Los Angeles and New York Continuums of Care (CoCs), for example, accounted for 23 percent 
of all homeless people in the nation in 2018.3 (A CoC is a local or regional planning body organized under HUD.)

The good news is that technological advances and data insights are giving policy leaders new tools in the fight 
against homelessness. Traditionally, entities combatting homelessness—governments, nonprofits, and community service 
organizations—have applied a variety of policy and economic approaches to address homelessness. These tools continue 
to be important, but today, their impact can be magnified by combining them with digital and data analytics tools.

This article focuses on the power of data analytics and digital technologies that can tackle homelessness, 
particularly in urban areas. Thanks to these technologies, we can target resources for the homeless more effectively 
and can use predictive analytics to better assist those at risk of becoming homeless.

The share of homeless as a proportion of the urban population has increased in the last two years
New York and California have the highest concentration of homeless

Traditional and emerging tools to address homelessness
Homelessness can be a result of wage stagnation, mental illness, lack of affordable housing, various forms of 
discrimination, lack of economic opportunities, addiction, and other such factors that have been explained in our 
article The homelessness paradox: Why do advanced economies still have people who live on the streets? Tackling 
homelessness in its entirety requires taking into account the three stages of homelessness:

Stage 1: People at risk of homelessness.
Stage 2: People who are currently homeless.
Stage 3: People who have found a home but may need help to keep it.
Most efforts to fight homelessness are focused on stage 2, given its urgency, but the other stages also should be 
addressed. Two sets of tools can help government leaders, policymakers, and social service agencies address 
homelessness at each of these stages:

Policy and economic tools. These include traditional approaches to reduce the number of people experiencing 
homelessness. For example, eviction prevention grants and other legal and support services are offered to prevent 
homelessness in the first place. The “housing first” strategy, which has garnered bipartisan support, is designed to 
focus on those at risk (stage 1) as well as those who are already homeless (stage 2). (See the sidebar “‘Housing 
First’ and other policy-driven preventive mechanisms.”) A variety of job training and substance treatment programs 
can help to prevent a return to homelessness (stage 3).

Digital and data analytics tools. These use technology to help homelessness-focused agencies become more effective on 
the ground (figure 3). This toolkit can amplify and refine traditional approaches. For example, a case management 
system that uses data analytics and digital solutions to track, monitor, and support people across all three stages 
of homelessness can be a game-changer for urban communities.

Several tools can address homelessness at different stages

“Housing first” and other policy-driven preventive mechanisms
The roots of the “housing first” strategy can be traced to the city of New York’s right to shelter mandate of the 
1980s. This mandate required the city, which has the nation’s second-highest homeless population, to provide 
temporary emergency shelter for every eligible constituent every night. Since then, the city has followed the mandate 
through a patchwork of options including private apartments and “cluster rooms” in hotels. Many states have adopted 
similar programs, producing a rising number of permanent housing options across the country.4

A housing first strategy can be implemented in various ways:

Permanent supportive housing (PSH) provides affordable housing and support services to individuals experiencing 
mental illness, HIV/AIDS, and other serious health issues. In 2017, for example, Los Angeles County used PSH to 
permanently house more than 16,000 people—more than any other jurisdiction in the nation in that year.5

Rapid rehousing, a relatively new model, focuses on helping people find a home as quickly as possible, thereby giving 
them space to tackle other challenges such as employment, education, and health care. Rapid rehousing thus shifts the 
initial focus away from preconditions such as income, addiction, or a criminal record and toward first providing 
housing. Y-Foundation, a Finnish organization, used this approach to provide more than 6,000 homes for the 
chronically homeless and 10,000 homes for low-income individuals and families. Its efforts helped cut chronic or 
long-term homelessness in the nation by 35 percent from 2008 to 2016.6

In addition to “housing first,” two other noteworthy measures can aid people at risk of homelessness. 
Eviction-prevention grants provide financial assistance to persons struggling with rental arrears due to a sudden 
event such as illness, death in the family, or unemployment. Legal assistance for low-income tenants in housing 
courts reduces the asymmetry in access to legal services between low-income tenants and landlords.

Exploring the digital and data analytics toolkit
Technology can enhance coordination among different players, including governments, the private sector, 
social enterprises, community organizations, and citizens, supplementing traditional approaches to reducing 
homelessness.

Let’s see how an integrated care and case management system can provide a starting point for homelessness-focused 
agencies, and how a coordinated entry system (CES) can integrate data from multiple sources, both internal and 
external, to help them develop more effective solutions (figure 4).

The digital, data, and analytics toolkit can address a range of homelessness-related issues

Homelessness care and case management: Coordinated data entry for homelessness
The Homeless Emergency Assistance and Rapid Transition to Housing (HEARTH) Act was an important piece of legislation 
passed in 2009. With the homelessness population growing in the wake of the 2008 economic crisis, it was important to 
take a focused approach to end homelessness. A year later, in 2010, the White House and the US Interagency Council on 
Homelessness (USICH) released a strategic plan to prevent and end homelessness called Opening Doors.7 The plan 
focused on ending homelessness for four specific cohorts: families with children, youth, veterans, 
and the chronically homeless.

With the passing of the HEARTH Act, HUD mandated a CES for all CoCs.8 This was an important shift toward collecting 
and managing homelessness data across the United States. Before the mandate, each CoC was collecting homelessness 
data in its own customized homelessness management information systems (HMIS).

The emergence of the CES has helped address one of the most critical issues faced by many urban communities across 
the United States: coordinating support services across the three stages of homelessness. The integration of 
homelessness data from entry to exit and beyond provides CoCs with a broader view of the problem, providing the data 
needed to drive insights and decision-making. More importantly, it has helped refocus resources on families with 
children, youth, veterans, and the chronically homeless. These systems enable CoCs to achieve three things:

Increase customer focus. Homelessness-focused agencies can now focus on the “person” coming into the system. A 
streamlined entry process focuses on understanding the client needs and determining the right program for them, 
whether a supplemental nutrition assistance program or a mental health services program.

Prioritize vulnerable clients. The system focuses on understanding the vulnerability of the person coming in. Through 
a series of questions focusing on the history of housing and homelessness, health risks, physical abuse history, 
legal issues, financial history, and social relationships, a homelessness agency can understand the severity of the 
situation. Thus, an agency can target the right population at the right time with the right services.

Build a data-driven system. A CES can help integrate data across the homelessness life cycle. This data can provide a 
starting point for understanding the state of homelessness and, more importantly, drive data-driven decision-making.

Aggregate and visualize data: Integrating homelessness data to drive insights
Is there a dearth of data on homelessness? No. HUD collects annual point-in-time data and housing inventory count 
data for all CoCs across the United States. A separate HMIS contains data collected by each CoC. The National Center 
for Homeless Education also maintains a database on the education of homeless youth. Data is collected at the state 
and local levels too. This data, however, often sits in jurisdictional silos, making it hard to use it to see the 
whole picture.

However, some attempts have been made to make sense of the reams of data that are available and use them 
meaningfully. The Los Angeles Homeless Services Authority (LAHSA), for instance, has collected and visualized 
homelessness data on different cohorts of the homeless population, including adults, families, veterans, and youths. 
Its winter shelter program dashboard provides information on average occupancy at each winter shelter in the city and 
county of Los Angeles.9 These dashboards can help LAHSA identify shelter vacancies and allow community service 
organizations to match people experiencing homelessness with available shelter space.

Similarly, the National Alliance to End Homelessness has a “State of Homelessness” portal that aggregates and 
visualizes homelessness data drawn from across the nation.10 In addition to tabulating the number of people 
experiencing homelessness, the portal can be used to examine sheltered versus unsheltered populations, homelessness 
by race and ethnicity, the availability of beds in each state, and trends in permanent housing solutions. Such open 
data portals can help gauge the success of various initiatives and highlight communities that are reducing 
homelessness. These insights can help refocus resources to appropriate geographic regions and cohorts, both locally 
and nationally. However, local agencies haven’t yet been able to integrate data in this way.

GIS analytics: Using location-based technology to locate homeless populations
Location-based capabilities represent a natural next step toward enhancing the value of homelessness data. They can 
help governments answer critical questions: Where are large pockets of homeless located? Where is the housing stock 
available? Which areas should caseworkers prioritize?

Identifying an ever-moving homeless population can be difficult. Mobile and GIS technologies, however, can collate 
real-time location data from caseworkers and citizens. LAHSA, for instance, has launched an online Los Angeles County 
Homeless Outreach Portal that allows community members to report on homeless persons who need help. When a resident 
sees homeless persons on the street, he or she can report their location on a map, add other details, and send a 
request to the homelessness outreach team. Based on this data, the outreach team can try to connect the person 
experiencing homelessness with different services.11

Supply-demand optimization: Using data to match people to affordable homes
Many people in the United States become homeless due to a lack of affordable housing or through eviction.12 Other 
nations receive a steady flow of refugees from war-torn and unstable countries. Many of these migrants are employable 
and can find jobs, but need help finding affordable homes. This group can be prevented from falling into homelessness 
if they are helped with finding affordable homes quickly.

Canada, for instance, opened its borders to 25,000 Syrian refugees between December 2015 and March 2016.13 WoodGreen 
Community Services, a Toronto social service agency, launched the Housing Opportunities & Marketplace Exchange (HOME) 
online portal to help these migrants find homes in the greater Toronto area.

The HOME portal is a user-friendly website that allows individuals and businesses to list houses and other services 
on the website. In a few simple steps, migrants can register on the portal, search for listings, quickly locate the 
homes or services they require on a map, and contact donors or landlords directly. Since its January 2016 launch, 
the portal has helped at least 149 families find homes at rental rates 5–15 percent lower than the market rate. The 
portal also helped the government save more than US$8.63 million annually on subsidized housing. Based on its 
success, the HOME portal was expanded to include the city’s homeless population.14

Digital tools for caseworkers: Making data-driven decisions
Data insights also can help government agencies and outreach workers make their operations more efficient. For 
instance, one of the most important outcomes of building a CES for homelessness is the ability to prioritize clients 
based on their vulnerability.

Moreover, such data systems are available to outreach workers in the field. For instance, San Francisco’s online 
navigation entry system allows outreach workers to access homelessness data via a mobile app.15 The app also helps 
workers locate, assess, and connect the homeless with the right services. A similar tool in the West Sacramento 
Outreach Grid provides outreach workers with locations of homeless camps and provides access to homelessness data to 
help them make decisions on the spot.16

Predictive risk modeling: Helping those at risk of homelessness
The idea that the government should focus more on preventing problems instead of fixing them isn’t new, of course.17 
Today, however, advances in data analytics and artificial intelligence (AI) technologies such as machine learning 
have exponentially improved our ability to crunch massive amounts of data to identify patterns and spot potential 
problems.

Multiple ongoing pilots use predictive analytics to assess the social vulnerability of people and identify those at a 
high risk of homelessness. The Los Angeles–based Economic Roundtable is experimenting with a predictive model to 
identify people prone to falling into chronic homelessness.18 New York University’s Center for Urban Science + 
Progress is collaborating with Women in Need to build predictive models to provide better services to the homeless 
population.19 These predictive models can help prevent homelessness (stage 1) as well as relapse (stage 3).

Behavioral nudges: Using data to design better nudges
Behavioral science, or “nudge thinking,” attempts to influence the choices people make. In the case of homelessness, 
this may mean encouraging better choices concerning health care, employment, and substance abuse. Nudges have been 
developed to help patients take their prescriptions, remember their upcoming appointments, make healthier lifestyle 
choices, and improve their adherence to mandatory training under unemployment insurance. All these are important 
steps toward preventing homelessness and continually improving quality of life. These nudges can be used in stages 1 
and 3 of homelessness to help prevent or avoid a relapse into homelessness.

Nudges also have been used to improve citizen participation. For instance, the city of Denver has installed donation 
parking meters that allow citizens to donate change to worthy causes. Using nudge thinking, the parking meters were 
painted red and placed at strategic locations to maximize donations. These were routed to community programs 
providing affordable housing, job training, food programs, and others.20 The city of Pasadena has installed similar 
meters to collect money for organizations serving the homeless population.21

Coordinating the homelessness ecosystem: Building coalitions across federal, state, and local levels
Homelessness is not, and should not, be the government’s sole responsibility. A broader ecosystem with multiple 
stakeholders—nonprofits, the private sector, and citizen activists—is needed to solve the problem.22 Data and digital 
technology tools can help all those working against homelessness. Consider the following examples:

Supporting data-driven decisions. Through its Built for Zero partnership with Tableau Software, Community Solutions, 
a New York–based nonprofit, helps communities collect and visualize real-time data on homelessness and housing stock 
to make decisions. Since its launch in January 2015, the initiative has helped house more than 95,000 people, 
including 65,000 veterans.23

Finding the right home. Zillow’s Community Pillar program helps people with bad credit and financial history find 
homes through landlords who are willing to relax some screening criteria such as credit history, unemployment, 
and lack of references.24

Connecting migrants to housing. Airbnb’s Open Homes initiative provides “no-charge” listing to landlords to help 
people in need of temporary housing.25 These listings, targeting refugees and disaster evacuees, are coordinated 
through local nonprofits. More than 6,000 homes are listed under the program across the United States and Europe; it 
has helped house 25,000 people since its launch.

Enabling citizen participation. The WeCount app in Seattle allows people experiencing homelessness to send requests 
for clothing, outdoor gear, and personal-care items. This peer-to-peer platform helps donors find the requests and 
donate items at predesignated drop-off sites.26

The fight against ending veteran homelessness in the United States is a good example of an ecosystem approach at 
work. The Opening Doors plan released by the White House and the US Interagency Council on Homelessness has helped 
reduce veteran homelessness in the United States by nearly half.27 The program’s success is due primarily to its 
formation of a broad ecosystem including CoCs, government agencies, community leaders, nonprofit organizations, 
and Veterans Affairs medical centers. So far, the program has ended veteran homelessness in three states and 71 
communities.

Powering the homelessness fight with data
The homelessness issue requires a multipronged approach that assembles policy, economic, structural, 
and technological solutions. Traditional policy approaches can be augmented with data and analytics tools. It all 
likely begins, however, with a robust CES that collects the “right” data concerning homelessness. A range of 
analytics tools can be built upon this data to improve our ability to examine the problem, improve decision-making, 
design nudges, and build predictive models to help people experiencing homelessness, and, in some cases, prevent it 
in the first place.

A supporting ecosystem can increase our chances for success—and that supporting ecosystem can be more powerful when 
it has access to the data needed to make good decisions. Ending homelessness is an audacious goal, and that journey 
begins with data.
"""

article2 = """
Analytics and AI-driven enterprises thrive in the Age of With

More than a decade after the concept of big data became part of the lexicon, only a minority of companies have become 
insight-driven organizations—and culture may be the culprit.

THE amount of data available to organizations every day continues to proliferate at a staggering volume. But 
technologies such as analytics and artificial intelligence (AI) have the potential to help businesses make better use 
of these massive volumes of data. In an age of collaboration between humans and machines—what we call the "Age of 
With"1—organizations can gain advantage by designing systems in which humans and machines work together to improve 
the speed and quality of decision-making.

Learn more
Explore the Analytics collection

Download the Deloitte Insights and Dow Jones app

Subscribe to receive more content

But not every organization is optimizing the opportunities available in the Age of With. Some do little or nothing 
with data to aid their decision-making. Others carry out analytics projects in pockets of the business. Far fewer 
consistently embed analysis, data, and evidence-based reasoning into their decision-making process.

Most large companies fall into the last two categories, as do all the companies surveyed recently by Deloitte (see 
sidebar, “Survey methodology: Becoming an insight-driven organization”).

It’s understandable why many companies feel they are far down the path of becoming an insight-driven organization. 
Many have invested in creating the requisite data initiatives, analytics, or data science groups. Many have created 
chief data officer (CDO) or chief analytics officer (CAO) organizations. The vast majority have invested in tactical 
solutions.

These evolutions seem natural. It has been more than a decade since the term “big data” became part of the lexicon. 
Many legacy issues that traditionally posed barriers have now been eliminated or reduced. These include the high cost 
of data storage, expensive proprietary software, and the need to devote capital to expensive data centers.

But what is the reality? How many companies have truly evolved into insight-driven organizations?

To find out, in April 2019, Deloitte posed questions to more than 1,000 executives working at large companies (500+ 
employees) who interact with, create, or use analytics as part of their job. The goal of the survey was to see how 
many identified their company as being in the top two categories of the Insight-Driven (IDO) Maturity Scale (see 
figure 1)—analytical companies and analytical competitors2—as well as discovering how fully they leverage data and 
tools to make decisions, and what role culture and talent play in IDO maturity. Among our findings:

Most executives do not believe their companies are insight-driven. Fewer than four in 10 (37 percent) place their 
companies in the top two categories of the IDO Maturity Scale, and of those, only 10 percent fall into the highest 
category. The remaining 63 percent are aware of analytics but lack infrastructure, are still working in silos, 
or are expanding ad hoc analytics capabilities beyond silos.
Culture can be a catalyst or a culprit. Establishing a data-driven culture is harder than acquiring the right tools 
or hiring the right talent. But it pays off. Organizations with the strongest cultural orientation to data-driven 
insights and decision-making were twice as likely to have significantly exceeded business goals. Among the 37 percent 
of companies in the survey with the strongest analytics cultures, 48 percent significantly exceeded their business 
goals in the past 12 months, making them twice as likely to do so compared to the 63 percent that do not have as 
strong an analytics culture.
Aim high for analytics champions. Executive sponsorship is vital to this level of organizational change and the best 
champion sits in the corner office. According to the survey, the CEO is the lead champion of analytics in 29 percent 
of companies surveyed, and these companies are 77 percent more likely to have significantly exceeded their business 
goals. They are also 59 percent more likely to derive actionable insights from the analytics they are tracking.
Most executives are not comfortable accessing or using data. Fully 67 percent of those surveyed (who are senior 
managers or higher) say they are not comfortable accessing or using data from their tools and resources. The 
proportion is significant even at companies with strong data-driven cultures, where 37 percent of respondents still 
express discomfort. This points to a major opportunity for companies to provide more education and improve the user 
experience if they want every employee to use insights as part of their work.
The Insight-Driven Organization Maturity Scale

Survey methodology: Becoming an insight-driven organization
To obtain a cross-industry view of how organizations approach business analytics and where they fall along an 
analytics continuum, Deloitte conducted an online survey of 1,048 executives (senior managers or higher) who interact 
with, create, or use analytics as part of their job and work for US-based companies with 501+ employees. Thirty-six 
percent are senior vice-presidents and above, and 13 percent are from the C-level. We eliminated 928 potential 
respondents representing approximately 35 percent of the potential survey base because they do not have any 
interaction with analytics in their companies. The survey was fielded in April 2019 with a margin of error of ± 3.03 
percent at the 95 percent confidence level.

Business analytics becoming mainstream
Three-quarters of survey respondents report that their organization’s analytical maturity has increased over the past 
year, and nearly as many—70 percent—expect business analytics to be more important in the next three years than it is 
now.

Accompanying these indicators of increased organizational awareness is the finding that over the next few years, 
business analytics as an organizational priority is expected to be on par with such critical drivers of business 
value as risk management, reputation management, product/service innovation, and managing growth expectations (see 
figure 2). In other words, analytics is becoming an established fact of business life and no longer the sole domain 
of the IT or finance department.

AI, the more technology-intensive relative of business analytics, is not yet used as commonly as some other business 
and management tools—though other Deloitte surveys suggest that its use is growing even more rapidly than business 
analytics.3

The importance of business analytics is rated close to reputation and risk management

As figure 3 shows, analytics is being used to support a number of strategic areas representing a broad spread of 
fairly mainstream use cases with no low frequencies. The top use—identifying business process improvements—aligns 
with one of the top organizational priorities among respondents shown in figure 1—overall cost efficiency. Using 
analytics to improve processes, such as optimizing pricing or inventory levels, represents a classic use of analytics.

Slightly less common is the use of analytics to guide product and service development or monitor competitors. This is 
not surprising. Typically, there is less structured numeric data available in these functions. Also, as is discussed 
later in this report, companies are far less likely to rely on unstructured data such as product images or customer 
comments gathered from interacting with a chatbot while shopping online.

Identifying business process improvements and understanding and improving customer experience top the list of use 
cases for analytics

The insights trifecta: Data and tools, talent, and culture
Being an insight-driven organization is not the result of any single factor; it is multidimensional. For 
organizations to fully leverage the insights they derive and embed them into decisions and actions, a combination of 
three drivers is required: data and tools, talent, and culture.

Beyond BI and spreadsheets
Although it’s only one component of success, investment in and adoption of data and tools is perhaps the most visible 
and easily measured manifestation of where a company is on its journey to becoming an insight-driven organization.

The traditional workhorses of the data analytics universe—spreadsheets such as Microsoft Excel and business 
intelligence tools such as Microsoft Power BI or IBM Cognos—are the most commonly used tools. Yet 67 percent of the 
companies surveyed also use at least one advanced tool such as SAS, an open source tool such as R, a programming 
language such as Python, or an AI tool.

67% of the companies surveyed deploy at least one advanced tool to access and generate business analytics

This range of tool sophistication is encouraging. However, most organizations in the survey are at risk of hindering 
the success of their analytics by limiting the types of data they employ and limiting the chance to adopt a 
cross-enterprise approach.

According to our survey, most organizations (64 percent) today report relying on structured data from internal 
systems/resources. Far fewer (18 percent) have taken advantage of unstructured data (such as product images or 
customer audio files) or comments from social media. These data types can be difficult to put in the row-and-column 
relational format characteristic of traditional data storage, but over the last decade several new technologies have 
emerged to address that, including Hadoop and other open source projects, cloud-based architectures, approaches to 
managing streaming data, and new storage hardware environments.4

These forms of data are often more challenging to interpret but can deliver a more comprehensive and holistic 
understanding of the bigger picture—particularly of the world outside an organization. A look at companies that 
exceeded their business goals shows a connection with this appreciation for unstructured data: Executives who say 
unstructured data is one of the most valuable sources of insights are 24 percent more likely to have exceeded their 
business goals, according to our survey.

The majority of companies today also adopt a fragmented, siloed approach to analytics tools and data, 
which correlates with diminished business success. Sixty percent of companies in the survey that use different tools 
and systems across different teams or business units exceeded their goals last year. But among a much smaller 
group—the 26 percent that use a single, common set of tools and methods across the enterprise for accessing and 
analyzing data—an impressive 80 percent exceeded their business goals last year. The absence of one enterprise 
approach to finding insights is a common barrier to effectiveness.

Data terms decoded
Analytics: The systematic quantitative analysis of raw data to draw conclusions that help drive business strategy and 
performance.5

Artificial intelligence (AI): The development of computer systems that can perform tasks previously performed only by 
humans.

Big data: Big data is structured and unstructured data generated from diverse sources in real time, in volumes too 
large for traditional technologies to capture, manage, and process in a timely manner.6

Machine learning (ML): A method of data analysis that automates the building of analytical models. These 
algorithm-based models are primarily built from statistical techniques and theoretical computer science and leverage 
large datasets to continuously learn and improve.7

Case study 1: A common data and analytics environment at Procter and Gamble
Procter & Gamble (P&G) has long maintained a common, enterprisewide approach to data management and analytics as part 
of its Global Business Services organization. It established global standards for data type and quality; with these 
standards, aggregating and comparing data across product lines and regions became significantly less complicated with 
the data being stored in standard formats in the central data warehouse. Over time, managers found that the single, 
companywide database played a much more strategic role in aiding decision-making by serving as the “one truth” for 
the entire corporation. The company’s IT organization also automated the generation of reports that were used across 
multiple business units. In addition to simplifying the task of retrieving data and performing some basic analyses, 
the reports served to standardize the way data was visualized across the company.8 With consistent visuals, 
analysts and managers from one unit could step into a role, or even a meeting, with a different product or region and 
quickly understand the situation. The IT organization also developed scaled and standard solutions for accessing 
analytics, including the Decision Cockpit, a web-based customizable dashboard that tracked the most relevant data and 
news for each individual employee. For management teams, P&G developed a patented physical environment for 
information-based executive decision-making called the Business Sphere. Leadership teams would typically gather in 
over 50 Business Spheres around the world at regular intervals to review product and market performance for the 
previous periods and make decisions about how to proceed going forward.9

Data science is a team sport
Talent can be spread broadly across an organization or concentrated among a select few. D.J. Patil, appointed as the 
US government’s first-ever chief data scientist in 2015, liked to say that data science is a team sport.

But if data science comprises teams within organizations, the teams are generally small and homogeneous. The survey 
data shows that two-thirds of organizations rely on a select group of employees who have been trained on analytics or 
data science, versus 27 percent who say that all employees are trained (quite rare in our experience) or the small 
minority that trains no employees.

Organizations need to embrace a diversity of roles and skills. Instead of relying on siloed teams of highly technical 
quantitative experts, companies would do well to cultivate a wide variety of people throughout the organization who 
are curious, numerate, and capable of translating between analytics/data science methods and business requirements. 
This might be called the “democratization” of data science.”10

Data from the survey confirms the merit of this approach. In companies where executives report that all employees 
have been trained on analytics, 88 percent exceeded business goals, compared to just 61 percent of companies in which 
only select employees have been trained on analytics.

Concomitant with the need to involve all employees in the use of analytics for decision-making is the need for 
user-centered design and stakeholder analysis. Decision-makers, data scientists, and business analysts, all must care 
about the business outcomes and be consulted frequently on analytics projects. Effective users of analytics are those 
who understand what matters most, prioritizing the right questions and zeroing in on the right decisions to improve. 
They need to understand where to find the insights and quickly know what to discard, to avoid being overwhelmed by 
the massive volumes of data that businesses now routinely generate or receive.

The issue of improving companywide access and use of analytics is not restricted to employees at lower levels in the 
organization. The survey shows that 67 percent of those surveyed (who are senior managers or higher and interact with 
data) say they are not comfortable accessing or using data from their tools and resources. Surprisingly, 
comfort level rises with title level, which could show access to resources also rises with level in the organization. 
In any case, the fact that two-thirds of executives at large organizations are not comfortable navigating what is now 
the lifeblood of every business is a gap that could yield huge dividends if addressed.

67% of our respondents aren’t comfortable accessing or using data from their tools and resources

Case study 2: Building an analytical talent ecosystem at Principal
Principal Financial Group, based in Des Moines, Iowa, offers insurance, retirement, and asset management products. 
Recruiting data scientists and AI specialists is difficult anywhere, and especially away from hotspots such as the 
Bay Area, Boston, and New York; so Joseph Byrum, the company’s chief data scientist, has been forced to be creative 
about building a talent ecosystem. In addition to hiring data scientists, he’s explored crowdsourcing and close 
relationships with university analytics programs. Over the course of his career, Byrum has worked with thousands of 
crowdsourcing projects, and he strongly believes that they generally yield more innovative solutions than working 
with internal domain experts. He works with several different crowdsourcing firms. In addition to crowdsourcing, 
Byrum and Principal also rely heavily on an even less common talent source—student capstone projects in analytically 
oriented programs at universities. There are now hundreds of such programs in the United States alone,11 and many of 
them have “capstone” programs in which students are expected to work on real business problems with corporate 
sponsors. Byrum has set up over 60 capstone projects at a variety of universities—many involving important problems 
for Principal to solve. Of course, Principal does need to hire some internal people as well to help develop AI and 
analytics solutions, and Byrum has thus far built a team of about 20. He hires often through the capstone projects, 
which Byrum finds much more useful than interviews. He says that the staff that he does hire are a little different 
than most data science types in other companies—they have quant skills, but also design thinking and systems 
engineering skills. He also puts a strong emphasis on their ability to communicate and collaborate and uses 
psychological profiles to ensure that members of his team complement each other.12

A culture that acts on insights
Among the key drivers that help companies scale from carrying out analytics projects in pockets to becoming an IDO, 
a data-driven culture is the most difficult to establish. It also appears to be the one factor holding back many 
organizations.

A data-driven culture is one in which important decisions are made based on data and analytics (assuming that data is 
available). It is important to note that data, technology, and solid statistical and machine learning capabilities 
are facilitators but not necessarily drivers of insight-driven decision-making. There has to be a willingness to act 
on analytically derived insights—to make decisions, change processes, and adapt behaviors based on insights rather 
than intuition.

The survey highlights this powerfully by revealing a strong correlation between culture and business performance: 
Organizations that reported having the strongest cultural orientation to data-driven insights and decision-making are 
twice as likely to have reported exceeding business goals in the past 12 months. Forty-eight percent of these 
businesses say they outperformed their target versus just 22 percent of those with a more diluted analytics culture (
see figure 4).

A twofold difference in not just meeting but exceeding business goals is a significant difference, and it contributes 
to the dividing lines between the insight haves and have-nots we describe in the introduction.

The link between insight-driven culture and business performance

Creating any form of desired culture is a challenge for most organizations, and analytics is no exception. Only 39 
percent of respondents say their company has a strong cultural orientation to data-driven insights and 
decision-making, and a similarly low percentage (37 percent) feel employees in their company are aware of the 
importance of data analytics.

But, as with other areas in this survey, organizations that have a strong analytics culture and employee awareness of 
analytics are more likely to have exceeded business goals, illustrating the bottom-line benefits of investing in 
creating the right culture and elevating employee awareness of the importance of analytics.

How companies assign responsibility for analytics is a critical factor in moving up the maturity scale. Here the 
results are encouraging: Analytics is deployed companywide at the majority of organizations (57 percent), 
while one in three deploy across specific company groups and only one in 10 deploy them in an ad hoc manner.

As with talent, the survey data indicates diffusing responsibility across organizational lines is more effective than 
localized responsibility (see figure 5). However, the survey shows most rely on a pool of identified experts rather 
than charging all employees with responsibility for developing insights or learnings from data analytics. In other 
words, analytics is not yet a team sport.

Broadest responsibility for analytics correlates most with exceeding business goals

The concept of necessary-but-not-sufficient is a critical one for insight-driven maturity. It is easy to mistake the 
necessary for good enough; but building tools and acquiring data are not sufficient to reach the top levels of IDO 
maturity. As figure 5 illustrates, spreading responsibility for analytics across the enterprise and making 
analytics-gathering and decision-making a team sport are crucial to success. Acquiring tools without engaging teams 
will not lead to better business outcomes.

The IDO Maturity Scale: Five kinds of organizations
As stated earlier, when companies in the survey were asked to rank themselves on the IDO Maturity Scale (see figure 
1), 63 percent placed themselves in the bottom three categories—analytics aware, localized analytics, and analytics 
aspirations, meaning they are not insight-driven organizations (see figure 6).

The methodology used in the survey eliminated approximately 35 percent of the potential survey base because they did 
not have any interaction with analytics in their companies. This means the insight-driven organizations in the survey 
make up 37 percent of a group that is, at a minimum, analytics aware. Given that the overall population of companies 
includes those that are not even analytics aware, the number of insight-driven organizations in real life is very 
likely even smaller than this survey indicates.

In addition to measuring where companies placed themselves on the maturity continuum, we also wanted to examine what 
patterns emerge when we looked at how they approach analytics culture, data and tools, and talent. To analyze this, 
Deloitte cross-referenced respondent’s self-ranking on the continuum with other factors.

In general, development in key input (tools, talent, culture) and outcome (business success) metrics rises across the 
maturity continuum.

An interesting exception is the group that is lowest on the continuum, those that are analytics aware. They rated 
themselves higher (close to the top performers’ rankings) in all the key factors making up analytics maturity. Given 
that they represent only 6 percent of respondents, this anomaly represents a very small percentage of the data. It is 
possible that their atypical responses may be a layman’s appraisal of the factors driving analytics maturity. In 
other words, they may be an example of the adage, “You don’t know what you don’t know.”

The otherwise steady progression of best practices on the continuum below illustrates the importance of leadership 
and culture in becoming an insight-driven organization. These two factors are most likely to be the drivers of this 
segmentation, in our experience. For example, committed leaders can provide data and tools, and hire the necessary 
people.

How companies approach analytics: The Insight-Driven Organization Maturity Scale

Making culture a catalyst
In our experience—reinforced by this survey—the vast majority of companies do not have initiatives in place to 
address data-driven culture issues. Initiatives around data and technology, and even hiring specialized talent, 
are not enough to bring about the cultural changes needed to help companies evolve to being insight-driven.

Here are some recommended steps to bring about the cultural changes needed to become an IDO:

Hire or promote leaders with a strong orientation to analytics-based strategy and competition.
Educate employees at all levels and in all functions about the role of analytics in business decision-making.
Implement individual performance assessment tying the use of analytics to incentives.
Encourage leaders to model examples. In meetings, for example, demonstrate asking for data points to back up business 
decisions.
Make it easy for employees to act on data and analytics through nudges, an effective way to motivate desired actions.13
Use social proof, a concept explored by the prominent social psychologist Robert Cialdini and a standard part of the 
“nudge” toolkit, to inspire action by discussing how other companies are doing this.14
Reward trying and risk-taking, even if efforts fail. Create a culture that respects the notion of honorable failure.
Know the limits of analytics: If you can’t get the data, you can’t gain the insights.
Enlist an executive sponsor, ideally the CEO, so that you can melt away the “permafrost” of change-resistant middle 
management.
Final thoughts
Our 2019 survey results clearly show insight-driven organizations represent a minority of businesses today, 
despite the number of years technologies such as big data and analytics have been readily available. The most common 
culprit is culture. Buying and using analytics tools is not hard—changing behaviors is. By emphasizing education, 
enlisting executive sponsors, and modeling and rewarding the right insight-driven behaviors, organizations doing 
business in the Age of With can benefit from human collaboration made greater with AI and analytics.
"""

article3 = """
Predictive analytics in health care
Emerging value and risks

As the health care industry begins to use new technologies such as predictive analytics, government health agencies, 
doctors, and primary health providers must be aware of risks and agree on standards.

Abstract
TECHNOLOGY is playing an integral role in health care worldwide as predictive analytics has become increasingly 
useful in operational management, personal medicine, and epidemiology. This article will delve into the benefits for 
predictive analytics in the health sector, the possible biases inherent in developing algorithms (as well as logic), 
and the new sources of risks emerging due to a lack of industry assurance and absence of clear regulations.

Learn more
Explore the Analytics collection

Visit the Health care collection

Subscribe to receive related content

Download the Deloitte Insights and Dow Jones app

Health care has a long track record of evidence-based clinical practice and ethical standards in research. However, 
the extension of this into new technologies such as the use of predictive analytics, the algorithms behind them, 
and the point where a machine process should be replaced by a human mental process is not clearly regulated or 
controlled by industry standards. Government health agencies, doctors, and primary health givers need to be aware of 
the risks emerging and agree on levels of assurance as society continues to move into a new era of decision-making 
supplemented, and at times replaced, by evidence from digital technologies. More specifically, this paper will look 
at the various ethical issues and moral hazards that need to be navigated following the adoption and use of 
predictive analytics in the health care sector with an emphasis on accountable algorithms.

Introduction
Predictive analytics can be described as a branch of advanced analytics that is utilised in the making of predictions 
about unknown future events or activities that lead to decisions.

It is a discipline that utilises various techniques including modelling, data mining, and statistics, as well as 
artificial intelligence (AI) (such as machine learning) to evaluate historical and real-time data and make 
predictions about the future. These predictions offer a unique opportunity to see into the future and identify future 
trends in patient care both at an individual level and at a cohort scale.

Predictive analytics is based on logic that is drawn from theories developed by humans to fit a hypothesis (
supervised learning). A set of rules and processes are developed into a formula that undertakes calculations and is 
known as an algorithm. Predictive analytics can also be based on unsupervised learning which does not have a guiding 
hypothesis and uses an algorithm to seek patterns and structure in data and cluster them into groups or insights. In 
unsupervised learning the machine may not know what it’s looking for but as it processes the data it starts to 
identify complex processes and patterns that a human may never have identified and therefore can add significant 
value to researchers looking for something new. Both supervised and unsupervised predictive modelling are valid 
analytical tools to use in a well-rounded application of these technologies.

Predictive analytics is increasing in its application and has been very useful in various industries including 
manufacturing, marketing, law, crime, fraud detection, and health care. The health care sector, with its many 
stakeholders, stands to be a key beneficiary of predictive analytics, with the advanced technology being recognised 
as an integral part of health care service delivery. This paper will look at the various moral and ethical hazards 
that need to be navigated by government agencies, doctors, and primary caregivers when leveraging the potential that 
predictive analytics has.

With new technologies come new risks. This paper will evaluate various scenarios in the use of predictive analytics 
with a particular focus on service delivery within health care.

Heath care in the digital era
Two of the most disruptive factors in recent times are the rise of the internet and the smartphone. Together they 
have allowed for people around the world to have access to a large repository of knowledge and information at their 
fingertips. These have transformed industries, including arguably the most regulated and traditional of them, 
health care, which is undergoing drastic change.

The move toward the adoption of technology in the health care sector has had a tremendously positive impact on 
medical processes along with the practices in which health care professionals engage.1

Some of the key milestones include the digitisation of health records, access to big data and storage in the cloud, 
advanced software, and mobile applications technology. All of these milestones have presented various advantages in 
the health care sector, including an ease of workflow, faster access to information, lower health care costs, 
improved public health, and the overall improvement of quality of life.

They have also aided in significantly reducing health care wastage and in the development of new drugs and 
treatments, along with helping to avoid preventable deaths.2 Going forward, technology will continue to play a 
fundamental role in improving the health of people and reducing mortality rates among people of all age groups. 
Predictive analytics will play a central role in this.

A risk emerging for predictive analytics includes the centralisation of data which presents a tremendous risk in 
terms of security and integrity of the data. Given the increasing amount of data that is often stored in the cloud or 
otherwise accessible via the internet, there is the persistent threat of hacking from individuals with malicious 
intent. There are also ethical issues to be considered, given the role the cloud technology plays in predictive 
analytics and the overall outcome.3 In this article, we focus on the ethical issues and leave security of data and 
the cloud to another time.

The significance of predictive analytics in health care
To better understand the various possibilities of predictive analytics in health care, it is first important to 
acknowledge the different ways through which health care can benefit from this discipline. These include operational 
management such as the overall improvement of business operations; personal medicine to assist and enhance accuracy 
of diagnosis and treatment; and cohort treatment and epidemiology to assess potential risk factors for public health.

Benefits and risks associated with predictive analytics in health care

Benefits of predictive analytics
Improving efficiencies for operational management of health care business operations
Accuracy of diagnosis and treatment in personal medicine
Increased insights to enhance cohort treatment
Efficiencies for operational management
Predictive analytics allows for the improvement of operational efficiency. Big data and predictive analytics are 
currently playing an integral part in health care organisations’ business intelligence strategies. Real-time 
reporting is relatively new but can provide timely insights into data and can be used to dynamically adjust the 
predictive algorithms in line with new discoveries and insights.

This technology allows the scrutinisation of historical and real-time patient admittance rates to determine ebb and 
flow, while also providing a capability to evaluate and analyse staff performance in real time. As an example, 
surge issues in hospitals creating bed shortages may be able to be addressed if the data provides insights which can 
then be used to prevent the issue from occurring in the first place. Extra staff may be able to be deployed to a 
ward, or a seasonal occurrence may enable prior planning to deal with the issue before it arises. This, in turn, 
allows for the overall improvement of service delivery to patients, helping to ensure that they receive the best 
possible quality of care. Patients can enjoy an increased accuracy of diagnoses, which in turn allows for a more 
effective treatment of their illnesses.4

As an example in operational management, predictive analytics insights can help optimise staff levels so managers 
know how many staff members they should plan to have in a given health care facility to achieve optimal 
patient-to-staff ratios. This can be achieved by utilising historical data, overflow data from nearby facilities, 
population data, demographic data, reportable diseases, and seasonal sickness patterns in a predictive analytics model.

Operational management can also benefit as the technology exists to assess weather patterns such as ambient 
temperature readings, and calendar variables such as day of the week, time of the year, and public holidays to 
forecast patients seeking care. One can estimate the volume of walk-in patients that a facility can handle, 
allowing them to recruit and roster staff accordingly,5 helping optimise operations.

Predictive models can also assist in the recruitment and assessment of new staff competencies. With the increased 
demand for aged-care services, pressure will increase on health care organisations, and especially aged-care 
institutions, to ensure staff are fully trained, meet competency models, and have the skills as well as emotional 
capacity to handle their work in a society with an ageing population. This is within a context of increased pressures 
on medical facilities in general. Predictive analytics needs to be handled carefully in this environment but could be 
applied in interviews to construct a logistic regression model from which a candidate’s performance can be predicted. 
This would be particularly useful when processing large numbers of applications for new roles and trying to narrow 
the field to a shortlist of suitable candidates.

In these roles, self-control, resilience, and leadership are key behaviours that might be useful to assess.

However, applying sophisticated actuarial mathematical modelling to human behaviour is complex. Humans are not 
machines and are less able to be analysed, assessed, and predicted. There are whole fields of study such as 
psychology, sociology, anthropology, political science, and behavioural economics, to name a few, which offer a wide 
range of models and approaches to consider. Mathematics is a base for predictive analytics and the engines that drive 
it—algorithms. It is an integral part of contemporary social and behavioural sciences with many of today’s profound 
insights into human behaviour drawing on mathematical formulae and insights. Predictive analytics can benefit 
efficiencies in health care operations for staffing optimisation and fit, but considerable work still needs to be 
done to provide accurate insights into individual human behaviour.

From a regulation perspective, predictive risk profile models can be developed to identify the risk profile of 
aged-care services based on data such as pressure injuries, staff-to-patient ratios, qualified staff, wages, 
patient turnover, and profitability statistics. This information can highlight anomalies in the system and areas that 
need investigation, as well as help predict what resources and training are required for the future provision of 
quality patient-centred services.

Predictive analytics in the health care sector also allows for a more definitive diagnosis of patients, followed by 
the appropriate treatment of the identified ailment(s). For hospitals this can mean a significant optimisation in 
operations and a reduction in readmissions. Predictive tools such as remote patient monitoring and machine learning 
can work hand in hand to support decisions made in hospitals through risk scoring as well as threshold alerts.6 This 
technology can allow the involved parties to proactively prevent readmissions, and emergency room visits, as well as 
other negative events.

As always, it is important to look at what truly matters for caregivers and patients. Predictive analytics has great 
potential, not just for patients but to caregivers as well. Going forward, it is becoming an integral component of 
service delivery in the health care sector, thereby making it a necessity and not a luxury.7 Using predictive 
analytics would help ensure that health care facilities can deliver exceptional services for a long time to come in 
an environment of population growth, while also addressing issues of timely treatment for patients and providing a 
more accurate diagnosis for patients.

Personal medicine
In personal medicine, predictive analytics can play a key role at the individual level and enable the use of 
prognostic analytics and big data to allow for doctors and other involved parties to find cures for certain diseases 
which they might not be familiar with at a given time. This introduces more accurate modelling for mortality rates at 
an individual level. Notably, it’s long been known that some medicines seem to work for a specific group of people 
but not others. This is because people are complex and unique and there are many things to be witnessed in an 
individual’s DNA (genome) and how it’s expressed.

While it is virtually impossible for one health practitioner to manually analyse all of this information in detail, 
big data and predictive analytics allow the involved parties to uncover unknown correlations, insights, and hidden 
patterns through examining large datasets (big data) and forming predictions based on them. These can be applied 
effectively at the individual level, and consequently caregivers are more likely to come up with the correct 
treatment or drug to treat a specific illness.8

Predictive analytics in health care is also increasingly being used to advise on the risk of deaths in surgery based 
on the patient’s current condition, previous medical history, and drug prescription, as well as to help in making 
medical decisions. For example, statistical tools can detect diabetic patients with the highest probability of 
hospitalisation in the following year based on age, coexisting chronic illnesses, medication adherence, 
and past patterns of care. The University of Pennsylvania utilises predictive analytics to identify patients on track 
for septic shock 12 hours before the condition occurs9, and health insurance companies are increasingly sophisticated 
in applying such models to assess risk.

Cohort treatment
The increasing digitisation of electronic health records and legislated performance reporting requirements for 
hospitals and other medical facilities provide valuable and large datasets to be able to obtain insights into the 
health of a community. Access to this data is closely monitored and legislated to avoid the risk of identification 
and to protect individuals. The drive toward open data means that pressure is increasing to release data for the 
common good and more and more datasets are being made available for research purposes globally and in Australia.

Predictive analytics on large population studies using volumes of health system data including geographic, 
demographic, and medical condition information can generate profiles of community and other cohort health patterns 
and inform health organisations and government agencies on where to better target interventions such as ‘quit 
smoking’ or ‘obesity’ campaigns, thereby increasing effectiveness. Predictions on the likelihood of disease and 
chronic illness based on historical data could create early interventions that aim to reduce the financial and 
resource load on the public health system in the future.

Data could also be used from the pharmaceutical sector to highlight clusters of diseases and disorders and predict 
and redirect supply chain requirements and resources to target demand more accurately and avoid medicine shortages.

Epidemiological studies are based on risk assessments and statistics that aim to identify and prevent illness for 
populations at risk. Predictive analytics can provide fast and accurate insights to utilise risk scores and give 
insights into collective health issues beyond now and for the future. This will help to proactively identify groups 
of people at risk into the future for health issues such as disease outbreaks and cancer clusters.

Ethics and moral hazards in predictive analytics for the health care sector
There is always risk in statistical modelling and predictions. However, predictive analytics is providing a new 
source of risk as the technology increases the pace of the decision-making process, and the exact point at which the 
decision needs to be handed over from a machine to a human mental process is usually unclear and unregulated. 
Algorithms behind computer processes are known to be biased unless very clear risk controls and assurance processes 
are actively engaged and addressed. Computer systems reflect the implicit values of the people coding and training 
them and currently accountability for coding and training algorithms is not regulated or consistently applied across 
the industry.

There are a significant number of ethical dilemmas and an emerging moral hazard, resulting in increased risks, 
to be aware of in applying predictive analytics to the health care industry. Some of these risks are thousands of 
years old and are amplified due to faster decision-making processes with the digital disruption, and others are 
emerging as technology and analytics become more prevalent. This article has briefly touched on a number of 
significant issues, each of which could warrant their own detailed article.

The health care landscape is complex and difficult to navigate. Our ethical responsibilities in a given situation 
depend in part on the nature of the decision and in part on the roles we play. A patient and a family member play 
different roles and have different ethical obligations to each other than a patient and their doctor. These are 
entrenched, however, in key ethical principles that are embedded as far back as the origins of the Hippocratic Oath. 
This was developed by Hippocrates, an ancient Greek doctor, and is the earliest known expression of medical ethics 
requiring new doctors to swear to uphold ethical standards and abstain from wrong doing and harm. In this paper, 
it is assumed that the majority of caregivers and family members, as well as the allied health system, aim to align 
with Hippocratic-based ethics with an additional modern emphasis on patient autonomy, privacy, and respect.

Key considerations within the context of predictive analytics are that respect, privacy, autonomy, and doing no harm 
are accepted key principles within ethics and that moral hazard is an extension of this.

New sources of risk
Fast pace of technology and impact on decision-making processes
Moral hazard and human intervention points with the machine (including choice architecture dilemmas)
Lack of regulation and algorithm bias
Privacy pressures
Fast pace of technology and impact on care
Change is happening at a faster pace than ever before globally. The term digital disruption has arisen to capture the 
essence of just how fast everything is changing based on new technologies. The old way of doing things is not only 
changing, it’s changing at speeds that are often difficult to keep up with. The way we do things and our thinking are 
literally uprooted with all the digital choices we have now. We can book medical appointments on our phone, 
see a doctor online, order clothes online, and even apply for a personal loan online through crowdsourcing. Most of 
this was not possible 10 years ago. The day-to-day ways of working, human relationships, and even recreational 
pastimes are changing.

The health care industry is not immune. Doctors are now under pressure to combine clinical personal care with data 
capture. Traditional doctor and patient relationships are impacted with the doctor needing to ensure they capture 
information digitally from patients as much as possible, resulting in a need to mix on-the-spot personal care and 
human touch with machines and data entry. One challenge is finding a balance between patient care and data capture 
within the traditional allotted appointment times whilst maintaining a trusted doctor and patient relationship. The 
storage of the data is also a potential risk and can lead to loss of trust if breached, as shown recently with the 
large number of people (reportedly over 1 million) that opted out of the Australian government’s move to an 
electronic health record system. Not everyone will trust the security of the data being kept by their doctor.

Doctors are increasingly finding they need to continually evolve their computing skills as technology systems become 
more and sophisticated and are linked with the ability to read and interpret information such as pathology reports 
from digital sources. As an example, X-rays are rarely held up to light boxes any more but are available on software 
systems on a doctor’s desktop computer or laptop.

Patients are also driving the disruption with new expectations. In other parts of life, patients are offered 
convenience, real-time information, value for money, and options when considering services. This has led to an 
expectation of these things from their health care providers, resulting in online doctor services, self-help, 
instant payment of rebates, and choices such as home health. This extends to the expectation that patients now see 
more data capture than ever before and are increasingly aware that treatments might be able to be more specifically 
tailored to their DNA and health history.

Digital disruption is not necessarily moving at the same pace across the entire medical industry. Pockets of care are 
still heavily reliant on traditional approaches such as the reliance on paper records with associated data quality 
and linkage issues. However, the overall pace of change is accelerating and is having a considerable impact on the 
sector. With this comes emerging ethical issues that need to be addressed and which are outlined in further detail in 
this article.

Moral hazard and human intervention points with the machine
Moral hazard has roots in many areas, including behavioural economics and the insurance industry. Essentially people 
are often considered to undertake more risky behaviour if they think they have a safety net. One key example of moral 
hazard is that people are inclined to undertake more risky behaviour on the basis under which they are insured, 
over and above what they would normally do. For example, a worker becomes less diligent on safety issues on a work 
site because he knows he is covered by labour accident insurance if something untoward should happen. Essentially 
risk is transferred to someone else (the social fund), thereby adversely modifying the behaviour of the insured 
person.10 The transfer of risk and liability within the medical industry is complex and this risk combined with 
misdiagnosis from a machine adds to the complexity that needs to be addressed when integrating predictive analytics 
into health care.

This could increase risk in health care if, for example, a doctor relies on a computer to give a diagnosis over their 
own assessment. They may take more risks because they believe they are protected with the computer being accountable 
and bearing the cost of the risks. This challenges the ethics of respect and doing no harm, with the key decisions 
being outsourced to a machine and the accountability lines being blurred in the diagnosis and treatment plan.

The accuracy of the machine may be proven to be higher than that of the doctor, but if a doctor relies solely on the 
machine, it is questionable whether the doctor is doing no harm for multiple reasons. Various ethicists argue that 
the human touch is vital in recovery and that outsourcing decision-making in health care to machines is not 
respectful. The successful use of predictive analytics in health care needs to consider the importance of aligning 
with accepted ethical standards and the intervention points for when the human touch or an empathic human decision is 
more critical than that of a machine’s.

The effectiveness of predictive analytics in the health care sector drills down to the role of the different 
stakeholders therein. One area that could raise a moral hazard is the role of the doctor. Previous research has 
highlighted that the most extensive ethical encounter of predictive analytics is its probability to affect the role 
of the doctor.

It is noted that predictions on adverse medical events by the predictive analytics models can promise greater 
accuracy than prognostication by clinicians.11 However, reliance on such models may be called into question without 
clear documentation of the point at which the machine-based decision is assigned to a human mental process. To avoid 
any complications along the way, doctors and caregivers should capture data and discuss treatment pathways in detail 
with patients as usual and that as part of this treatment process they clearly track the decision-making process 
points between the human and the machine.

New skills will be required to work hand in hand with technology. The mastering of these skills will need to include 
at what point a caregiver decides to deviate from a machine-based recommendation and back their own judgement, 
observations, and experience as well as mastering excellent communication with their patients and their families.12 
This will help support the decision-making process, ensuring caregivers do not rely solely on the safety net of 
trusting the machine but instead continue to apply a human mental process to diagnoses, with the machine aiding their 
accuracy but not overriding their judgement.

This will help doctors and caregivers to manage the trade-offs that are involved in different clinical outcomes, 
even while taking into consideration the predictions made using relevant models. The ideal outcome is that these 
models are our tools and not our masters13 and should be used in conjunction with a human mental decision-making 
process.

Moral hazard and liability in predictive analytics can also involve lawsuits. Case law points out that doctors can be 
held accountable for injury that could have been avoided had they more carefully reviewed their patients’ medical 
records. To avoid such outcomes, predictive analytics models may be of positive use for all parties if they are 
integrated into the existing decision support systems. Medical negligence lawsuits may increase if patients feel a 
doctor overrode a machine’s recommendation. Liability may also arise if a doctor follows a predictive analytics model 
recommendation and it contains an error. To reduce the risk doctors should not become complacent and need to document 
their decision-making processes, clearly articulating when their judgement overrides the machine in as much detail as 
possible.

The use of predictive analytics in health care and society in general is evolving and the best approach is to view 
this new technology capability as a useful tool that augments and assists the human decision-making process—rather 
than replacing it. Adhering to models in predictive analytics should be discretionary and not binding. Doctors need 
to be able to override the diagnosis or recommendation when their judgement ascertains it is appropriate to do so. A 
clear risk model should be available to assess all factors and assist the decision as the use of machines in patient 
diagnosis and care is best used when integrated with a human mental process.

Choice architecture is another aspect to consider within the ethics of risk assessments for predictive analytics. 
Predictive models provide a series of results based on data. Assumptions are built into these data, and options 
provided by predictive analytics will carry risk scores.

Health care providers need to assess the options from the analytics results and present patients with choices. Choice 
architecture is a behavioural economics concept that aims to provide interventions that influence people without 
impacting their freedom of choice.14

Information from the predictive technology is designed to help providers and patients with more accurate diagnoses 
and clearer findings for decision-making about treatments. The way the information from the analysis is presented to 
the patient may influence their decision and so both care givers and analysts involved in predictive modelling need 
to be aware of the risks of presenting the information and consider choice architecture frameworks when designing 
communications with patients.

If the treatment options carry risks, then it can be potentially an issue about how the information is presented, 
with complexity revolving around the preferences and values of the health care provider and the patient. These are 
tough decisions and doctors need to be able to apply a mental process to the predictive analytics and feel able to 
override recommendations on multiple factors to present their choices based on unique factors. These may include the 
mental and emotional stability of the patient, risks of the proposed intervention, potential errors in the analytics, 
stakeholder opinion, potential liability, and risk of automation bias which occurs when a person automatically makes 
the customary choice even if the situation calls for another choice.15

Decisions about the ease of overriding the predictive model to suggest alternate treatment plans over the machine 
evidence should be made on a case-by-case basis and clearly documented for future liability or ethical concerns.

Lack of regulation and algorithm bias
A potential issue with predictive analytics is the possibility of bias or impartial representation. Extrapolative 
analytics models require a sizable amount of data that are representative of the entire population as opposed to a 
mere fraction of it. Predictive analytics in the health care sector is very useful if it’s applied to benefit the 
large majority of the population. A challenge is ensuring equitable representation without bias.

Bias in building predictive models also needs to be addressed with the development of accountable algorithms wherein 
specific decision-making processes can be traced back to within the predictive analytical model. Algorithmic bias 
occurs when the technology reflects the attitudes and values of the humans, conscious or otherwise, who are coding, 
collecting, selecting, or using the data to train the algorithm. People often place a great deal of trust in 
algorithms and consider them to be neutral and unbiased. However, this is incorrect.

Most of the algorithms driving predictive analytics are developed by fallible human beings who all hold prejudices 
and biases—whether conscious or unconscious. Without a continuous feedback loop of improvement and true attempts at 
reducing bias, serious statistical errors can occur within predictive analytics. There is no clear legislation or 
policy framework in this area in Australia, so an ethical issue can occur unless risk controls are put in place 
specifically to address bias. These controls are currently voluntary and motivators to circumvent them might be the 
promise of profits or the attraction of ‘an amazing find’.

Risk matrix

Explicit attempts to write algorithms that are accountable, as well as fair and equitable, are not always at the top 
of the agenda when organisations are struggling to keep up with digital disruption. Government legislation and 
regulations do not specifically cover algorithm development or use and rely on a system of controls which is unclear, 
and clearly voluntary. The system relies on the majority of people in technology knowing to utilise risk models that 
help them avoid bias and voluntarily doing the right thing. The European Union’s General Data Protection Regulation (
GDPR) requires organisations to be able to explain their algorithmic decisions. However, this is open for 
interpretation and there are no clear guidelines for what this should look like. The mathematical and computer 
programming audit trail could clearly highlight any logical incompetence in design. However, this needs to be 
considered with a social sciences approach. Is it being used in a socially acceptable way? How is this measured? Does 
it exploit human vulnerabilities? How was bias removed? Is the data sample suitable? Tracking the accountability 
trail and ethical landscape is complex.

There are examples in health care of self-regulation being implemented in the absence of government policy, 
such as the protocols regarding electronic blood pressure machines that were found to have a margin of error when 
they were first introduced. The margin of error impacted on the level of seriousness of the patient’s hypertension, 
which in some cases could have meant the difference between life or death. The European Society of Hypertension 
International Protocol for the validation of blood pressure monitors now exists and sets a series of protocols and 
validations of machines for self-regulation, supplementing dedicated hypertension protocols in countries such as 
Britain, Australia, and the United States. This shows that if industry takes the issue seriously enough, they don’t 
need to wait for legislation. Risk controls can be introduced voluntarily.

Another ethical aspect to consider is the building and validation of the model to be used in the predictive analysis. 
It is important for the entire undertaking to be patient-centred and have patient-centred perspectives, without which 
they could be considered unethical.16 Patient-centred care is respectful of, and responsive to, the preferences, 
needs, and values of patients and consumers. Dimensions of patient-centred care are generally accepted as respect, 
emotional support, physical comfort, information and communication, continuity and transition, care coordination, 
involvement of family and carers, and access to care. Projects utilising predictive analytics in health care need to 
align with the intent of patient-centred care to remain ethically viable.

The establishment and introduction of ethics committees in government agencies, regulatory bodies, and associations 
may go some way in the modern age to addressing the potential for inequality and bias when using predictive analytics 
in health care. Ethics committees are used in clinical trials and at some hospitals, and are well entrenched and 
respected in the universities and research sector.

These models work well and could be adapted by organisations and government agencies to self-regulate in the absence 
of clear legislation so as to protect the welfare and rights of people through accountable algorithms and 
technologies that actively aim to avoid bias. There are examples of a few government departments and organisations 
setting up their own corporate ethics committees or partnering with universities. This means that the risk for bias 
in projects involving the design of algorithms for predictive analytics could be reduced through controls agreed via 
a rigorous ethical assessment exercise run through an ethics committee prior to implementation.

The standards for validation and transparency could also present some ethical issues along the way when applying 
predictive analytics. It is noted that before any prognostic analytics model is utilised in medical care, it ought to 
be carefully appraised for effectiveness and any potential adverse consequences. It is important to establish an 
appropriate validation standard, analysis plans, and other avenues that would help to guarantee the integrity of the 
entire undertaking and the effectiveness of the analysis to be conducted.17 This includes technology-led models. In 
Australia the National Safety and Quality Health Service (NSQHS) Standards aim to protect the public from harm and 
improve the quality of health care. They describe the level of care that should be provided by health service 
organisations and the systems that are needed to deliver such care. However, the guidelines for technology-related 
projects are not as strong as those for performance reporting or clinical trials and there is much work to be done to 
provide clear ethical guidelines in this space.

Privacy pressures
Opening up medical data for research is not new. Insights into symptoms, diseases, treatment patterns have been 
benefiting populations for a number of years. However, the amount of data being collected is larger than ever before 
and is growing faster and faster with the move to electronic health record keeping and faster data-sharing. 
Predictive analytics capitalises on data, and this needs to be collected from patients and other involved parties.

The move to digital records means that there is strong growth in the amount of health care data available and the new 
wealth of opportunity they provide to increase wellness, but also in the rise of some serious privacy considerations. 
The move from paper- to electronic-centred patient health records has made the health care industry rich in data and 
how the data is collected and interrogated is protected by the Privacy Act of 1998 (Privacy Act), which is an 
Australian law that is essential in regulating as well as handling the personal information of an individual. The 
Privacy Amendment (Enhancing Privacy Protection) Act 2012 amends the Privacy Act of 1988.

In Australia, data derived from individuals is protected by the Privacy Act that precludes the release of personal 
sensitive information to unauthorised parties. This covers situations within the health sector when personal health 
information from a patient is collected, as well as situations when data derived from an individual is used in 
research. De-identification and encryption of data is required in order to conduct research and protect personally 
sensitive information, and includes access controls and applying security measures such as codes to ensure privacy of 
individuals is retained, while encouraging data-sharing for research purposes when appropriate and possible.

The Internet of Things (IoT) advances have resulted in unprecedented levels of personal data being captured from 
wearable devices, social media, and even shopping patterns. This provides rich datasets for health researchers and 
for predicting health patterns and behaviours. The data economy means that this information that is primarily 
collected in the commercial sector can be made openly available for sale or use.

Predictive analytics encourages data-sharing to produce more results that are accurate. The bigger the datasets the 
higher likelihood of accuracy in the predictions. This often challenges the concept of privacy and can put data at 
risk if it isn’t handled correctly in line with legislation and privacy controls. The greater reliance on the use of 
technology means we need to ensure continued compliance with ethical requirements. There is strong potential in being 
able to use de-identified patient data to improve health services for everyone in the community. However, privacy is 
a very important right for a patient18 and is an important condition for other rights such as freedom, as well as 
personal independence.

With the onset of advanced technological developments in the health sector, there is a need for privacy to be upheld 
and there are strict laws that are set up to direct health sector providers on how they should collect information 
about a patient’s situation. This also includes how the information is stored and how it can be used, or shared.

According to the Australian Charter of Health Care Rights, each person that is involved in care, as well as 
treatment, is obliged legally as well as professionally to keep information about their clients private at all times. 
At times, information about a patient needs to be shared among different health care providers. They are only allowed 
to share this information with consent. Despite the significant benefits of utilising predictive analytics in health 
care at an individual and cohort level, there is a real need to align with privacy controls and keep data private. 
The ethical issues inherent in breaches of privacy are covered in the legislation of the Privacy Act and all 
predictive analytics models and projects should align with the legislation at all times.

Conclusion
The concern that predictive analytics may reduce patient care to a set of algorithmically derived probabilities is 
important and real. Particularly as legislation and governance lags behind technology disruption. However, 
the benefits are also important and real.

Technology is playing an integral role in the world today and all sectors are benefitting from what it has to offer. 
The health care sector is no exception. It can benefit significantly from predictive analytics, and it can be argued 
that this technology is a core aspect of the future of medicine and health care delivery in general.

The advantages associated with sensibly designed and implemented predictive analytics in the health care sector far 
outweigh their potential issues. Millions of people around the world stand to benefit from its adoption, 
with patients able to enjoy an improved service delivery that anticipates challenges and addresses them proactively. 
Diagnosis would be more accurate, as would the treatment that follows. Caregivers would also benefit, given how easy 
it would be to access useful information and take appropriate steps toward seeing the health of their patients improve.

However, even with these advantages, there are many emerging risks that need to be navigated for all involved parties 
to benefit from the full potential of predictive analytics.

Mostly, this would involve setting clear risk controls to cover bias, address emerging ethical considerations, 
and ensure clearer documentation for accountability. With policymakers still moving to catch up with the drafting of 
appropriate legislation, this would require self-regulation from those responsible for writing the algorithms. 
Existing predictive models and analysis also need to avoid breaking any existing laws such as those around privacy or 
violating ethical standards.

Predictive analytics has a strong and healthy place in the future of health care delivery. However, we need to 
remember that the algorithms and models behind predictive analytics are not perfect and need to be made more 
accountable and transparent with clear human intervention points when appropriate. They also need a clear foundation 
to be set that seeks to be ethical and nonbiased in its application, preferably one guided by legislation.
"""

article4 = """
Macro technology forces
Insights In Depth: Tech Trends 2020

This episode focuses on the macro forces shaping the technology of today and the future. Deloitte leader Bill Briggs 
and FedEx CIO Rob Carter show us ways to navigate the technology trajectory and invest in cutting-edge tech.

TRANSCRIPT
TANYA Ott: Back in the early 1800s, every train was a private entity. So, there were several railway companies that 
each laid their own tracks and they laid them to different dimensions. Meaning, they couldn’t share. And there was a 
lot of duplication.

Learn more
Read the Macro technology forces chapter

Explore the full Tech Trends 2020 podcast collection

View Tech Trends 2020

Create a custom Tech Trends 2020 PDF

Go straight to smart. Get the Deloitte Insights app

Rob Carter: In 1864, President Lincoln passed the Pacific Railway Act, which set a standard gauge for the railroad at 
four foot, eight and a half inches. Today, if you go out and measure a railroad track, you'll see that, rail to rail 
on center, it is still four-foot, eight-and-a-half inches.

Tanya: For transportation, it was the beginning of a concept called dominant design.

Rob: Dominant design became a really important part of how we connected this country and unified the east and the 
west and the north and the south.

Tanya: And that fed the Industrial Revolution. Who would have thought that four feet, eight and a half inches would 
set the U.S. on a trajectory that means today we can not only imagine self-driving vehicles, but we’re starting to 
share the streets with them?

It may seem like technology is evolving so quickly you can’t possibly keep up. But that’s okay, because my guests are 
going to guide us through some emerging technologies that could answer questions you don’t even know you have.

I’m Tanya Ott and my guides for today’s deep dive into Tech Trends are Bill Briggs …

Bill Briggs: I’m the global chief technology officer for Deloitte Consulting, and I lead our emerging technology 
research incubation and help clients figure out how to take full advantage of all the advancing technology happening 
in the world around us.

Tanya: and Rob Carter. …

Rob Carter: I’m the chief information officer at FedEx Corporation. What that really means is we run a big global 
network, all around the world 7/24/365. There is technology that powers every airplane that flies, every truck that 
rolls, every package that ships, and the tools that our team members need to keep the world on time.

Tanya: Rob’s been with FedEx for 27 years and in the technology sector for even longer.

 

Rob: When I started in technology, there was a huge disparity in the types of technology that you deployed. IBM 
computers didn’t communicate with Burroughs Computers. Didn’t communicate with DEC VAX computers and on and on. Every 
network was different. Every compute infrastructure and operating system was different. But today, we see dominant 
design taking hold of computers as well.

Bill: I love it, because one of the things I spend a lot of time on is, even though it feels like there’s so much 
change happening in technology and all these different dimensions and it’s only increasing, at the same point, 
there are some of these larger macro forces that are closely tied to some long-lasting design principles and 
technology that actually hold true. It gives you some confidence that even though, every day, you can open up that 
TechCrunch or your analyst briefing [and see] all these new buzzwords and interesting things happening, we can map 
them to some more lasting [trends], we sometimes even call them the eternities, things around data, computer, 
and interaction patterns. It allows us to actually chart a path forward. I love starting out with the Pacific Railway 
Act! Interoperability is a very physical manifestation because it plays true today too.

Tanya: Bill, for about a decade now, the team at Deloitte has been tracking emerging technologies. I would love to 
have you get in the way-back machine and think back to those early years of the Tech Trend reports. What were you 
thinking about some of the technologies like a cloud or artificial intelligence? Did you think they were going to 
work? Did you think they were going to be profitable for companies?

Bill: It was a bold position when we started. It’s the 11th year that [we’ve] published. We said it needs to look out 
at 18 to 24 months. We were always trying to have that pragmatic view of prognostication, because part of it was, 
we need[ed] to show that these are real in the wild, and they mean something to folks that are going to be actually 
deploying them for operations, for customer engagement, for their business.

In that frame, things like Cloud and AI certainly were on the map. A lot of the conversations at that point where 
what are they? What do they really mean? How do we demystify them? What’s nice is you look at them, there’s been an 
evolution in all of these bigger topics. You take cloud—it continues to be a dominant design pattern. We’re talking 
now about how do we have autonomics, and we go into DevOps [the practice of development and operations engineers 
collaborating] and NoOps [an environment where software and software-defined hardware are provisioned dynamically] as 
things within the IT shop—How do we actually take full advantage of that capability? That’s something that Rob and I 
could geek out on. If we’re talking to Fred [Fred Smith, founder and CEO of FedEx] and the executive team, 
it’s more about what does it mean to you in terms of speed of responding to market conditions, demands, and altering 
the cost structure. And that has held true over the years. It’s less about the whats of the individual technology. 
Our goal is to change it quickly into a conversation about the so what? How do you apply it? And then the now what? 
Because it’s not a shiny object parade of wouldn’t-it-be-nices. I’m a big Beach Boys fan, but it’s really about what 
do you do about it to create material value?

Tanya: The pace of change has been pretty intense in those 11 years and it continues to accelerate, which has got to 
be both kind of exciting and overwhelming for companies. Rob, walk us through how FedEx has approached this sort of 
whirlwind pace of technology advancements. Can a company play in all the spaces? Do they have to plant a flag in one 
space or is it something in the middle there?

Rob: We recognized quite a while back that the technology that got us here wasn’t going to get us to this new future 
that was so cloud-enabled and so flexible and so able to connect to resources everywhere around the planet. We began 
to look hard at the legacy technologies that we had and figure[d] out ways that we could refactor that technology 
base and make it significantly more modern and significantly more useful in this cloud-enabled world where everyone 
is looking to tap into resources, like shipping services, but also like computers, like telecommunications, 
and all of the things that maybe we used to do on our own.

An interesting legacy story for us is that Fred Smith said, “the information about the package is as important as the 
package itself.” And that was such a critical juncture in our company, to pivot to not just moving things, 
but moving things with information moving along with them. I believe, that was the beginning of the digital twin 
movement, because our story has for a long time been a tale of two networks, a digital network that represents the 
things that are moving on the physical networks that we operate that connect 98 percent of the world’s GDP. This 
digital twin concept caused us to build a lot of technology. In the early days, we connected our trucks with 
computers that allowed them to take their handhelds right out to the edge of interaction with our customers. There 
were no cell phones or mobility, so we actually bought up the 800-megahertz spectrum coast-to-coast and put radio 
towers out there. Not so that we can talk to the couriers that were out on the road, but so that their handheld 
computers could interact with the systems and provide visibility all the way out to the edge of interaction [with our 
customers].

Tanya: Bill, when you’re talking with company leaders and you’re trying to help them navigate this idea of the 
rapidly changing technology—Where do they get in? How do they balance it all as they’re thinking about it? What are 
the things that you have them think about?

Bill: I love to piggyback on Rob’s story there. The initial reaction is usually market- and customer-facing, 
which is fantastic because it’s how do we bring these together in ways that allow us to engage differently with our 
constituency: How do we take new products and services into the market, how we think about our business and our 
business model. That’s huge and is going to continue to dominate. But sometimes we don’t focus enough on the 
enterprise and how work gets done in core operations—say, how do we deploy technology in ways so we can fundamentally 
work differently and think about how we deliver against the engine of the enterprise. And the fact that this story of 
the enterprise optimization enablement, the digital twin being a feature, that helps with some efficiencies across 
logistics operation fulfillment. But it’s also one of the promises FedEx has had to customers in the market about a 
different level of confidence in tracking and visibility into the end-to-end delivery cycle: How those two aren’t 
separate beasts and how do we create, not an abstract promise, but very bounded and pointed things that we can do to 
materially meet an unmet need or shape a new ambition, by linking the customer market-facing with the ... you know, 
people don’t often start with, how does finance or supply chain or HR, those things being the front of digital 
transformation, but there’s so much power and potential there and they often link into outward facing–market facing.

Tanya: This leads us to the idea of where tech decisions live. For a long time, tech decisions lived in tech 
departments, but that’s evolved now to a space where it has to be C-suite decisions that are more holistic, 
looking at the entire organization. Does that mean that every top executive in a company also has to be a technologist?

Bill: We’re seeing the shift from 11 years ago. When we did Tech Trends it was for the CIO and the CTO and the IT 
department as the prime audience that wanted to understand what was happening and what it might mean. CIOs like Rob 
have become business leaders, respected and revered within the industry because of their ability to shift the dialog 
up into the C-suite and individual line-of-business owners and leaders, to really say, we ought to collectively 
understand technology enough to make sure we’re pushing the right ambitions forward. We’ve seen a lot of companies 
actually have formal, call it a tech fluency or a tech savvy initiative that’s typically sponsored by the CEO and 
sometimes even [extending] into the board to say, how do we make sure that we don’t have blind spots on potential? By 
the way, the CIO is still the one being looked at to drive not just the alignment, but probably instigate a lot of 
the change that needs to come because of the technology agenda.

Rob, if this was a video podcast, I could say this statement and see if you’re going to roll your eyes: The fact that 
every company is a technology company and that’s something that CEOs and boards tend to understand and 
appreciate—What’s your reaction?

Rob: There’s no question that in today’s world, technology is a C-suite discussion. We’re pretty fortunate at our 
company that our CEO has been an information architect for forever. We talked about the fact that “the information 
about the package is as important as the package” was something that was put into our DNA a long time ago. But having 
technology savvy, not just at the executive table, but at the board table, is particularly important for our company.

For example, in the year 2000, we established a formal committee of the board of directors known as the IT Oversight 
Committee. We’ve had legendary technologists on our board like Jim Barksdale, the guy that took Netscape public; Judy 
Estrin, a serial technology entrepreneur who did so much. Today, we continue to have technologists on our board. 
Chris Inglis, who was the deputy director of the NSA, a very noted cyber expert. This isn’t a subcommittee of the 
board. This is a full-blown committee of the board that looks not just at technology risk, but technology strategy 
and advantage and potential, which is what really has become the most important part of technology. It’s what is the 
potential that technology has to not just optimize your business but effectuate your customer and their ability to do 
business with you in an easier fashion and in a more productive way.

Tanya: We talked about the pace of change and one of the outcomes of that is that technology that was a good fit for 
a company when it was initially deployed eventually—I don’t know if this is your words or someone else’s words but I 
really like the phrase— become sea anchors. They basically can weigh a company down. Let’s talk for a minute about 
legacy tech. Rob, you sort of hinted at that a little earlier in our conversation. It can sometimes seem a little bit 
risky to try to tackle the legacy tech, but it’s something that’s got to be done, right?

Rob: It’s really important that we recalibrate our risk meters as we look at the state of technology that we’ve 
deployed over the years. At big enterprises, technologies had a tendency to collect, and it’s collected through a 
series of waves of technology that were born prior to what the dominant design is today. If you had stayed on that 
narrow-gauge railroad and insisted that you weren’t going to participate in the common gauge railroad, the next thing 
that you would see is tumbleweeds rolling down Main Street. You become obsolete if you don’t keep up with the 
dominant design.

So when I say recalibrate the risk, what I mean is that too often we believe that the safe thing to do is stay still 
and to just utilize the technology we’ve got—mainframe technologies, etc. I would argue that the chance of a 
next-generation company starting up today and building a data center and filling it with those classes of legacy 
technology is absolutely zero. What that means is, that is not the dominant design anymore, and if you’re stuck with 
that technology, it’s going to be problematic going forward. And tumbleweeds are sort of rolling to you. We 
underestimate the risk of staying still at the same time as we overestimate the risk of moving forward. We have to 
move. We can’t stand still in enterprises today. We’ve got to refactor, rebuild, and redeploy technologies that live 
in the dominant-design world of cloud-enabled, cloud-native, better integrated to the world around us than the 
islands of technology that we built inside our company.

Tanya: Bill, as you have been talking with C-suite execs, technology folks about what their companies are actually 
doing, what are you hearing from them in terms of the appetite or the risk-taking when it comes to moving from those 
legacy technologies?

Bill: There is this starting point where there’s typically some institutional angst around the topic of an ERP [
enterprise resource planning], replatforming or a core modernization, because in the past, it’s been told in a very 
self-contained story and they’re not easy. No one wants recreational open-heart surgery. So those who have lived 
through that pain, that they bring that is very real. But I like to flip it and say you have to tell the story about 
what that investment is going to enable, the core being the foundation of all the new capabilities you want to bring 
to market. And the ability to take advantage of emerging technology like AI and blockchain and cloud and soon quantum 
and the things coming behind it, and link those two narratives and almost debunk or demystify the fact that you have 
technical debt. That is just an exhaust of having technology investment over the years. Most executives that haven’t 
been given a little bit of a talk track about how to think about it, you immediately think the technical debt means 
malfeasance in a way, where your tech leaders and your people, your IT department, your consultants, your vendors 
have made bad decisions and built bad code. That’s almost never the case. It happens, but it’s a very small 
percentage. The vast majority of technical debt is [what] I call misfeasance of investments made at a point in time, 
sometimes with perfectly justified tradeoffs and it promises that will come back in the next release and do it 
better. That’s a big chunk of technical debt. Then what Rob described is another huge chunk, 
which is nonfeasance—things that were absolutely state [of] the art when they were done, it’s just that the times 
have changed and it can’t participate in the full potential of the technology landscape around us today. So you have 
to do something about it. In a way, it’s almost resetting the talk track around why it’s so important for growth and 
for competitive advantage. By the way, there might be some cost and efficiency plays at the outcome, but that’s very 
hard to justify in its own right. Anyone that says a cloud migration transmission project or a core modernization 
project is going to be an immediate cost neutral or cost negative, that’s likely a very optimistic posture. Let’s 
just put it that way (laughs). But the return is absolutely positive over time.

Tanya: That’s a great transition into the Tech Trends report, where you break down technologies into these four 
categories. You label them enablers, foundation, disruptors, and horizon next. And I’d love to go step by step 
through these. First of all, what are the enablers?

Bill: What the whole point of this, just to put in a sentence of context, is when you do a trends report, 
the conflict, the tension between novelty and shiny objects, something that a Rob reading it hasn’t heard of before 
or thought of before, that is a very different promise than saying, “Here’s the things that we think should be at the 
front of your agenda in the next 18 to 24 months, the core investments that you should be making that are actually 
going to lead to material change.” And we wrestled with that tension over the years, so we decided to use the first 
chapter to look at those macro technology forces that have proven themselves over time. And we think they’re going to 
last. What’s nice is, you can actually start mapping some of the more interesting small or novel type of activities 
that are happening into those bigger buckets, which isn’t just a nice taxonomy for a research piece. It actually 
helps you get confidence about investing in areas because you see it as maturity and evolution and not these 
brand-new, out-of-nowhere concepts that come in. So in that macro forces framing, we try to basically say, 
the enablers are the pieces that are decade-plus and we continue to see investment. No one’s finished with the 
journey, but big topics like cloud and analytics and experience, which is that emphasis of human-centered design for 
not just customers but employees. It’s three big areas that a massive amount of advancement happened over the last 10 
years continues to happen.

Tanya: The first category then is enablers. The next category is foundation, which like its title means, 
these are kind of the foundation of a lot of this.

Bill: And the point is not to take a step away from trends, because there are trends happening in the foundational 
areas as well. But they’re the pieces that don’t often get the frontline headline when we talk about emerging tech 
and tech trends. One of them is core modernization, which we had a great discussion on. One of them is the broader 
definition of risk, cybersecurity, privacy, and regulatory compliance. And now the ethics and morality of tech 
investments. And “just because we could, should we” being frontline to strategy. And the third one is the business of 
technology, which is really the idea of how does the IT department in the business evolve to be able to deliver 
against a very different landscape, with an emphasis on faster time of delivery to respond to market conditions and 
more agile, not just within IT, but with the business. That’s the up or delivery model of the technology organization 
to take advantage of all these advances today and what’s coming tomorrow.

Tanya: The next category is disruptors. Is that a word we’re still using? It seems like everybody [and] their brother 
uses the word disruptor or disruptive.

Bill Sure. We are. The point being, looking out the next five to 10 years, to the things that we think are going to 
be seen as inevitable as what cloud analytics and experience look like today. And those [are] blockchain, cognitive, 
which is how we talk about AI and advanced automation, and then digital reality, which is this intersection of 
augmented reality, virtual reality, voice, computer vision. Moving from point-click-type, which is the old way, 
we interacted with technology systems, we moved into touch and swipe, and now we’re getting into talk and gesture. 
The end state is going to be, the system knows who I am, what I’m trying to do, why I’m trying to do it. Which gets 
to the last column of, if we want to be really provocative, the horizon next. The three: the ambient experience is 
exactly what I just described. It’s saying we want to move from even these advanced ways to interact with systems to 
something that’s just more seamless and intuitive. There’s a great quote from Leonardo Da Vinci—people think it was 
Jobs, actually it’s Da Vinci—“The ultimate form of sophistication is simplicity.” In this case, the ultimate form of 
simplicity is transparency. And then we have the second of the three in this column, exponential intelligence, 
which is, really getting into broad, general-purpose AI and the capabilities that [it] represents. Quantum is the 
last, not as a replacement of cloud or traditional computing, but a great augmentation and new things we can do. 
Everything that’s happening we can plot pretty confidently into these big buckets. Some of them here and true. Some 
of them coming. All of them important. But again, they’re ingredients and the whole point of the exercise is how do 
we create recipes that matter for our business, our market?

Tanya: I do want to break this down, though, because to the untrained eye, all of these technologies can kind of seem 
to come out of nowhere, right? But you argue that they’re really predictable and we’ve had breadcrumbs along the way. 
So why did you give us an example of what led to one of those disruptor forces that you talk about, digital reality 
or cognitive or blockchain?

Bill: Cloud is a perfect example. If ten years ago you asked what was different, there would be some, especially with 
CEOs and CTOs, hearty debate about why are we even talking about cloud, it’s nothing more than logical partitioning 
from the mainframe on a more distributed and open level? Or, it’s nothing different than grid computing that has been 
around for a long time. Or, it’s nothing different than automated provisioning, which is something we’ve been working 
on. And the truth of that then and now is those are building blocks that have led to the broader cloud capability. 
What’s really interesting is how that’s moved up. Originally cloud was about, I have access to computing or 
networking or storage, the lowest-level building blocks. Now the cloud players are saying, we’re going up the stack 
into AI and blockchain and quantum and all these other capabilities on top of the core. But the fact that cloud is 
absolutely a sum of many parts, many of which have been around for a long time, is the truth. And it gives us 
confidence that these things don’t just come out of nowhere. And you could get on the same path with AI, 
with blockchain, with quantum even. The pieces that make up these macro forces are happening in front of us right now.

Tanya: So we’re pulling the thread through. When you look at those things that are in that horizon next category, 
which is the fourth category, you talk about ambient experience and exponential intelligence and quantum. Maybe you 
could give us a prediction on just one of those? How do we get from where we are now to that thing?

Bill: Sure. Quantum is the one that everyone agrees on the impact and the fact that technology’s maturing, 
and it will make a material difference in our world. The question becomes timing. There are massive advances 
happening on all of those simultaneously. Some within the same shop and some across startups and academia and the 
like. The fact that we’re already saying there are specific use cases that quantum will be able to solve that 
traditional computing couldn’t, or it can solve it in an exponentially different and better way, we have good 
examples of that. Life sciences on drug discovery or looking at complex systems in material science—those are all 
things that we know of, but we’re surface-scratching because we’re taking existing problem sets and saying how could 
quantum apply to them? We’ll see a whole new generation of problem statements and solutions as it matures. So the 
counsel isn’t that everyone should be standing up a thousand-person quantum team right now. But the flip side of it 
is, having folks within the organization so you have your own credible opinion of how real are the different 
components, how it might be applied, what it might mean, and be ready to scale that up as the technology piece mature 
is absolutely appropriate. Probably measured more in the individual people versus the departments of people.

Tanya: Rob, I want to bring you back into the conversation here, because when we’re looking at some of these horizon 
next things like quantum, for instance, how are you at FedEx thinking about, how you would apply quantum? What’s your 
thought process on that?

Rob: Quantum really hasn’t become an enterprise technology yet, but obviously it’s something that we have to look at 
that horizon and understand what that class of computing could do to the traditional ways that we seek solutions. 
Today you’d sort of roll that back to large scale AI and our ability to utilize these technologies in ways that help 
us operate more efficiently or help customers get their questions answered or do analytics that provide clairvoyance 
to things we didn’t even think of asking. So we’re seeing the early waves of that massive and high-performance 
computing wave show up in the business. But quantum is something that’s still out there on the horizon. My friend Bob 
Johansen at the Institute for the Future often says, “The future is here, it’s just not evenly distributed.” So, 
that’s been true forever. When we first started interacting with customers, we were using things like America Online 
and CompuServe to allow them to dial in through their warbling modem and make a connection and track a package. Those 
connections would go to a big mainframe and return information for them. When we first stood up FedEx.com, 
the very first page of FedEx.com was a picture of a swooping package going across the screen with a small form that 
said, enter your tracking number here and you would type in a tracking number and of the internet it would go and 
connect with our back-end systems and provide tracking information. That won a Smithsonian Award for what was touted 
at that point, in 1994, as being the first business transactional website. At that point in time, most websites were 
doing nothing more than an about page or a message from the CEO. So technology has this way of showing up early but 
in an uneven way that then builds rapidly toward these incredible futures that we are now experiencing. At the same 
time, looking ahead to the potential of the future.

Tanya: Rob, I’m guessing it could be kind of easy to get fixated on some of these individual technologies or these 
trends. But is there a danger there?

Rob: We have a philosophy that says our goal is to be less wrong tomorrow than we are today. We just fundamentally 
believe you can’t be perfect in the technology choices or in the way that you deploy technology, but you can 
certainly be thoughtful about spotting the right places for investment, making small but mighty investments in things 
like blockchain and AI and IoT technologies that are going to be very important and are rapidly becoming more important.

Bill: The danger becomes being enamored with the technology itself. You have to understand the technology to be able 
to get to a credible “so what” of how it can be applied and when it can be applied. That’s the point. The quicker you 
can get to the now what? Which is, let’s take some of the potential and invest in a way that’s appropriately 
risk-modeled and scaled. What we find a lot is when we do visioning sessions, especially with business execs and 
in-line employees in different departments for companies, that once you actually bring these concepts to life and let 
them see them and understand how they could work, even if it’s within a completely different context and completely 
different industry, that show-versus-tell goes a long way to saying, could it do this, could it do that, and can we 
bring them together to solve problems that we’ve just always assumed were constraints that were permanent or 
limitations in the way that the world could work or our jobs could work? So that idea of the “now what”—how do you 
accelerate that experimentation and have a much more fact-based and confident investment to either scale it or not? 
And that last pieces is doing nothing on some of these areas can absolutely be strategic as long as it’s deliberately 
undertaken.

 

Tanya: Rob, as we’re sort of closing out this conversation, I’d love for you to talk about how at FedEx you’re 
looking at making these big investments in the macro forces that we’re talking about.

Rob: We have a strong belief that reimagining business on a regular basis and focusing on next-generation 
technologies has big payoffs. We’ve placed a few bets that are really important. We’ve placed some bets on 
blockchain, for example. When you think about what we do for a living, we create custody chains of information from 
the time we first take a shipment from a customer to when we deliver it to the end recipient. And we have a very 
strong custodial chain that moves along that path. We very much think that the future will expand those custodial 
chains for supply chains all the way back to the point of manufacturing. The reason that that’s so important is for 
the provenance and authenticity, of whether it’s pharma or whether it’s luxury goods or whether it’s food products, 
we believe that that heritage of products as they move along the supply chain is something people are very much going 
to want to know.

We also believe that the Internet of Things is a critical investment for us. We’re making investments. We have many 
patents. It’s been a very busy patent filing season for us around IoT in transportation. But we’ve got a lot of 
sensor-based logistics where we put sensors and technology into shipments and allow them to be monitored all the way 
through the journey. Not scanned at individual points in time, as has been the legacy of tracking, but a journey 
that’s visible and transparent and recorded. Now, managing the data and the information that comes off of these kinds 
of sensors is a big deal. The sensor itself is the easy part. How do you take all that machine exhaust that comes off 
of literally millions of sensors at some point and manage that through to being useful information for customers and 
operational efficiency? But we very much believe that these classes of investments are going to pay off significantly 
and that that’s the future that we’re working our way toward.

One of the most important tenets of innovation for us, though, is what we call the two wings of innovation. We 
believe that if you only attempt to innovate internally to a company, it’s like a one wing bird, it doesn’t fly and 
it kind of flaps around in a circle. You have to embrace partners and leadership and thought from the outside and 
products from the outside of your company as the second wing. That’s what really allows innovations to soar and for 
you to make progress toward your goals.

Bill: A very visceral picture of the one-wing, floundering bird.

Tanya: Totally.

Rob: We’ve long been a company that believed in analytics and managing data and we apply it operationally and we 
allow customers to apply it to their world as well. But the journey that we’ve been on has been from empirical data 
that lets you see accurately through an analysis of what happened in the past, to real time information which allows 
you to fairly carefully and closely examine what’s going on in the present, to then predictive technology and 
analytics that lets you peer into the future with important algorithms that will allow you to see what’s likely to 
happen next. Finally, what we call clairvoyant technology, which is not a question that we even knew to ask, 
but one that gets revealed inside of the information as it collects that that identifies patterns is super important.

Bill: I love that turn of phrase. I promise that the next 10 times I use it, [it] will be direct attribution before I 
appropriate. That’s great.

Tanya: Bill and Rob, thank you so much for this conversation. It’s one of many coming on Tech Trends. We’re going to 
be diving deep into some of these other issues as well.

Tanya: That was Bill Briggs, the global chief technology officer for Deloitte Consulting, and Rob Carter, 
the chief information officer at FedEx Corporation.

Bill and Rob gave us a lot to think about. The macro forces they discussed are the underpinnings of the 2020 Tech 
Trends. And we’re going to take a much deeper dive into the tech trends in upcoming episodes. Scott Buchholz, 
chief technology officer for the government practice within Deloitte Consulting LLP, is going to be our guide. Scott, 
tell me what you got coming up.

Scott: We’ve got a lot of interesting things coming up. Several of the trends for this year are actually in response 
to persistent challenges, and others represent new opportunities or evolving opportunities that are likely to drive 
significant change in enterprise technology. The trends that we’re going to be talking about include ethical 
technology and trust, where every organization is being disrupted by technology and every interaction, every choice 
represents an opportunity to gain or to lose trust.

Next is finance and the future of IT, where we’re looking at the relationship between the CIO and the CFO, 
how we finance innovation and emerging technologies, and how we think about governance and controls both inside IT 
and in finance, accounting, and procurement as we move forward in this new agile world.

Tanya: Another topic that you’re tackling, Scott, is going to be digital twins, which is where we’re sort of bridging 
the physical and the digital within an organization.

Scott: Yes. Digital twins are virtual simulations. What we’re seeing is, as the capabilities have increased over the 
years, that they’re moving out of the manufacturing into other industries. Basically, the ability to create a virtual 
simulation and interact with physical reality and vice versa is a really interesting trend.

Human experience platforms [is next]. What we’re starting to see is a combination of things. One is, people are 
designing systems for the complete human, perhaps not just what the organization wants, but what the human being 
wants, the constituent, the employee, the customer, or what have you. The other half of this is that technology is 
increasingly able to, in real time, detect and respond to human emotions using things like voice stress analysis and 
computer vision.

Tanya: You also are doing something with architects.

Scott: Yes. So within IT, what we’re finding is that architects are some of the most underutilized resources in a 
number of organizations. And we see people getting increasingly thoughtful about how they deploy some of their most 
experienced senior technical resources.

And last but not least is horizon next, as we look beyond our typical 18- to 24-month horizon to try to see what’s 
out there in terms of emerging technologies, not just for the next two years, but for the next five and beyond. And 
how you can actually take that collection of maybes from the future and work backwards to today to figure out what 
you should be doing to anticipate those in the futures.

Tanya Great, Scott, I can’t wait to have those conversations

You can also connect with us on Twitter at @DeloitteInsight. I’m on Twitter at @tanyaott1. I am Tanya Ott. Have a 
great day!

This podcast is produced by Deloitte. The views, thoughts, and opinions expressed by podcast speakers and guests are 
solely their own and do not necessarily reflect those of the hosts, the moderators, or Deloitte. This podcast 
provides general information only and is not intended to constitute advice or services of any kind. For additional 
information about Deloitte, go to Deloitte.com/about.
"""

article5 = """
Pump your own data
Maximizing the data lake investment

“Data lakes” give business users direct access to raw data for analysis, potentially speeding the decision-making 
process. But CDOs need to put the right security, support, and governance measures in place for this model to work 
effectively.

ORGANIZATIONS are constantly looking for better ways to turn data into insights, which is why many government 
agencies are now exploring the concept of data lakes.

Learn More
Read the full CDO Playbook

Create a custom PDF

Learn more about the Beeck Center

Subscribe to receive public sector content from Deloitte Insights



Data lakes combine distributed storage with rapid access to data, which can allow for faster analysis than more 
traditional methods such as enterprise data warehouses. Data lakes are special, in part, because they provide 
business users with direct access to raw data without significant IT involvement. This “self-service” access lets 
users quickly analyze data for insights. Because they store the full spectrum of an enterprise’s data, data lakes can 
break down the challenge of data silos that often bedevil data users. Implemented correctly, data lakes provide 
insight at the point of action, and give users the ability to draw on any data at any time to inform decision-making.

Data lakes store information in its raw and unfiltered form—whether it is structured, semi-structured, 
or unstructured. A data lake performs little automated data cleansing or transformation. Instead, data lakes shift 
the responsibility of data preparation to the business.

Providing broad access to raw data presents both a challenge and an opportunity for CDOs. By enabling easy access to 
enterprise data, data lakes allow subject matter experts to perform data analytics without going through an IT 
“middleman.” At the same time, however, these data lakes must provide users with enough context for the data to be 
usable—and useful.

CDOs can play a major role in the development of a data lake, providing a strategic vision that encourages usability, 
security, and operational impact.

Rapid insights at a federal manufacturing facility
A federal manufacturing facility’s CIO wanted faster access to large volumes of data in its native format to scale 
and adapt to the changing needs of the business. To accomplish this, the facility implemented a data lake, 
which stores distributed servers to efficiently process and store nonrelational data. This platform complements the 
organization’s existing data warehouse to support self-service and open-ended data discovery. Users now have 
on-demand access to business-created data sets from raw data, thereby reducing the time to access data from 16 to 
three weeks.

Establishing the data lake platform: Avoiding a data swamp
A poorly executed data lake is known as a data swamp: a place where data goes in, but does not come out. To ensure 
that a data lake provides value to an organization, a CDO should take some important steps.

Metadata: Help users make sense of the data
Imagine being turned loose in a library without a catalog or the Dewey Decimal System, and where the books are 
untitled. All the information in the books is there, but good luck turning it into useful insight. The same goes for 
data lakes: To reap the data’s value, users need a metadata “map” to locate, make sense of, and draw relationships 
among the raw data stored within. This metadata layer provides additional context for data that flows through to the 
data lake, tagging information for ease of use later on.

Too often, raw data is stored with insufficient metadata to give the user enough context to make gainful use of it. 
CDOs can help combat this situation by acting as a metadata champion. In this capacity, the CDO should make certain 
that the metadata in the data lakes he or she oversees is well understood and documented, and that the appropriate 
business users are aware of how to use it.

Security: Control access for individual roles
By putting appropriate security and controls in place, CDOs will be better positioned to meet increasingly stringent 
compliance requirements. Given the vast amount of information data lakes typically contain, CDOs need to control 
which users have access to which parts of the data.

Role-based access control (RBAC) is a control mechanism defined around roles and privileges through security groups. 
The components of RBAC—such as role permissions, user roles, and role-to-role relationships—make it simple to grant 
individuals specific access and use rights, minimizing the risk of noncleared users accessing sensitive data. Within 
most data lake environments, security typically can be controlled with great precision, at a file, table, column, 
row, or search level.

Besides improving security, role-based access simplifies the user experience because it provides users with only the 
data they need. It also enhances consistency, which can build users’ trust in the accessed data; this, in turn, 
can increase user adoption.

Data preparation: Equip users to cleanse data sets
Preparing a new data set can be an extremely time-consuming activity that can stymie data analysis before it begins. 
To obtain a reliable analytic output, it’s usually necessary to cleanse, consolidate, and standardize the data going 
in—and with a data lake, the responsibility of preparing the data falls largely into the hands of the business users. 
This means the CDO must work with business users to give them tools for data prep.

Thankfully, software is emerging to help with the work of data preparation. The IT organization should work 
collaboratively with the data lake’s business users to create tools and processes that allow them to prepare and 
customize data sets without needing to know technical code—and without the IT department’s assistance. Equipped with 
the right tools and know-how, business data users can prepare data efficiently, allowing them to focus the bulk of 
their efforts on data analysis.

Enablement: Allow users to use familiar tools
Self-service data analysis will go more smoothly if users can use familiar tools rather than having to learn new 
technologies. CDOs should strive to ensure that the business’s data lake(s) will be compatible with the tools the 
business currently uses. This will greatly enhance the data lake platform’s effectiveness and adoption. Fortunately, 
data lakes support many varieties of third-party software that leverage SQL-like commands, as well as open source 
languages such as Python and R.

Governance: Maintain controls for non-IT resources
Once users have access to data, they will use it—which is the whole point of self-service. But what if a user makes 
an error in data extraction, leading to an inaccurate result? Self-service is fine for exploring data, 
but for mission-critical decisions or widespread dissemination, the analytical outcomes must be governed in a way to 
guarantee trust.

One approach to maintaining appropriate governance controls is to use “zones” for data access and sharing, 
with different zones allowing for different levels of review and scrutiny (figure 1). This allows users to explore 
data for inquiry without exhaustive review while simultaneously requiring that data that will be broadly shared or 
used in critical decisions will be appropriately vetted. With such controls in place, a data lake’s ecosystem can 
perform nimbly while limiting the impact of mistakes in extraction or interpretation.

Figure 1 illustrates one possible governance structure for a data lake ecosystem in which different zones offer 
appropriate governance controls:

Zone 1 is owned by IT and stores copies of the raw data through the ingestion process. This zone contains the least 
trustworthy data and requires the most vetting.
Zone 2 is where business users can create their own data sets based on raw data from zone 1 as well as external data 
sources. Zone 2 would be trusted for group (i.e., office or division) use, and could be controlled by group-sharing 
settings.
Zone 3 data sets, maintained by IT, are vetted and stored in optimal formats before being shared with the broader 
organization. Only data in zone 3 would be trusted for broad organizational uses.


Adopting a culture of self-service analytics: Empowering people to use data lakes
Implementing a data lake is more than a technical endeavor. Ideally, the establishment of a data lake will be 
accompanied by a culture shift that embeds data-driven thinking across the enterprise, fostering collaboration and 
openness among various stakeholders. The CDO’s leadership through this transition is critical in order to give 
employees the resources and knowledge needed to turn data into action.

Enabling analytics at a federal agency
A federal agency CIO team built and deployed analytics tools to support operations to influence an insight-driven 
organization. The goal was to create an environment where stakeholders were consistently incorporating analysis, 
data, and reasoning into the decision-making process across the organization, such as enhancing data infrastructure. 
To give users the ability to utilize these tools to their full potential, an “Analytics University” was implemented. 
This was well-received; more than 20,000 field employees completed level 1 courses, with 90 percent saying they would 
recommend them to a colleague. The support by upper management to invest in the use and understanding of data 
analytics across the organization encouraged a data-driven culture, and this culture shift continues to enable 
business adoption of big data technologies.

Invest in data leaders
CDOs are responsible for more than just the data in the data lake; they are also responsible for helping to equip the 
workforce with the data skills they need to effectively use the data lake. One way to help achieve this is for CDOs 
to advocate for and invest in employees that have the necessary skills, attitude, and enthusiasm. Specialized 
trainings, town halls, data boot camps—a variety of approaches may be needed to foster not only the technical skills, 
but the courage to change outdated approaches that trap data in impenetrable silos. The best CDOs will create an 
organization of data leaders.

CDOs may need to work with senior business leaders and HR in the drive for change. They should strive to overcome 
barriers, highlight data champions throughout the organization, and lead by example.

Practice nimble governance
Governance over data lakes needs to walk a very fine line to support effective gatekeeping for the data lake while 
not impeding users’ speed or success in using it. Traditionally, governance bodies for data defined terms, 
established calculations, and presented a single voice for data. While this is still necessary for data lakes, 
governance bodies for a data lake also should establish best practices for working with the data. This includes 
activities such as working with business users to review data outputs and prioritizing ingestion within the data lake 
environment. Organizations should establish thorough policy-based governance to control who loads which data into the 
data lake and when or how it is loaded.

Keep current on the technology
Technology is never static; it will always evolve, improve, and disrupt at a dizzying speed. The technology 
surrounding data lakes is no exception. Thus, CDOs must continue to make strategic investments in their data lake 
platforms to update them with new technologies.

To do this effectively, CDOs must educate themselves about current opportunities for improving the data lake and 
about new technologies that will reduce users’ burden. Doing so will open up the ability for more users to use data 
in their everyday work and decisions. Keeping oneself up to date is straightforward: Read journals and trades, 
attend conferences and meetups, talk to the users, and be critical of easy-sounding solutions. This will empower a 
CDO to sift through the vaporware, buzzwords, and flash to identify tactical, practical, and necessary improvements.

To invest or not to invest?
The challenges associated with traditional data storage platforms have led today’s business leaders to look for 
modern, forward-looking, flexible solutions. Data lakes are one such solution that can help government agencies 
utilize information in ways vastly different than was previously possible.

It would be easy to say that there is a one-size-fits-all approach and that every organization should have a data 
lake, but this is not true. A data lake is not a silver bullet, and it is important for CDOs to evaluate their 
organization’s specific needs before making that investment. By planning properly, understanding user needs, 
educating themselves on the potential pitfalls, and fostering collaboration, a CDO can gain a solid foundation for 
making the decision.
"""

article6 = """
Investigative analytics
Leveraging data for law enforcement insights

Technological advances can improve law enforcement agencies’ investigative methods, but investigators often struggle 
to decode the massive wall of data. Here’s where digital tools and techniques can help.

CAPTAIN Johnson has a problem. As the head of investigations for her agency, she and her personnel are charged with 
both drug and death investigations. The past few months have witnessed activity on both fronts: an alarming rise in 
drug overdose deaths from opioids. Her team faces questions about the source of the drugs, with inconclusive 
toxicology results from the medical examiner.

Learn more
Explore the Government and public services collection

Read more from the Analytics collection

Subscribe to receive related content from Deloitte Insights

Where can she begin? First, Captain Johnson asks her team to determine which criminal actors or organized criminal 
groups are involved in the diversion, distribution, and trafficking of similar opiates in the region. Her narcotics 
team finds a number of leads but, in fact, have too much data to analyze—from prescription drug monitoring program (
PDMP) data and public health records to financial records and social media. Homing in on the exact threat is like 
finding a needle in a haystack. Many days and many cups of coffee later, the investigators are overwhelmed and about 
to give up. But every day the problem grows, and Captain Johnson needs results.

Individuals and organizations involved in criminal and illicit activity are becoming increasingly sophisticated. Bad 
actors are harnessing the power of new technologies as quickly as they’re being invented. The good news is that 
technological advances can also improve the investigative methods of law enforcement agencies. In the words of one 
homicide supervisor, “In 1993, there were no social media or surveillance cameras. We just had blood typing that was 
used to help identify people from a crime scene. Now DNA is used, which can identify an individual using his DNA to 
one in a billion, quadrillion, or greater.”1

These benefits bring their own challenges. The digital world presents investigators with a massive, seemingly 
impenetrable wall of data. For example, in one year alone, a single FBI investigation collected six petabytes of 
data—the equivalent of more than 120 million filing cabinets filled with paper.2 When targeting illicit activity, 
law enforcement and intelligence agencies must not only grapple with a wide variety of new and unfamiliar data 
sources, but also need to make better use of the data they already collect. Without effective data analysis, 
law enforcement agencies will struggle to counter the criminal actors they are charged with targeting.

This kind of data analysis is no easy task for law enforcement agencies, with their resources mostly dedicated to 
core law enforcement functions of keeping communities safe. Most law enforcement personnel don’t have training in 
data science and digital research, which are generally required to perform advanced analytics. To get access to 
data-driven insights, many police departments have turned to data science volunteers from local universities for 
help, and even amateur genetics enthusiasts.3 Resource constraints, however, can limit an agency’s ability to bring 
outside experts in on a consistent basis, possibly resulting in a daily struggle to find relevant information for 
investigations, even though data is now everywhere.

But new investigative tools and techniques can help law enforcement agencies overcome resource constraints and come 
to grips with massive digital data. Artificial intelligence (AI), open source data management tools, predictive 
analytics solutions, and social media exploitation capabilities are helping many investigators and operators make 
sense of mountains of data. New tools can help make previously unseen connections between information and identify 
where key information is lacking, while new sources can provide access to reams of data, and new partnerships can 
help exploit the data. But most of all, these practices can give investigators more of what they really need: time. 
The FBI’s Counterterrorism Division’s efforts to streamline its data systems has enabled a 98 percent reduction in 
manual work for analysts and a 70 percent cost reduction.4 Leveraging these tools and approaches can help 
investigators return to where they belong: not writing code or searching through databases, but rather tracking 
criminals and keeping our communities safe.

The problem is not the data but the approach
Captain Johnson and her team’s data problem is a common one. Historically, investigative analysis and targeting have 
often been guided by what information an agency has access to. Just as with a hammer every problem looks like a nail, 
often every investigation begins with whatever data is most readily available. But there may be critical data sources 
that are missing, too hard to access, or too complicated to analyze, leading to blind spots in an investigation.

To simplify the investigator’s data problem, there are three fundamental questions every organization should answer: 
1) What information do we have? 2) How do we make sense of it all? And 3) What key pieces of information are we missing?

Ironically, the first step in the solution to being overwhelmed with data is often to create an even larger pool of 
data. That is because many organizations have huge volumes of data but are unable to use it effectively, 
due to computing and integration challenges. Outdated and insufficient computing power and platforms hinder advanced 
analysis. Silos, both within and outside the organization, inhibit meaningful access to integrated data that may help 
an investigation. Breaking down these walls is typically a key first step. For one police department in the United 
Kingdom, that meant bringing together intelligence data sets, command and control systems, and operational data 
streams, to give officers a unified picture on a single screen.5

Then this larger, fuller pool of data can be sorted and managed by a small team of professionals. By bringing data 
sources together, many investigators can be supported by a team of data scientists and analysts that can help them 
make sense of the data that they do have through “exploratory data analysis,” a standard process by which data sets 
are measured for breadth, depth, and quality (explored later in this article). Important data sources include 
in-house data maintained by law enforcement and intelligence agencies; commercial data sources; and open sources such 
as social media activity, property records, criminal histories, professional licenses, medical databases, and a host 
of other sources.

Once an organization fully understands the breadth of data it currently has access to, it can begin pondering the 
next question: What key pieces of information are we missing? Starting with a target- or problem-centric approach can 
help investigators create a holistic picture of persons, places, and objects of interest from a large pool of data. 
That holistic picture, in turn, can begin to show where key holes in the analysis lie. Perhaps there is no 
information on how two known associates communicate, or where a suspect receives his mail. Knowing these gaps can 
help guide future collection and surveillance to build the right case much faster.

Five steps to harnessing data more effectively
Knowing there is a more efficient way to look at data, what should agencies do to achieve results? A few key steps 
can help agencies of all sizes come to grips with the data challenges presented by complex investigations.

Take stock of your current data sources and how they are utilized and managed.6 Often critical to this effort is 
performing a detailed and honest data audit, including who controls access to which data sources and which can be 
used together. One data analysis company stated that their law enforcement clients often find that they already have 
more data than they think. Simply making this data organized and available to investigators can jump start cases.7 
Achieving greater data integrity upfront can enable greater insight downstream. For Captain Johnson’s department, 
that process might include examining internal databases, such as the records management system, and external 
databases, such as the relevant PDMP, and ensuring that data can be accessed and integrated.

Find new data sources. As investigators apply the target-centric approach to data the organization has now pooled and 
organized, they can begin to see where there are common gaps in knowledge. Everyone leaves a digital trail, 
including criminal actors and organizations, and this can allow investigators to home in on what’s missing. One new 
source of data can connect dots and break open cases. For example, one detective for the Orlando Police Department 
stated that simply beginning to treat opioid overdoses as crime scenes rather than accident sites led them to look 
through the phones of overdose victims. Cross-referencing the contact lists from different overdoses quickly led to 
the identification of a handful of dealers.8

New sources of data can be anywhere from the databases of other agencies to lying in the street next to victims. 
Identifying data sources of value from other agencies, commercial or open sources, or even publicly available social 
media data, can fill critical gaps in an agency’s investigations.

Determine how to capture and store collected data. A massive pool of all available data is not helpful to 
investigators if it cannot easily be used to find the exact information needed. This requires that the data be 
structured, organized, and stored in specific ways. The specific set of data management tools needed will depend on 
the exact volume and nature of the data an agency is dealing with. For example, powerful graph and novel data 
structures can allow organizations to create data stores that fuse different types of data into a single descriptive 
visualization of how specific persons, places, and objects relate to one another.9 The appropriate data structure can 
also allow new tools and capabilities to be added on as needed. For example, with a well-defined data structure, 
an agency could use predictive modeling to predict where certain types of crime are likely to take place, 
allowing more efficient use of resources. And agencies are seeking creative ways to store their data: In Utah and 
Oregon, some police departments are finding that they can store data together in a consortium, leading to reduced 
costs and increased insight.10

Address the need for analytics skill sets in personnel, more capable computing systems, and better integration across 
data stores. Investigators are not data scientists, nor should they be. But agencies also often lack dedicated 
training or career tracks for analytics. Combine that with the fact that siloed legacy systems are often inadequate 
for processing and analyzing the larger, more complex data sets produced by modern investigations, and you likely get 
a recipe for poor use of data. This problem is not unique to law enforcement. Many commercial organizations face 
exactly the same problem when trying to identify potential fraud within their organizations. As a recent Deloitte 
report found in a forensic financial investigation, “When data volumes are small, basic analytical skills and 
spreadsheet programs might be adequate to handle preliminary analysis of structured data from enterprise systems and 
other software applications, as well as unstructured data such as emails, texts, and voice recordings. But when that 
volume grows into the millions, analysis can require technology, advanced analytics, and forensic skills that aren’t 
readily available within many organizations.”11

Think about the type of reporting and outputs that are best suited to help investigators and analysts in the most 
efficient way possible. The days of handing someone a ream of paper or a complex set of spreadsheets—and leaving it 
at that—are over. While the specifics of data reporting and visualization will necessarily vary from case to case, 
departments should put some thought into how officers typically use data and what the most seamless way to consume it 
would be. 
How data analysis drives investigations in the real world
Technologies are not just new, interesting tools that improve existing processes; rather, they can open up entirely 
new ways of doing investigations. Where today, many investigators spend large amounts of time trying to find the 
right data, potentially leaving only a short time for analyzing it and piecing it all together, in the future, 
investigators should be able to rely on new data tools to more rapidly find data, allowing them to spend the majority 
of their time analyzing data. In 1970, a study of one medium-sized police department found that 50 percent of a 
patrol officer’s time was spent on tasks directly related to crimes (with the remaining half spent on administrative 
tasks).12 Some four decades later, a 2012 look at six different police forces found that this percentage has risen to 
80 percent—a 60 percent improvement.13

The massive impact from using these seemingly simple tools can be seen by revisiting the story from the introduction: 
Faced with the rash of overdose deaths in her community, Captain Johnson convinces her agency to develop a robust 
analytics capability. First, her agency assesses its existing data sources, performing an audit that includes 
storage, management, access, and utilization. Data analysts perform an exploratory analysis to understand their data 
quality and comprehensiveness. Realizing that they lack consistent access to critical data, including communications, 
prescriptions, and geotagged business data, the team compiles publicly available sources, data subscriptions, 
and social media data from nearby locations. Agency leaders then agree on a plan to ingest, process, and store data 
in the cloud, enabling instant access to the latest computing systems and eliminating data silos. They hire a couple 
of data scientists and provide training for interested officers to develop their own data skills.

Captain Johnson’s team was collecting information on pharmacies believed to be illegally supplying local addicts with 
prescription opioids. Equipped with new capabilities around the ingestion, processing, and storing of bulk data sets, 
along with new personnel resources with advanced analytics skill sets, Captain Johnson’s team is able to establish a 
risk ranking of the practitioner prescribers and patients found in the pharmacy data and can quickly cross-reference 
it against available drug overdose information. Upon doing this, they discover that a number of the overdose victims 
had received prescriptions and/or had opioids dispensed from some of the pharmacies in question.

Next, through the bulk ingestion and exploitation of open-source property record data, a capability the team did not 
have in the past, it realizes one of the high-volume prescribing practitioners appears to have an excessively large 
number of financial assets. In addition, analytics professionals on the team determine that the husband of the 
practitioner in question is linked via social media to a known dealer involved in heroin and fentanyl trafficking, 
both illicit opioids.

Investigators then track available social media profiles of recent overdose victims and determine that many of the 
victims had connections with both the known drug trafficker and the practitioner’s husband. By taking the property 
addresses of both the practitioner’s husband and the drug trafficker and by cross-referencing them against 
commercially available foreign-based shipping records, investigators determine that a number of packages have been 
shipped to the properties from a known industrial park in China.14

Captain Johnson and her team then work with their representative on a local federal drug task force to connect to a 
limited access database and ingest sharable information, which shows that the industrial park is home to an 
unregulated pharmaceutical facility. Another agency is working on spotting the shipments before they enter the United 
States, and a number of other local agencies are fighting a similar problem. Now, investigators have enough 
information to pursue warrants for enforcement activity.

With these new tools and methods, investigators are no longer overwhelmed and hoping for a break. They are now 
actively targeting a criminal network and getting to key targets faster. Better data sources, coupled with targeting 
expertise, have allowed Captain Johnson and her investigators to quickly obtain better results and more significant 
investigative outcomes.

Setting up systems for faster, easier data collection
We’ve seen how investigators across law enforcement and intelligence organizations use sophisticated computer 
programs to document linkages between people, places, and things. While this represents a leap forward from the 
analog era, it still requires painstaking data collection before an investigator can plot, analyze, and understand a 
network of actors.

What if this process could be partially automated, to enable analysis from day one? As with Captain Johnson’s 
department, setting systems up for an analytics capability starts with understanding the landscape of available data 
sources, from in-house data sources to social media activity. Then, the data sources have to be able to speak to each 
other. That means designing a system that organizes, formats, and stores data accessibly. For example, by formatting 
records so that names, dates, and locations are easily searchable, an investigator can reach across dozens of data 
sources to compile histories, profiles, and activities automatically.

In figure 1, various data sources are compiled into a single, structured output. Inputs include unstructured data 
such as scans from documents; semistructured data such as websites and social media; and structured data including 
property records, professional license records, and criminal records. The output is a single, multidimensional 
profile of an individual of interest, just as Captain Johnson might be interested in finding information about an 
individual suspected of overprescribing opioids.

Semistructured data sources can become structured profiles of people or places, which, in turn, can offer networks 
for investigators to analyze

But investigators rarely need just one person’s profile; instead, they often need to understand multiple forms of 
connections, from known relationships, to presence in the same geographies or businesses, to shared connections on 
social media. Using automated data analysis to find links among multiple structured output profiles, investigators 
can develop networks of actors and understand activities and behaviors across a group (see figure 1). For example, 
natural language processing can perform what is called “named entity recognition.” In layman’s terms, this means that 
AI can use contextual clues to tell the difference between the State of Georgia, the country, and the first name, 
for example.15 For investigators, this can mean differentiating between a suspect and an innocent citizen with the 
same name.

These tools all serve to increase the accuracy of investigations and helping police find the right conclusions 
faster. Instead of spending weeks or months to develop a detailed link chart, they can leverage both stored data and 
live data streams to develop an advanced starting point for their analysis, allowing them to more quickly get 
results. The increased speed and accuracy of investigations can be attested by the Durham, North Carolina police 
department. That department used natural language processing to study incident reports and find patterns in criminal 
activity. With that information, they were able to identify areas with a high incidence of specific types of crime, 
allowing them to deploy the right assets to the right areas ahead of time. The end result was a 39 percent drop in 
violent crime in Durham from 2007 to 2014.16

Aligning the organization with new capabilities
It’s not always easy to change the way an agency goes about conducting its investigations. But as criminals become 
increasingly sophisticated, law enforcement and intelligence professionals have to harness all available data or risk 
being left in the dark. To be sure, new solutions, tools, and methods will not take root overnight. They need 
continued support and may require new technical and targeting capabilities for investigators, operators, and analysts 
alike.

Organizations have a natural inertia to keep doing things in the same ways, with the same tools. The more changes 
that better technologies force upon an organization, the more likely they are to be resisted.17 However, 
a few concrete steps can help any agency get started on the journey to investigative transformation:

Leaders championing the change. Having an active and involved champion can increase the likelihood of success for 
technology implementation.18
Bringing technology and investigative staff together. Technology should not be developed in a bubble. By working 
together in cross-functional teams, technology staff can develop the right tools faster, and investigators can be 
sure they are getting tools that they can actually use.19
Communicating with partners. No organization, however large, has all the data it needs. Therefore, interagency 
partners can be crucial to long-term success. Continuous and open communication with these partners regarding agency 
goals and data needs can increase the likelihood that they share needed data when you need it most.20
While these steps are just the beginning, they can offer a path to a new era of investigations, one where law 
enforcement is not overwhelmed by data, but can harness it for good.
"""

article7 = """
Automated machine learning and the democratization of insights

The predictive insights organizations can gain from the oceans of data they generate are currently limited by the 
availability of experts who can crunch this data. Automation may change that.

AS data proliferates in organizations, there is an increasing need to understand its implications through the 
generation of insights. Insight generation through business intelligence and analytics has been available for almost 
half a century, but it typically required the help of trained analysts. The insights needed by decision-makers within 
an organization were constrained by the number of analysts, and without easy access to analytics, these analysts were 
often forced to rely on experience and intuition. To make matters worse, difficult-to-use technologies made it 
challenging for most business people to find and analyze the data they needed to generate insights.

Learn More
Explore the Analytics collection

Visit the AI and cognitive technologies collection

Subscribe to receive related content

Over the past several decades, multiple technologies have been used to democratize the creation of insights, 
including interactive statistical packages, spreadsheets, easy-to-use visual analytics tools, and the like. But we 
don’t think they are enough for today’s complex technology and data environment.

The rapid increase in the amount of data and the power of sophisticated algorithms to analyze it means that new 
interventions are required to deliver new levels of insight. Previous democratization technologies were mostly 
capable of generating descriptive analytics insights about the past. Companies increasingly want to generate 
predictive models that provide insights about what might happen to their businesses in the future and prescriptive 
analytics that guide employees and customers to take actions that drive business results. Achieving these goals 
requires a level of statistical and data science sophistication that is still relatively rare within organizations, 
and that limits the number of useful insights that a company can produce.

The role of automated machine learning
Or at least it used to be a limiting factor. Predictive analytics—which is the same as more straightforward forms of 
statistical machine learning—can now be performed largely on an automated basis. Many of the key tasks required for 
machine learning—including data preparation, “feature engineering” or variable transformation, trying out different 
algorithm types, creation of program code or APIs for model deployment, and even creation of explanations of what 
factors are particularly important in a model—can increasingly be done by machines. Automated machine learning 
software is now available from AI-oriented firms such as Google, established analytics firms such as SAS, 
and startups such as DataRobot and H2O.ai.

Automated machine learning (often called AutoML) can certainly enhance the work of professional analysts and data 
scientists by automating workflow and dramatically increasing the speed with which a variety of overall hypotheses 
and individual model attributes can be tested. The rise of analytics and big data has led to many new or rediscovered 
algorithms. Most statistical analyses in the past relied heavily on linear regression analysis. More recently, 
logistic regression has become much more popular for making predictions of binary outcomes that are frequently used 
to drive day-to-day business activities. Now, a wide range of algorithms is available to the machine learning 
modeler. Data and algorithms are expanding rapidly, but human capabilities—even those of quantitative 
professionals—are not. AutoML is a way to enhance the productivity and effectiveness of even the best-trained 
analytical professional or data scientist.

At a large US property and casualty insurance company, for example, modeling productivity for data scientists was the 
primary objective in adopting AutoML. Thus far, notes the head of data science support, “It has been a very helpful 
throughput tool.” The insurance giant uses AutoML to get a quick reading on the ROI of alternative machine learning 
projects. “We get some data, turn DataRobot (an AutoML tool from a Boston-based startup) loose on it, and see what 
the prediction accuracy is for the model. It’s so quick that we can figure out the value of an analysis without 
taking a lot of time to assess it,” notes the manager. The company can learn what the key parameters of the model 
are, what algorithm is best-suited to the problem, and what the likely ceiling is on model accuracy. If it seems to 
be a promising analysis, the company will take it further—typically using nonautomated machine learning tools—and 
perhaps put it into production.

At Sumitomo Mitsui Card Company (SMCC), the largest credit card company in Japan, AutoML has been applied both to 
risk modeling and customer insight/marketing applications. In the risk modeling area, some analysts and data 
scientists were doing machine learning manually, but it could take up to half a year to build and validate a model. 
The use of AutoML cut that time to hours or a few days. Hiroki Shiraishi, who leads a group providing machine 
learning infrastructure to SMCC’s business units, notes that the company wanted to accelerate the process of 
analyzing credit card data, and there were not enough skilled analysts to meet the need. Therefore, increasing 
modeling productivity was a key objective.

Democratizing machine learning beyond data scientists
The greatest benefits in expanding insights, however, can come from broadening the population that can perform 
sophisticated machine learning analyses.1 Data scientists are typically difficult to hire and retain, and can be a 
limiting factor to insight generation even with greater productivity. In addition, business analysts with only 
moderate quantitative skills often understand the business and customer needs better than many data scientists. For 
these reasons, companies are attempting to expand the population of users of machine learning beyond data scientists. 
While some AutoML tools, such as Google’s Cloud AutoML and H2O.ai’s Driverless AI, are more oriented to more 
traditional data scientists (that is, individuals with PhDs in statistics and/or computer science), there are several 
platforms (such as DataRobot’s AutoML tools) that are oriented to both data scientists and quantitatively oriented 
business analysts.

For example, at 84.51, a subsidiary of Kroger that performs sophisticated data and analytics work for the grocer, 
the initial focus for AutoML was improving the productivity of data scientists. But the group has also used the 
automated tools to expand the number of people who can do machine learning. 84.51 has been growing its data science 
function to meet demand for modeling and analytics to solve complex business problems. It has been a challenge to 
find data scientists with the array of skills needed to work with business partners to engineer solutions and to 
develop and deploy models using current best methods. 84.51° employs tools such as DataRobot to “expand the bench.” 
Some experienced data scientists were concerned that they were moving to a world in which knowledge of algorithms and 
methods had no currency—a common issue with AutoML—but the company’s leaders emphasized that the new tools empowered 
people to get things done more efficiently, and there is now no pushback. 84.51 now regularly hires 
“insights”-focused data scientists—people who don’t have as much experience with machine learning, but who are 
skilled at communicating and presenting results, and who have high business acumen. Aided by AutoML, a substantial 
number of use cases and steps within traditional model development (such as use case identification and exploratory 
analyses) fit within their capabilities.

There is an even stronger focus on expanding the user base with AutoML at Royal Bank of Canada (RBC). It is investing 
in artificial intelligence and machine learning, currently employing over 200 data scientists working across the 
bank. Samer Nusier, the bank’s director of portfolio management and credit strategy, explained that many of the 
bank’s serious data scientists prefer to develop and tune their models using traditional methods. He, however, 
is an advocate of the “citizen data scientist” supported by AutoML. He notes that of the three traditional data 
science skills—math, computer science, and business domain knowledge—the math and computer science work are 
increasingly being done by tools like AutoML. When business analysts who understand the data and customer behavior 
create the models, they can be as useful as models created by data scientists. “It gives them superpowers,” he notes. 
Nusier feels that “purple people”—those who understand both some analytics and are business experts—can be equally 
valuable if supported by AutoML.

The proliferation of roles that can perform advanced analytics means that companies will need to clarify who does 
what and establish a governance model that balances capabilities, benefits, and risks. It probably wouldn’t be 
feasible, for example—at least at the moment—for a business analyst to employ a deep learning neural network model 
for image or speech recognition. Providing secure access to the volumes of appropriately cleansed and frequently 
updated data required for analyses is often another initial step. However, for straightforward machine learning 
models involving regression-oriented tools, there may no longer be any need to employ a data scientist. Automated 
machine learning tools, which will undoubtedly continue to advance in capability, can make possible the generation of 
advanced analytical insights at a much faster and broader level than ever before. The ability of an organization to 
take advantage of the curiosity, talent, and ingenuity at all levels of the company to increase performance is the 
underlying business driver and will be a central tenet to establishing and maintaining a competitive advantage going 
forward.
"""

article8 = """
Data ecosystems
How third-party information can enhance data analytics

Companies are increasingly seeking better insights by tapping into third-party data. Outside data can bring lots of 
opportunity, but using it effectively can be challenging.

MASTERING data analytics is how companies avoid flying blind. Increasingly, this requires tapping into data from 
outside an organization's four walls. Growing numbers of companies are doing so in pursuit of an analytical edge. But 
using third-party data sources effectively can be very challenging. To boost the business value of their companies’ 
analytics efforts, leaders can adopt key practices to navigate the complexity of third-party data.

Signals
In a recent survey, nearly half of companies reported using external data in their analytics activities.1
Organizations in industries including financial services, logistics, technology, health care, retail, and the public 
sector are using external data to gain new insights that can help increase efficiency and revenue.2
In another survey, 92 percent of data analytics professionals said their firms needed to increase use of external 
data sources; 54 percent said their companies plan to increase spending on it.3
Half of analytics professionals surveyed recently reported that their companies are selling their own data.4
To gain external data or services based on that data, companies in recent years have been executing mergers, 
acquisitions, and commercial partnerships.5
Mining external data for insights is increasingly important
Learn more
Explore the Signals for Strategists collection

Subscribe to receive related content

Companies know they can gain valuable insights by analyzing the data they generate from their operations. But 
internally generated information can leave gaps, and companies are increasingly moving to incorporate new, 
nontraditional, and external sources of data into their analyses. This data can include almost anything, 
from historical demographic and weather data to satellite imagery and private company information.

Companies increasingly operate as part of networks consisting of business partners such as suppliers, resellers, 
channel partners, regulators, and other stakeholders. These networks are often globally distributed and potentially 
affected by economic, political, and/or environmental factors. Analyzing external data can help companies see risks 
and opportunities that they would miss with inputs limited to data generated from internal operations, customers, 
and first-tier suppliers. Analyzing external data can illuminate how factors such as shifting consumer behaviors, 
competitor initiatives, or geopolitical events can affect a business.

As most business and technology professionals know, the volume of data being created, shared, and stored is 
increasing at an exponential pace. According to one study, the data stored in data centers will nearly quintuple by 
2021 to reach 1.3 zettabytes globally by 2021.6 (One zettabyte is equivalent to one trillion gigabytes.) Along with 
the volume of data available, the potential value of analyzing this data grows bigger by the day.

It’s not surprising, then, that companies on the leading edge of data and analytics are more likely to make use of 
external data. An MIT Sloan Management Review report published last year found that the companies making the most 
innovative use of data and analytics were more likely than others to leverage more external data sources, 
including social, mobile, and publicly available data.7 A different study found that faster-growing companies were 
more likely to be planning to expand their ability to source external data than companies with lower growth rates.8

How to get value from external data
External data sources are helping businesses personalize marketing offers, improve HR decisions, gain new revenue 
streams by launching new products or services, enhance risk visibility and mitigation, and better anticipate shifts 
in demand for their products and services. For instance, a major semiconductor manufacturer used third-party data to 
build models that could predict the best types of customers to target in marketing campaigns. This external data 
helped train the models to identify potential targets that fit similar profiles to the company’s most engaged 
customers. These “lookalike” models helped the organization optimize marketing spend and reduced a major campaign’s 
cost-per-engagement by 75 percent.9

There are numerous other examples of analytics programs generating value with external data. Several startups monitor 
social networking data to try to predict patterns of external job-seeking behavior and retention risk; they claim 
their data is more predictive of an employee’s likelihood of leaving than any internal data available. Others use 
geolocation and weather data to predict crop yields, helping farmers optimize their use of fertilizer. Retailers are 
using economic data and forecasts, data from suppliers, and geolocation data to better predict demand and reduce 
stockouts. Some companies are using satellite imagery to estimate mall traffic and predict retail sales; others are 
using aerial imagery to estimate oil inventories to better underwrite loans to refiners. (For select examples, 
see figure 1.)

How companies have used external data

The challenges of using external data
Access to external data is getting easier in some ways, but it can still be daunting. Organizations report a wide 
variety of business and technical challenges in deriving insights from external data.15 (Figure 2 summarizes some of 
these challenges.) Among the business challenges are the size and complexity of the data-provider market, 
which can make it hard to identify the right data sources and partners. Negotiating acquisition of data can be 
arduous, depending on factors such as whether ongoing access to data is needed for refreshing machine learning 
models, usage restrictions, whether the vendor wants a share of revenue gained from the data, and liability if the 
data proves to be inaccurate or tainted. This process can involve lengthy risk and legal reviews of vendor contracts 
and licensing agreements. The ongoing management of a growing roster of data-sharing relationships and partnerships 
can be taxing as well.

Challenges with using external data

The technical challenges include fundamentals such as assessing data quality and accuracy: A variety of studies have 
demonstrated that third-party data can be riddled with inaccuracies.16 There can also be inconsistencies between 
external and internal data to resolve before performing an analysis. Data preprocessing such as cleansing and 
formatting it for analysis is time-consuming. Some estimates suggest that this can account for 80 percent of the 
effort in data analysis projects. And securely storing and cataloging data in an easily accessible manner can require 
updating information management processes and capabilities designed to handle only internal data. The longer it takes 
to work through these challenges, the less time available to react to market trends and external events with agility.

Tap into a data ecosystem
Research suggests that most companies haven’t yet developed the capabilities necessary to use external data 
effectively. To close this gap, companies may find it helpful to think of themselves as participants in a data 
ecosystem, which some have defined as a network of actors that directly or indirectly consume, produce, or provide 
data and other related resources.17

To be good at using external data means being competent in identifying, evaluating, procuring, and preparing external 
data in a consistent and timely manner. Companies will need a continuous process for identifying, engaging with, 
and evaluating new external data sources and partners and, when appropriate, integrating these data sources into 
analytics processes or product offerings. Maximizing the value of external data often requires integrating it with 
internal data for a more insightful analysis.

Companies may find it valuable to form a cross-functional group that acts as the organization’s interface to the 
broader data ecosystem. This group could draw on competencies from multiple areas—including product management, 
business analysis, data science, legal, and procurement—to address both the organizational and technical challenges 
mentioned above. Some organizations have created special roles charged with scanning the third-party data market and 
matching business requests with relevant sources, a role Gartner has described as “data curator.”18 Curators can help 
companies quickly identify and assess data sources matched to business needs, while reviewing external data sets for 
quality and accuracy using consistent evaluation processes. Companies that have an effective tech-scouting capability 
may look at that group for inspiration.

A variety of ways to connect to the ecosystem
Organizations looking to connect to a data ecosystem can turn to a wide and growing variety of data and insights 
providers. Gartner Group categorizes data services, for instance, by the level of insight they provide:19

Simple data services. Data brokers collect data from multiple sources and offer it in collected and conditioned form. 
The data is used as additional input to a decision process by a person, an application system, or a device in an IoT 
ecosystem.

Smart data services. Data is enhanced by applying analytical rules and calculations. The results often take the form 
of scores or the tagging of objects, as in services from marketing data providers and credit ratings agencies.

Adaptive data services. Customers submit data pertaining to specific analytical requests. Providers combine that data 
with data from other sources.

There are other ways to segment this dynamic marketplace as well. For instance, some providers specialize in serving 
industry sectors such as hedge funds or health care providers. In addition, consulting and systems integration 
services providers are reacting to client demand for new insights from external data by leveraging publicly available 
data or information from third-party data partners. These providers then integrate those sources with clients’ 
internal data and perform custom analysis.

The new data frontier
Companies are making growing use of data from third parties. To get more value from their data analytics efforts, 
companies should consider enhancing their ability to identify, evaluate, and contract for new external data through a 
data ecosystem management program under a chief data office that links to business, IT, and legal teams. The pressure 
on companies to innovate and to improve the efficiency and effectiveness of their operations is unrelenting; they 
cannot afford to relent in their pursuit of insights to help them do so. For many companies, effective use of 
external data is a critical new frontier.
"""

article9 = """
Meeting the future: Dynamic risk management for uncertain times

The world is changing in fundamental ways, leading to dramatic shifts in the landscape of risks faced by businesses.
DOWNLOADS
Open interactive popup
 Article (9 pages)
Beyond the profound health and economic uncertainty of our current moment, catastrophic events are expected to occur more frequently in the future. The digital revolution, climate change, stakeholder expectations, and geopolitical risk will play major roles.

  22:46
Audio
Listen to the article
The digital revolution has increased the availability of data, degree of connectivity, and speed at which decisions are made. Those changes offer transformational promise but also come with the potential for large-scale failure and security breaches, together with a rapid cascading of consequences. At the same time, fueled by digital connectivity and social media, reputational damage can spark and spread quickly.

The changing climate presents massive structural shifts to companies’ risk-return profiles, which will accelerate in a nonlinear fashion. Companies need to navigate concerns for their immediate bottom lines along with pressures from governments, investors, and society at large. All that, and natural disasters, too, are growing more frequent and severe.

MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
Stakeholder expectations for corporate behavior are higher than ever. Firms are expected to act lawfully but also with a sense of social responsibility. Consumers expect companies to take a stand on social issues, such as those fueling the #MeToo and Black Lives Matter movements. Employees are increasingly vocal about company policies and actions. Regulator and government attention is reflecting societal concerns in areas ranging from data privacy to climate.

An uncertain geopolitical future provides the backdrop for such pressures. The world is more interconnected than ever before, from supply chains to travel to the flow of information. But those ties are under threat, and most companies have not designed robust roles within the global system that would allow them to keep functioning smoothly if connections were abruptly cut.

Companies require dynamic and flexible risk management to navigate an unpredictable future in which change comes quickly. The level of risk-management maturity varies across industries and across companies. In general, banks have the most mature approach, followed by companies in industries in which safety is paramount, including oil and gas, advanced manufacturing, and pharmaceuticals. However, we believe that nearly all organizations need to refresh and strengthen their approach to risk management to be better prepared for the next normal. The following discussion describes the core of dynamic risk management and outlines actions companies can take to build it.

Nearly all organizations need to refresh and strengthen their approach to risk management to be better prepared for the next normal.

The core of dynamic risk management
Dynamic risk management has three core component activities: detecting potential new risks and weaknesses in controls, determining the appetite for risk taking, and deciding on the appropriate risk-management approach (Exhibit 1).

Exhibit 1

Detecting risks and control weaknesses
Institutions need both to predict new threats and to detect changes in existing ones. Today, many companies maintain a static and formulaic view of risks, with limited linkages to business decision making. Some of these same companies were caught flat footed by the COVID-19 pandemic.

In the future, companies will require hyperdynamic identification and prioritization of risks to keep pace with the changing environment. They will need to anticipate, assess, and observe threats based on disparate internal and external data points. Dynamic risk management will require companies to answer the following three questions:

How will the risk play out over time? Some risks are slow moving, while others can change and escalate rapidly. Independent of speed, risks can be either cyclical and mean reverting or structural and permanent. Historically, most firms have focused on managing cyclical, mean-reverting risks, like credit risk, that go up and down with macroeconomic cycles. Historically, the fundamental long-term economics of business lines have held firm, requiring only tweaks through the cycle. Credit risk in financial services is an example of such a risk. However, the traditional principles of trajectory and cyclicality of risks are increasingly becoming less relevant. The global economic shock caused by the COVID-19 pandemic has demonstrated that many companies were not prepared for events with profound and long-lasting impact that could fundamentally change how business is conducted.
Are we prepared to respond to systemic risks? In today’s world, risk impact can go well beyond next quarter’s financial statements to have longer-term reputational or regulatory consequence. Institutions must also consider whether the event triggering the risk has broad implications for their industry, the economy, and society at large—and what that means to them. The COVID-19 pandemic has had direct impact on most companies but has also meaningfully shifted the global economy and societal terrain. Companies should consider whether they have the controls, mitigants, and response plans in place to account for worst-case-scenario, systemic risks. For example, as companies house more personal data, the risks associated with data breaches become more systemic, with the potential to impact millions of customers globally. These firms need to consider proactively how to protect against and react to such breaches, including by working with external stakeholders, such as customers, law-enforcement agencies, and regulators.
What new risks lurk in the future? Companies will need to cast nets wide enough to detect new and emerging risks before they happen. Traditional risk-identification approaches based on ex post facto reviews and assessments will not suffice. Most institutions have not had historical losses linked to climate change, and many have not encountered significant reputational blowback from being on the wrong side of a social issue. Institutions will need to work across business and functional divisions to maintain forward-looking, comprehensive taxonomies of the fundamental drivers of their risks. To get a real-time view of those drivers, companies should look to internal performance metrics, external indicators, and qualitative views of what business leaders see in their day-to-day work. Scenario-based approaches and premortems also play a critical role by letting leaders play out what might go wrong before it does.
Determining risk appetite
Companies need a systematic way to decide which risks to take and which to avoid. Today, many institutions think about their appetite for risk in purely static, financial terms. They can fall into the simultaneous traps of being both inflexible and imprudent. For example, companies that do not take sufficient risk in innovating can lose out to more nimble competitors. But at the same time, companies that focus on purely financial metrics can unwittingly take risks—for example, with their reputation by continuing a profitable business process that runs counter to societal expectation.

In the future, companies will need to set appetites for risk that align with values, strategies, capabilities, and the competitive environment at any given time. Effective enterprise risk management will help them dynamically delimit risk taking, directly translating financial and nonfinancial principles and metrics into a concrete view of what the firm will and will not do at any given time. Companies will need to be able to answer the following three questions:

How much risk should we take? Rapid changes can quickly uproot companies’ risk profiles. They will need to adjust their risk appetites to accommodate shifting customer behaviors, digital capabilities, competitive landscapes, and global trends. For example, many companies that categorically refused to use the cloud five years ago are migrating to cloud-based storage and software solutions today, driven by improved technology and security. Geopolitical instability has the potential to increase counterparty and currency risk considerations for the travel and infrastructure industries when considering engineering, procurement, and construction contracts for megaprojects lasting several years. The COVID-19 pandemic has sparked pharmaceutical companies to consider afresh which risks they are willing to take to develop and produce treatments quickly.
Should we avoid any risks entirely? Companies will want to draw some clear lines in the sand: no criminality; no sexual harassment of employees. But for many risks, the lines are not clear, and each company will need a nuanced perspective built on a strong, objective fact base. For example, will risk drivers such as climate change render risks in certain businesses fully untenable (for example, developing real estate in certain coastal regions)? Or should the reputational risk of being caught in the middle of highly charged environmental and social-responsibility issues drive a company out of certain business segments altogether (for example, in the way some retailers made the decision to stop selling guns)? Companies will need to develop views on such questions and update them continuously as their environments and corresponding fact bases evolves.
Does our risk appetite adequately reflect our control effectiveness? Companies are more comfortable taking the risks for which they have strong controls. But the increased threat of new and severe nonfinancial risks challenges status quo assumptions about control effectiveness. For example, many businesses have relied on automation to speed up processes, lower costs, and reduce manual errors. At the same time, the risks of large-scale breaches and violations of data privacy have increased dramatically, heightening during the COVID-19 crisis as digitization accelerates substantially across many industries. With less risk of manual errors but greater risk of large-scale failures, institutions will need to adjust their risk appetites and associated controls to reflect evolving risk profiles.
Deciding on a risk-management approach
Firms need to decide on how to respond as they detect new risks or control weaknesses. Today many rely on linear, committee-based governance processes to make decisions about risk taking, slowing their ability to act.

In the next normal, however, institutions will need to make risk decisions rapidly and flexibly, laying out and executing responses, whether immediate or prolonged, about how to avoid, control, or accept each risk. The decisions should actively engage leaders from across an organization to determine the mitigation and response efforts that have worked well in the past, as well as those that have not. In that way, the organization can develop the ways it manages risks in today’s world. Companies will have to be able to answer the following questions:

How should we mitigate the risks we are taking? Historically, many companies have relied heavily on manual controls and on human assessments of control effectiveness. That approach can generate excess, costly layers of controls in some areas while leaving gaps or insufficient controls in others. Today, the art of the possible in defending against adverse outcomes is rapidly evolving. Automated control systems are built into processes and detect anomalies in real time. Behavioral nudges influence people to act in the right ways. Controls guided by advanced analytics simultaneously guard against risks and minimize false-positive results.
How would we respond if a risk event or control breakdown occurs? In the event of a major control breakdown, companies need to be able to switch quickly to crisis-response mode, guided by an established playbook of actions. Most companies have done little to prepare for crises, seemingly taking an attitude that “it won’t happen here.” However, in the evolving world, firms will need to build crisis-preparedness capabilities systematically. As the COVID-19 crisis has demonstrated, companies with well-rehearsed approaches to managing through a crisis will be more resilient to shocks. Preparation should involve identifying the possible negative scenarios unique to an organization and the mitigating strategies to adopt before a crisis hits. That includes periodic simulations involving both senior management and the board. Companies should maintain and periodically update detailed crisis playbooks. Their strategies should include details on when and how to escalate issues, preselected crisis-leadership teams, resource plans, and road maps for communications and broader stakeholder stabilization.
How can we build true resilience? Resilient companies not only withstand threats, but they emerge stronger. Companies can learn from every actual risk event and control breakdown, honing risk processes and controls through a dynamic feedback loop. On a grander scale, firms also have the chance to turn the fall-out from true crises into competitive advantage, as the COVID-19 crisis is demonstrating. For example, some companies providing vacation rentals realized that they would need to do more than provide amenities and hygiene measures. They have started offering tailored customer experiences, including games, virtual cooking classes, and remote nature tours, built on an understanding of customer microsegments. These companies have started to differentiate themselves from their competitors and are positioned to emerge more resilient, even within a very hard-hit sector. Companies should prepare to ensure five types of resilience: financial, operational, organizational, reputational, and business-model resilience. Business-continuity, financial, and other plans can provide buffers against shock. But true resilience also stems from a diversity of skills and experience, innovation, creative problem solving, and the basic psychological safety that enables peak performance. Those characteristics are helpful in good times and indispensable when quick, collaborative adaptation is needed for an institution to thrive.
Five actions to build dynamic risk management
Today, many firms see enterprise risk management as a dreary necessity but hardly a source of dynamism or competitive advantage. It can suffer from being static, siloed, and separate from the business. But dynamic and integrated risk management, which includes the ability to detect risks, determine appetite, and decide on action in real time, is growing ever more critical. Leaders can take five actions to establish the necessary capabilities (Exhibit 2).

Exhibit 2

Dynamic and integrated risk management, which includes the ability to detect risks, determine appetite, and decide on action in real time, is growing ever more critical.

1. Reset the aspiration for risk management
To meet the needs of the future, companies need to elevate risk management from mere prevention and mitigation to dynamic strategic enablement and value creation. This requires clear objectives, such as ensuring that efforts are focused on the risks that matter most, providing clarity about risk levels and risk appetite in a way that facilitates effective business decisions, and making sure that the organization is prepared to manage risks and adverse events.

In practice, risk managers should engage in a productive dialogue with business leaders to gain an in-depth understanding of how the business thinks about risk day to day and to share the risk capabilities they can bring. Businesses typically approach decisions with a reasonable risk-versus-return mindset but lack key information to do this effectively alone. For example, business units often do not have a full systematic understanding of the full range of risk drivers or a clear view of how a stressed environment could affect the company.

More broadly, businesses typically also lack an enterprise-wide view of how a risk might unfold. For example, climate risk may affect most aspects of some companies’ businesses, from the impact of physical climate risk on operational facilities and supply chains to market repricing of carbon emissions to shifts in market demand and competitive landscape. The COVID-19 pandemic has had a similarly cross-enterprise impact on nearly every company. It should be an objective of dynamic risk management to provide an enterprise view.

2. Establish agile risk-management practices
The increasingly volatile, uncertain, and dynamic risk environment will demand more agile risk management. Companies will need to tap into people with the right skills and knowledge in real time, convening cross-functional teams and authorizing them to make rapid decisions in running the business, innovating, and managing risk.

Building teams and decision bodies dynamically requires the ability to understand quickly the nature of the risk at hand, including its significance and how quickly it may play out. This helps determine who needs to be involved and how people should work together. One fintech company, for example, runs daily huddles to discuss customers, bringing together a cross-functional team of business and risk leaders and other subject-matter experts to review new customer complaints. This enables executives to review funnel metrics for the day side by side with customer complaints and helps them triage and remediate those complaints promptly, avoiding larger issues down the road.

Decisions themselves should receive appropriate transparency, but managers should not get bogged down in excessive bureaucracy. Companies can formulate a clear, principled view of what sorts of decisions require committee review versus execution by single responsible parties. In some cases, previously unforeseen issues and risks that have the potential to evolve rapidly may require special, fast-track decision-making mechanisms. One organization does regular crisis-preparedness exercises and has developed relevant playbooks that assign decision-making power if needed, depending on the type of issue.

3. Harness the power of data and analytics
Companies can embrace the digital revolution to improve risk management. Automation technologies can digitize transaction workflows end to end, reducing human error. Rich data streams from traditional sources, such as ratings agencies, and nontraditional sources, such as social media, provide an expanding and increasingly granular view of risk characteristics. Sophisticated algorithms enable better error detection, more accurate predictions, and microlevel segmentation.

One global pharmaceutical company adopted advanced analytics to help it prioritize clinical-trial sites for quality audits. The company used a model to identify higher-risk sites and the specific type of risk most likely to occur at each site. The company is now tightly integrating its analytics with its core risk-management processes, including risk-remediation and monitoring activities of its clinical operations and quality teams. The new approach identifies issues that would have gone undetected under its old manual process while also freeing 30 percent of its quality resources.

Another area in which advanced analytics can capture significant value is in the predictive detection of risk. One railway operator applied advanced analytics to predict major component failures. The company improved safety and reduced its total failure cost for rolling stock by 20 percent. Companies can also use natural-language processing to build real-time, digital dashboards of internal and market intelligence, enabling more effective risk detection, including in customer complaints, employee allegations, internal communications, and suspicious-activity reports.

4. Develop risk talent for the future
To meet the demands of the future, risk managers will need to develop new capabilities and expanded domain knowledge. Strong knowledge of how the business operates provides a critical foundation by supporting true understanding of the landscape of risk. This enables risk professionals to provide better oversight and more effective challenge while also acting as effective counselors and partners as their company navigates the risk landscape.

Risk managers will also need strong understanding of data, analytics, and technology, which are driving shifts in how most companies operate—a trend only accelerated by the COVID-19 crisis. This is true for how data and digital interfaces are affecting firm processes, how companies are employing artificial intelligence to support day-to-day decisions, and how the digital revolution is shaping risk management itself.

To put this all together, risk managers will need to develop agile capabilities and mindsets, allowing them to identify opportunities to convene stakeholders and contributors across functions rapidly and generate quick solutions. People will need the leadership and personal capabilities to tap into colleagues with the right skills and knowledge in real time.

5. Fortify risk culture
Risk culture refers to the mindsets and behavioral norms that determine how an organization identifies and manages risk. In moments of high uncertainty—such as those we are living through during the COVID-19 pandemic—risk culture is of exceptional importance. Companies cannot rely on reflexive muscles for predicting and controlling for risks. A good risk culture allows an organization to move with speed without breaking things. It is an organization’s best cross-cutting defense.

Beyond today’s travails, a strong risk culture is a critical element to institutional resilience in the face of any challenge. In our experience, those organizations that have developed a mature risk culture outperform peers through economic cycles and in the face of challenging external shocks. At the same time, companies with strong risk cultures are less likely to suffer from self-inflicted wounds in the form of operational mistakes or reputational difficulties and have more engaged and satisfied customers and employees.

Companies with strong risk cultures share several essential characteristics. Most important, true ownership and responsibility for risk culture sits with the front line, with executive-level accountability for cultural failings. To be truly lived, culture must be linked with the day-to-day business activities and outcomes of an institution. At the same time, someone needs to be responsible for coordinating the definition, measurement, reporting, and reinforcement of risk culture—for example, within a risk function, a COO organization, or HR. Without an enterprise-wide view and vocabulary, it is not possible to effect true, coordinated cultural change. Finally, attention to risk culture must be ongoing. Strong culture takes maintenance and requires reinforcement.

One fast-growing technology company announced a culture transformation as the CEO’s top priority. It selected 30 culture leaders from across the company to lead the effort. The initiative mobilized around one-fifth of its staff through workshops aimed at helping managers make risk-informed decisions and creating a new risk culture and mindset.

The world is facing both uncertainty and rapid change. For companies, risk levels are rising—as are the expectations of employees, customers, shareholders, governments, and society at large. Against this backdrop, we believe companies need to rethink their approach to risk management, to make it a dynamic source of competitive advantage.
"""

article10 = """
COVID-19: Implications for business

Our latest perspectives on the coronavirus outbreak, the twin threats to lives and livelihoods, and how organizations can prepare for the next normal.
COVID-19 and the great reset: Briefing note #33, November 25, 2020
DOWNLOADS
Open interactive popup

Special Report
COVID-19: Facts and Insights, October 30
 Full Report (129 pages)
 Article (3 pages)
What to make of the promising news on COVID-19 vaccines and new CEO insights on leading through the pandemic: here’s this week’s update.
The world has cheered recent announcements about COVID-19-vaccine candidates. Two are showing higher efficacy rates than many dared hope for. Higher efficacy provides greater benefit to any vaccinated individual and may help encourage uptake among some segments of the population. It also reduces the fraction of the population required to reach herd immunity (exhibit).

Exhibit

While that’s highly positive news, McKinsey research also finds that the new vaccines are likely to accelerate only slightly the timetable to the end of the pandemic. In the United States, normalcy is not likely until the second quarter of 2021, and herd immunity is not likely until the third quarter. In other words, the pandemic will not be vanquished soon, and businesses will continue to be challenged. See our latest briefing materials—129 pages to guide leaders through the difficulties of the next few months.

Leaders can also take inspiration and new ideas from their peers. This week, McKinsey experts spoke with five top executives to learn more about how they are leading through the pandemic. Amrita Ahuja, CFO of Square, spoke about the future of payments, including the potential for a cashless society and frictionless exchanges made possible by machine learning and even cryptocurrency. Jaime Augusto Zobel de Ayala, CEO of Ayala Corporation, talked about how the Philippines’ oldest conglomerate has taken on society’s pain points in housing, electricity, medicine, and water and his optimistic outlook for Asia. His main concern? The potential for geopolitics to create several sets of technological standards, producing inefficiencies and raising costs.

Telemedicine is one of the extraordinary growth stories of the pandemic. Annie Lamont, managing partner of Oak HC/FT, shared her insights on what it and other trends mean for healthcare investing. Lisa Weiland, CEO of the Massachusetts Port Authority, spoke with us about what it means to run a major transportation hub when passenger volumes are down by two-thirds. Airports are preparing for contactless journeys and other means to improve the customer experience. And Tim Welsh, vice chairman of U.S. Bank, explained how new skills are helping the nation’s fifth-largest bank better connect with its customers.

This year’s McKinsey Global Survey on Artificial Intelligence finds “no increase in AI adoption.” But the technology is far from stalled: one-fifth of companies are deriving at least 5 percent of their earnings from AI, and they plan to further their investments in this still-promising technology. And in the latest installment of McKinsey’s series with CNBC, senior partner Mary Meaney speaks with IBM CEO Arvind Krishna and Unilever chief HR officer Leena Nair to discuss how companies can organize for the next normal.

Finally, as the United States heads into a pandemic-altered Thanksgiving holiday, many are counting their blessings even harder than usual—and just as many are hoping for better days to come. A collection of articles and multimedia outlines McKinsey’s views on food security and the ways that food banks, distribution partners, philanthropic foundations, and the private sector can ensure that people across North America have reliable access to nutritious meals. Pradeep Prabhala leads the initiative and offers his thoughts in the New at McKinsey Blog.

Executives everywhere are thinking about the critical next months of the pandemic. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The Recovery Will Be Digital. The first two installments—a 172-page report on technology and data transformation and a 130-page report on the path to true transformation—are available now. Three more are coming as part of Our New Future, a multimedia series we created with CNBC.


 .


For the full set of our latest perspectives on COVID-19, download our briefing note and full briefing materials.
Download the briefing note
Download the full briefing materials
COVID-19 and the great reset: Briefing note #32, November 18, 2020
In the pandemic, capitalism’s adherents are reconsidering its recent history and its future direction. Two new reports can illuminate the path.
This week, McKinsey experts took a step back to consider the effects of the COVID-19 crisis on the economic system in which much of the world operates: capitalism. Two new reports offer complementary views. In “Rethinking the future of American capitalism,” James Manyika, Gary Pinkus, and Monique Tuin trace the extraordinary achievements of the American system and the work still to come, including the need to rectify unequal and increasingly disparate outcomes for people and places, increasing “superstar” effects, and declining investment in public goods.

“The case for stakeholder capitalism” considers the role of business in society—a role that companies should not resist, lest they “find themselves on the wrong side of history … and also at a competitive disadvantage.” Vivian Hunt, Bruce Simpson, and Yuito Yamada examine the rising expectations for business, detail five principles for companies to follow, and offer many practical insights as they take action. One tactic is simply to publish your targets: a Danish power company put forth a ten-year plan to switch from coal to renewables; they did it in nine years, while simultaneously increasing profits by 43 percent.

Employees may be the stakeholders that need the most attention. According to our latest research, almost a year into the crisis, employees—especially women, LGBTQ+ employees, people of color, and working parents—are crying out for more support (exhibit). Nearly all employers are aware of the challenges and have established polices to help, but they are finding it hard to execute their diversity, equity, and inclusion (DEI) strategies. Asking and answering a set of tough questions can help companies close the gap.

Exhibit

Our experts also considered the future of corporate training, an expensive and often ineffective activity—when it did succeed, it was through in-person, hands-on learning. The COVID-19 pandemic brought that to a halt, forcing companies to innovate. In our latest research, we chronicle the advances companies have made in the pandemic and the ways in which the new capabilities they have built have secured their competitive position.

Also this week, our industry researchers looked at data’s critical role in two transport industries, bulk and tanker shipping and airlines; considered the effects of the COVID-19 pandemic on supply chains in retail; and outlined our latest findings on consumer behavior, this time in Europe.

We have updated our comprehensive COVID-19 briefing materials. The new 129-page report is now available for download.

Executives everywhere are thinking about the potential for successful vaccines to deliver the next normal. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The recovery Will Be Digital. The first two installments—a 172-page report on technology and data transformation and a 130-page report on the path to true transformation—are available now. Three more are coming as part of Our New Future, a multimedia series we created with CNBC.


 .




COVID-19 and the great reset: Briefing note #31, November 11, 2020
A vaccine breakthrough and how companies are thinking about purpose: here’s the latest from McKinsey’s research.
This week saw some surprising news about a large COVID-19-vaccine trial: a leading candidate has an efficacy rate of about 90 percent. There’s a lot of green between this particular ball and the pocket, but the news was most welcome. COVID-19 vaccines have been a focus of our research, as seen in our July 2020 overview, which includes a full discussion of the key issues of manufacturing and distribution, and subsequent articles on the end of the pandemic, an optimistic scenario for the pandemic response in the United States, and the technology transfer that may be critical to beating the COVID-19 crisis.

With the end in sight, or at least in fuzzy focus, companies are thinking ahead. A critical challenge for companies in the postcrisis era will be articulating clear, meaningful, and authentic purposes. Some companies seem to have the answer: they know their reasons for being, communicate them easily to customers, and enjoy the results. Our new framework (exhibit) can help others think through these knotty issues.

Exhibit

Governments have not lost sight of their purpose, but fulfilling it has become much more difficult. The gap between incoming and outgoing funds may reach $30 trillion soon. Our latest research shows a particularly effective bridge for governments to consider: real estate. The public controls a vast amount of acreage, office space, and other assets, and governments can extract much more revenue from them without breaching the public trust. On a related note, as part of their purpose, many businesses will embrace sustainability; voluntary carbon markets can help them reach their goals.

Cost management may be the yin to purpose’s yang, but is no less essential. In our new survey of some 300 C-level executives, we look at the ways that the corporate center is evolving. Our latest observations find that many organizations are accelerating their cost-reduction targets, modifying their operating models on the fly, and redefining their functional priorities.

Our new regional research considers two large economies in Asia. China, the world’s growth engine for the past 25 years, has come back—in ways that may surprise you. Consumer behavior has changed, pockets of growth are shifting, and leadership and management practices are in flux; businesses that manufacture and sell in China must be alive to the changes. And in Australia, businesses would be wise to understand today’s more mindful consumers.

Finally, the McKinsey Podcast zeroed in this week on retail, where the talk was all about rapid revenue recovery. Say what? It’s true: winners are recognizing the shifts in consumer behavior, adjusting their offerings, and rebuilding their businesses. Successful companies have five traits in common.

Executives everywhere are thinking about the potential for successful vaccines to deliver the next normal. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The recovery Will Be Digital. The first two installments—a 172-page report on technology and data transformation and a 130-page report on the path to true transformation—are available now. Three more are coming as part of Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #30, November 4, 2020
Scenario planning and a new decision tool are helping executives cut through the murk of the pandemic’s many confusions.
Executives have noticed the striking rise in COVID-19 cases in many parts of the world, yet they remain positive—if a trifle more wary. In the October edition of our monthly survey of more than 2,000 leaders around the world, fewer executives than in September say that better economic conditions are on the way (exhibit). But the balance is still tilted toward a positive outlook, especially where profits and customer demand are concerned.

Exhibit

We also asked respondents to vote on which of McKinsey’s nine pandemic scenarios is most likely. As of October, they are solidly in favor of scenario A1 (a muted recovery) but also see B2 (a prolonged and insufficient recovery) as a scenario to consider. Unsure about the terminology? In a new interactive, we explain our scenarios, what executives are thinking, and how that thinking has changed over time.

MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
For decades, McKinsey has advocated for the advantages of scenario planning while also recognizing the ways the approach can fall short. The pandemic has illustrated both sides of the equation. Enter a new approach: parametric analysis, in the form of an “uncertainty cube.” Right now, the businesses facing the greatest uncertainty are limiting themselves to three or four macrolevel scenarios that provide general direction but not much detailed guidance. The uncertainty cube uses several macroeconomic and financing parameters to generate highly specific advice by which to steer such a business.

Operating models, too, have come under pressure at companies facing great uncertainty. What’s needed are new structures designed to cope with the unprecedented conditions of 2020 and beyond. One way forward may be to embark, at last, on a true transformation. A new video explains the logic.

This week, we were delighted to sit down with two executives cutting remarkable paths through the pandemic. Aneel Bhusri, co-CEO of Workday, reveals the secrets of life in the pandemic for the finance- and HR-software powerhouse. And Sir Mark Lowcock, under-secretary-general for humanitarian affairs at the United Nations, explains the panoply of effects of the crisis on the United Nations and its missions.

Also this week, McKinsey experts assessed the potential for India’s manufacturing sector to deliver much-needed growth and jobs; reviewed the impact of the pandemic on the global petrochemical industry; and considered the moves that South Africa’s insurers can make to thrive in the long term.

Finally, in the pandemic, many of us are spending more time at home with our families. Children are naturally curious about what parents do—and it isn’t always easy to explain. At McKinsey, we took a crack at it and developed a new explainer for the younger crowd. Hint: it involves fishing.

Executives everywhere are thinking through the contours of the next normal. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The Recovery Will Be Digital. The first two installments—a 172-page report on technology and data transformation, and a 130-page report on the path to true transformation—are available now. Three more are coming as part of Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #29, October 28, 2020
New views on the postpandemic futures of six sectors.
The old joke has it that nostalgia isn’t what it used to be. As the unrelenting COVID-19 pandemic rolls on, the future isn’t what it used to be, either. What used to be a simple idea now comes freighted with caveats, assumptions, and speculations. In the spirit of illumination, McKinsey researchers this week took a look at how things might develop in six sectors beyond the next few weeks.

The auto industry is one of the world’s largest and has been devastated by the pandemic: sales may drop by 20 to 30 percent in 2020, and we estimate that profits will fall by $100 billion. But automakers can respond. One example: software-subscription services, which enable people to pay for programs that unlock features from heated seating to full self-driving capabilities, allow dealerships to develop a better relationship with consumers while offering drivers additional flexibility and customization.

The US restaurant industry has given many iconic brands to the rest of the world. But today, the sector is in trouble. In our latest podcast, we review the industry’s predicament, which we explored earlier in some depth, and assess the innovative solutions that companies are devising. Takeout and delivery are here to stay, and restaurants are working to make those experiences better. Menus also need a rethink. People don’t order sides, appetizers, and desserts as frequently when they’re ordering for delivery—but as leaders know, those items are often the difference between profit and loss.

For banks, the pandemic has changed everything. Risk-management teams are running hard to catch up with cascades of credit risk, among other challenges. Down the line, we expect that automated underwriting will take hold for retail and small-business customers and will both reduce losses and save costs.

Asian insurers are looking at a more consolidated future. Our new report argues that insurtechs’ new, pandemic-oriented products and digital capabilities—not least the ability to reach millions of customers within a few months—mean that a programmatic approach to M&A is the surest strategy to overcome the industry’s structural weaknesses.

We recently hosted a panel discussion with Shobana Kamineni, executive vice chairperson of India’s Apollo Hospitals, to discuss the evolving nature of healthcare at scale. Apollo Hospitals comprises more than 7,000 physicians and 30,000 other healthcare professionals, and its app is downloaded about 30,000 times a day. Among the findings: public–private partnerships are working well and have the potential to influence the future of healthcare.

Finally, we consider five fundamental questions for US higher education, as colleges fashion their pandemic response. Also this week: a new survey of Europe’s small and medium-size businesses lays out the extent of the economic damage and owners’ muted outlook.

Executives everywhere are thinking through the contours of the next normal. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The Recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #28, October 21, 2020
Geopolitics is back. Business leaders need to think through the implications.
In the 1990s, adherents of Francis Fukuyama came to believe in the “end of history.” The COVID-19 pandemic and a host of other factors—such as climate change, cyberattacks, and terrorism—have helped history stage a resounding comeback. Now, according to Richard Haass, president of the Council on Foreign Relations, the trick for businesses is to adjust and hope that history is just back for a visit and not for revenge. In a discussion with McKinsey senior partner and cochair of the McKinsey Global Institute (MGI) James Manyika, Haass speaks about the return of geopolitics to the top of the CEO agenda. Supply chains are another critical focus of the renewed attention, as covered in MGI’s new report and partner Susan Lund’s comments in the Economist.

McKinsey research has documented the disproportionate effects of the crisis on ethnic minorities, both in the United States and elsewhere. Our latest report on the topic looks at the United Kingdom. Key findings: over the past two decades, every ethnic minority group has made progress, in both absolute terms and relative to the white majority, on a range of economic indicators (exhibit). But the COVID-19 crisis threatens that progress; not only do all ethnic-minority groups have higher age-adjusted COVID-19-related death rates than white people do, but Bangladeshis and Pakistanis, in particular, are concentrated in occupations that have been hard hit by furloughs and layoffs.

Exhibit

This week, our marketing experts zeroed in on B2B businesses and how they sell. The classic approach is person to person; think of pharma’s armies of “detailers.” However, the COVID-19 pandemic has moved almost all sales online, often to self-service digital platforms. Everyone seems to be happier with the new arrangements. Some 70 percent of buyers say they prefer digital interactions; sellers like the greater effectiveness. Videoconferences and live chats are helping companies seal the deal; traditional phone calls are now a last resort.

Also this week, our industry researchers examined the latest travel data from China to understand what it might mean for tourism and business travel elsewhere. They also considered the new challenges for innovation in consumer companies.

Executives everywhere are thinking through the contours of the next normal. Start with the McKinsey Download Hub to find McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The Recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #27, October 14, 2020
The stock market has many puzzled. Here’s our explanation.
In the middle of the deepest recession in memory, stock markets are reaching new highs. Why the disconnect? To understand the conundrum, McKinsey experts point to three factors. First, many investors still take a long-term perspective; they are looking ahead to the end of the pandemic. Another factor: five big-tech companies now make up 21 percent of the S&P 500, one of the world’s most-watched markets. And smaller, unlisted companies have absorbed a lot of the economic damage, such as the dramatic rise in unemployment. The overall stock market can do relatively well even when employment and GDP are severely depressed (exhibit).

Exhibit

Investors may also be focused on the vast differences in resilience at companies. We interviewed leaders at several UK companies that have done better than others during the crisis. What distinguishes them, in a word, is agility. From a common purpose to rapid decision making to empowered local teams, these companies found ways to respond quickly to COVID-19. A key finding: war-gaming for a no-deal Brexit built a solid foundation for supply-chain resilience.

A new podcast this week examined those same supply-chain issues, in the context of McKinsey Global Institute’s August 2020 report on risk and resilience. Experts Ed Barriball and Susan Lund explain the research finding that, on average, companies can expect a disruption to their production lines of one to two months—a very long time—every three-and-a-half to four years.

Another podcast lays out the path forward for the US retail industry; our experts explain what it means for the industry when so many categories are tilting toward online shopping. Short answers, from senior partner Becca Coggins: “we’re in the foothills of what omnichannel-driven convenience will look like” and “some big innovations will scale, now that consumer expectations have been reset.” In another report, we examined the same forces and their effect on Middle East and Africa retailers.

Also this week: McKinsey researchers examined the potential for medtech innovation, and a more productive future for insurers.

Executives everywhere are thinking through the contours of the next normal. Start with The McKinsey Download Hub, with McKinsey’s latest research, perspectives, and insights on the management issues that matter most, from leading through the COVID-19 crisis to managing risk and digitizing operations. Also consider our special collection The Next Normal: The Recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #26, October 7, 2020
As the effects of the pandemic intensify gender inequality, further threaten the economy, and raise hurdles for the health industry, companies’ actions now could see them through the crisis.
In the sixth year of our Women in the Workplace study, conducted in partnership with LeanIn.Org, we find that the effects of the COVID-19 crisis have exacerbated gender disparities and their implications for women at work, especially for mothers, female senior leaders, and Black women across America. In addition to being laid off and furloughed at higher rates than their male counterparts during the pandemic, women are—notably, for the first time in our research on the topic—considering downshifting their careers or leaving the workforce altogether at staggering rates.

The exodus might include as many as two million women. That would raise a significant barrier to achieving gender parity in leadership roles in years to come. People are thinking about leaving the workforce for a variety of reasons (exhibit). While many organizations are providing additional resources related to remote working and employee well-being, there is more to be done to meet employees’ needs for sustainable, flexible, and empathic work environments, especially for parents and caregivers.

Exhibit

Meanwhile, the global economic contractions resulting from the COVID-19 pandemic have far exceeded those of the Great Recession that ended in 2009 and have occurred at a much faster rate, hitting all sectors and many of the world’s largest employers. As companies plan for various outcomes in 2021, our research shows what companies seeking resilience can do today to achieve “escape velocity” from the crisis by focusing on EBITDA1 margins, revenue, and optionality.

An area where companies have already adjusted well is using technology to address changing work environments and to stay competitive. Our new global survey finds that organizations that are successfully responding to the crisis have deployed more advanced technologies, digital products, and tech talent to speed up innovation—and they expect most of these changes to outlast the pandemic.

Our research this week sheds light on two important issues facing healthcare providers. First, similarities in flu and COVID-19 symptoms could lead to a threefold spike in demand for COVID-19 testing as flu season in the Northern Hemisphere approaches. Maintaining sufficient capacity for testing and contact tracing will be critical in curbing further outbreaks and protecting high-risk groups. Second, the crisis has also led to a surgical backlog for elective procedures because of lack of hospital capacity, workforce shortages, and new safety protocols. Health systems will need to optimize current clinical operations to address the discrepancies in supply and demand.

This week we also explored how European marketing-and-sales leaders are navigating the effects of the pandemic, the domino effect for improving sales returns on investment, disruption that is reshaping construction-material distribution, and steps that distributors can take to stabilize operations and outperform competitors.

Executives everywhere are thinking through the contours of the next normal. Consider our special collection The Next Normal: The Recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 was edited by Dana Sand, an assistant managing editor in the Atlanta office.



COVID-19 and the great reset: Briefing note #25, September 30, 2020
Executives are more hopeful about the economy than they have been at any time so far during the COVID-19 crisis. Is an end to the pandemic at hand?
Six months after WHO declared COVID-19 a global pandemic, the responses to our latest McKinsey Global Survey suggest a positive shift in economic sentiment. More than half of all executives surveyed say economic conditions in their own countries will be better six months from now, while 30 percent say they will worsen (exhibit). That’s the smallest percentage of pessimists we’ve seen since the survey in April 2020.

Exhibit

Taking a cue from those executives, our researchers delved deep into the US situation, emerging with an understanding of what it will take to deliver an optimistic outcome. The case depends on the progress made to date—and the potential for more. We’ve learned much about the natural history and epidemiology of COVID-19. We’re developing better diagnostics, including rapid point-of-care tests, a few of which can be completed in about 15 minutes. Case management has improved. And pharmaceutical companies have turned out a remarkably robust pipeline of vaccine and therapeutic candidates. Put it all together, and an end to the pandemic is potentially within range.

Another new survey reveals the extent of the COVID-19 crisis’s disruption in working practices and behaviors. One-third of surveyed companies have accelerated the digitization of their supply chains, half have sped up the digitization of their customer channels, and two-thirds have moved faster to adopt artificial intelligence and automation. Many other workforce changes are also in progress.

Managers need to process these changes and many others, and come to grips with the long-term strategic-planning agenda. The essential question: what is the right way to think about 2021 and beyond? Should companies unbatten the hatches, or is it too soon?

For many families, it isn’t the workplace but the school that occupies the most attention. McKinsey’s latest look at education examines the variables that factor into decisions to reopen schools. Also this week, McKinsey researchers focused on cash management through the crisis and on the problems of budgeting in healthcare systems.

Executives everywhere are thinking through the contours of the next normal. Consider our special collection The Next Normal: The Recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #24, September 23, 2020
A potential end to the pandemic, a bright outlook for electric vehicles, and more.
In our latest public-health research, we assess the prospects for an end to the pandemic. Two standards must be met. In the United States and most other developed economies, herd immunity is most likely to be achieved in the third or fourth quarter of 2021. Key variables are the arrival, efficacy, and coverage of vaccines; we anticipate four scenarios (Exhibit 1). A return to normalcy might come sooner, possibly in the first or second quarter of 2021. Every day matters, for lives and livelihoods.

Exhibit 1

On the economic front, the COVID-19 crisis presents the greatest challenge in a decade for the auto sector. Global sales of light vehicles in 2020 might decline 20 to 25 percent from prepandemic forecasts. In the hardest-hit countries, sales could fall by 45 percent. Electric vehicles (EVs) have not been spared. But our new research finds that EV sales may come back quickly in the next couple of years, especially in Asia and Europe, for a few reasons (Exhibit 2).

Exhibit 2

The crisis has also set in motion a number of trends in mobility that will affect EVs, internal-combustion engines, and all the other ways that people get around. A second report considers five of these trends. One critical finding: as lives become hyperlocal, modes of transport will change. We expect a drastic decrease in private-car usage in some major European cities but only a slight decline in North America. Greater China will become even more reliant on public transit and rail as some drivers are coaxed out of their cars.

Our private-equity research teams chipped in a comprehensive look at the effects of the crisis on sectors, and what those mean for portfolio companies and firm strategy. Several analyses offer insights; one on debt-service coverage ratios finds that companies in industrial equipment and logistics are among the most vulnerable, along with real estate, travel, and retail. Telecom companies are better situated, as their business has been only mildly disrupted.

Also this week: McKinsey has long been committed to research into gender equality. In 2015, UN signatories set an ambitious target of achieving gender equality and empowering women and girls everywhere. Five years on, we assess the scant progress to date, blunted by COVID-19, and offer ten things that everyone needs to know about gender equality.

Finally, our researchers offer new views on the post-COVID-19 future of the global travel industry, graphic-paper producers, and the global gold industry. And, with disruption everywhere, people miss their old lives. That’s a problem for managers, who can take three practical steps to help people process their grief.

Executives everywhere are thinking through the contours of the next normal. Consider our special collection, The Next Normal: The recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #23, September 16, 2020
Companies return to work after an unusual summer—and grapple with an uncertain future.
What now? Over the past six months, business leaders have reorganized supply chains, set up remote operations, and made tough financial decisions. The world anxiously awaits an effective COVID-19 vaccine that can be readily distributed. Until then, the priority is to reenergize organizations—to act rather than react. Even as the uncertainties of the COVID-19 crisis multiply, the goal must be to rebuild for the longer term. There are many ways to lead, but regardless of the type of business or geography, ten actions can form a path to emerge stronger from the crisis.

We start with an idea—that returning is a muscle that needs to be exercised, not a plan to be executed once or a date to be achieved. We go on to more specific considerations, such as the need to make big moves fast and to be willing to rethink entire portfolios, including where work gets done.

Those are four of the ten actions, and they make for a good starting point. But companies must adjust for the particulars of their industry. Healthcare companies might want to pay strict heed to six trends that are affecting their business. Most were under way before the crisis. But a crisis has a way of bringing things to a head: the coming months might be the best opportunity in memory for healthcare companies to pursue exponential innovation, which could create an additional $400 billion in value by 2025. And now is the time to claim the hundreds of billions of dollars that could be saved through productivity gains.

McKinsey’s healthcare researchers also took a close look at the US blood supply, which was fragile before the pandemic and is now critical. Businesses have a big role to play in the solution. Blood donors frequently cite convenience and social pressure as prompts. Virtual campaigns for blood drives can help blood centers quickly reach large audiences and steer them to the locations most convenient for them.

CFOs have a critical task too: for many, it’s budgeting season. Our new research finds that the financial-planning process for 2021 presents an opportunity to turn hard-earned lessons from the COVID-19 pandemic into an enduring exercise in linking strategy to value. And leaders across organizations need to consider the problems of unresolved grief—another issue that the pandemic has dragged into the spotlight.

Our industry research this week looked at fintech, where the news is not altogether bad, though fintech companies may have to find a detour on the road to profitability. We also considered M&A in pharma, a long-running trend that should continue. Companies are advised to make sure that three capabilities—competitive advantage, capacity, and conviction—are up to snuff before pursuing COVID-19-era mergers.

Finally, the pandemic has forced a reckoning for many between the profit motive and a company’s social purpose. A team of McKinsey editors recaps how we got from there to here, and suggests where we might go next.

As summer turns to fall in the Northern Hemisphere, executives are thinking through the contours of the next normal. Consider our special collection, The Next Normal: The recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC. 


 .



COVID-19 and the great reset: Briefing note #22, September 10, 2020
McKinsey research focuses on the postpandemic future of developing Asia.
No nation has escaped widespread disruption from the COVID-19 pandemic, but some have fared better than others. This week, McKinsey researchers examined the state of the recovery in some of the emerging Association of Southeast Asian Nations (ASEAN) countries—Indonesia, Malaysia, Philippines, Thailand, and Vietnam—that began the crisis at a disadvantage and have suffered disproportionate effects. Our new report explores a series of trends that the pandemic has caused or accelerated. Within each is a seed of recovery, but stakeholders must be prepared to reimagine their country’s economy in five areas: manufacturing hubs, green infrastructure, investments in digital, talent reskilling, and high-value food industries.

We also looked in detail at developments in two ASEAN countries. In Indonesia, the pandemic is still raging; case counts and fatalities are rising sharply. The first priority is to mitigate and contain the outbreak. But even amid the current hardship and profound uncertainty, Indonesia can reimagine and reform itself by increasing national resilience, accelerating economic transition, rebuilding the tourist sector, and enabling genuine change.

Vietnam, too, is contending with short-term challenges as it emerges from the pandemic, especially in tourism and manufacturing, two of the country’s strengths. For the long haul, our new report argues that one essential element of growth is renewable energy. As a country likely to be heavily affected by climate change, Vietnam could accelerate its journey toward a less carbon-intensive future. A new national energy plan is a good sign; now, the challenge is to execute it. (For Vietnam and many other countries, education is another important cog in the engine of growth. This week, we published a comprehensive report on a more equitable and resilient education system.)

Elsewhere in the region, our latest CEO interview, with Peter Harmer of Insurance Australia Group, reveals new insights into “the CEO moment” afforded by the crisis. Asked about crisis resilience, Harmer says, “You have to tether resilience to real beliefs. We have a deep commitment to our purpose, which is to make your world a safer place. Our purpose is the framework through which all our decisions are made.”

Also this week, a new McKinsey survey tapped the wisdom of hundreds of executives across a swath of industries on the need for speed (exhibit). Most expect significant change across ten of 12 dimensions; surprisingly, only a few expect change in their corporate purpose. (Perhaps they have already embraced Peter Harmer’s view.) And our industry researchers looked at the promise of digital services at B2B service companies and contemplated the troubled present—and potential future—for downstream oil and gas in North America.

Exhibit

Summer may be over in the Northern Hemisphere, but great reading knows no season. It’s not too late to catch our annual summer reading list and our special collection, The Next Normal: The recovery Will Be Digital, featuring a 172-page curated volume that you can download—the first of five edited collections that accompany Our New Future, a multimedia series we created with CNBC.


 .



Preventing future waves of COVID-19Preventing future waves of COVID-19: Briefing note #21, August 31, 2020
After seven months of responding to the pandemic, we have learned some things. Here are some of the key lessons and how to apply them.
By Sarun Charumilind, Matt Craven, Jessica Lamb, and Matt Wilson
When history books one day recount the COVID-19 pandemic of 2020, it may well be a tale of human ingenuity and adaptiveness. Although the novel coronavirus (SARS-CoV-2), the virus that causes COVID-19, has infected more than 24 million people and left more than 800,000 dead as of this writing, the early projections of mortality were much worse.

Fears of millions of deaths by June 2020 have proven wrong—not because the disease is less lethal than anticipated, but because those fears ignored the ability of people to learn and change behaviors. Pockets of resistance against wearing masks and complying with other measures notwithstanding, the global public-health response has saved millions of lives. Increasingly, countries are restarting more aspects of normal life while keeping case numbers tenuously in check.

Pockets of resistance against wearing masks and complying with other measures notwithstanding, the global public health response has saved millions of lives.

Yet the threat to lives and livelihoods persists. A COVID-19 vaccine may yet “save the world.” But even if one proves effective, it will be many months before we will have the capacity to vaccinate everyone—and there are new concerns about reinfection.2 Therapeutics such as dexamethasone and remdesivir appear to provide important benefits for those with severe cases but are not alone sufficient to stop deaths from COVID-19.

New therapies are possible but by no means guaranteed. Countries will very likely need to plan for almost another year during which public-health measures are their primary tools for saving lives. In the meantime, the world cannot be idle. Societies have been upended, causing unprecedented disruption to economies, education systems, and the day-to-day lives of people everywhere. And as we and others have argued, saving lives and opening societies is a false trade-off.

In that area, too, our ability to learn and adapt is proving dispositive. Countries that have successfully reduced their number of COVID-19 cases have generally been more successful at reopening their economies. For them, controlling the virus ultimately has come down to two things: understanding what to do and executing well. Both have been challenging at various points. For example, the evidence base for the population-wide use of masks only became compelling a few months into the pandemic response. In contrast, the importance of testing has been clear from the earliest days, but many countries have faced operational challenges in ramping up their capacity.

While there is much more to learn, this article summarizes what response leaders have discovered so far about what to do and how to do it. Every jurisdiction is doing some of these things; none of them are new for experts in infectious diseases. But we have tried to describe specific considerations for practitioners looking to adopt and adapt best practices to their management of the COVID-19 pandemic. Given the outsize role that businesses are taking in the crisis response in numerous countries, many of the ideas are as relevant to private-sector leaders as to those in the public sector. Interventions are divided into three categories—detecting disease, reducing the number of new cases, and limiting mortality—and can be tailored for specific populations and settings (Exhibit 1).

Exhibit 1

Detecting disease
The ability to detect cases of COVID-19 is a critical prerequisite for effective public-health programs. A comprehensive program might include traditional disease surveillance, cluster analysis to understand local patterns of transmission, and wastewater surveillance for early warning of disease hot spots.

Disease surveillance
An ability to collect, analyze, and interpret data is fundamental to the management of infectious diseases. While many people have grown familiar with epidemiological metrics such as test-positivity rates and case-fatality ratios, many countries and regions still rely on 20th-century surveillance systems. The biggest gaps are in data collection and integration: there is no shortage of data-crunching horsepower in the world, but everyone is forced to work from the same imperfect data sets. Even seven months into the COVID-19-pandemic response, there is a surprising level of disagreement about questions as basic as the true number of people who have been infected with SARS-CoV-2 and the number of deaths attributable to it. Continuing to expand testing, as described later in this article, is a big part of improving surveillance.

The best surveillance systems seamlessly combine data from traditional sources with newer data sets, such as anonymized mobility tracking—and do so in near real time. They provide a high level of detail and transparency around the characteristics and location of those infected while protecting individual privacy. And they improve over time as a design principle, incorporating new sources of data and improving quality to reduce friction in the response.

The best surveillance systems seamlessly combine data from traditional sources with newer data sets, such as anonymized mobility tracking—and do so in near real time.

Cluster analysis
The medical community has learned much about how COVID-19 is passed from person to person and therefore how to prevent transmission. But there is more to learn about the specific nature of transmission in particular geographies. The examination of chains of infectious-disease transmission, or cluster analysis, helps medical professionals understand how, when, where, and between whom transmission occurs.

Locally relevant information can focus public-health measures on steps that will make a difference and deemphasize those that won’t. A study of more than 3,000 cases across 61 clusters in Japan, for example, identified healthcare facilities and retirement centers as among the most important centers of transmission.3 Similarly, clusters in the United Kingdom have been identified around retirement homes, in hospitals, and in meatpacking factories—the latter also being a source of clusters in Germany.

Cluster analysis has revealed the importance and characteristics of so-called superspreaders (infected individuals who pass the disease to many others). A deeper understanding of transmission dynamics may allow some regions to move from broad-based interventions to targeted ones. It can also allow for more nuanced risk assessments, for example, to determine who can safely access senior-care facilities.

Wastewater surveillance
An important advance in surveillance capabilities came with the discovery that SARS-CoV-2 is present in the stool of infected people and is detectable even in highly diluted samples, such as municipal wastewater. Wastewater sampling, used for decades to monitor for polio, appears to detect viral increases of COVID-19 up to six days earlier than diagnostic tests of individuals do.4 While a number of locations, including Queensland in Australia, Ashkelon in Israel, and Boise in the United States,5 are piloting or using this approach to monitor for COVID-19, wastewater remains an underutilized tool globally.

The wastewater-surveillance approach is most applicable in low-prevalence settings where an increase in cases is more noticeable and testing of individuals might otherwise be limited. Ideally, public-health leaders would have the ability to work upstream when increases in viral concentration are detected—for example, from testing town sewers to determining which neighborhoods are the source of the virus.

Reducing the number of new cases
Preventing new cases of COVID-19 ultimately requires reducing the opportunity for infected individuals to pass the disease to others. That can be done by identifying and isolating those who have been infected or are at high risk, ensuring physical distance and airflow management, reducing the risk of the encounters that do happen, and reducing case migration from higher-prevalence areas. The basic tool kit for the reduction of new cases is well understood by experts and nonexperts alike. It includes canceling mass events, restricting capacity in social settings (particularly indoors or with large numbers of people), implementing confinement measures, and restricting internal movement (Exhibit 2). Those measures can be reinforced through effective behavior-change communication and focused implementation for high-risk groups or specific geographies. And since COVID-19 vaccines are likely to be approved eventually, leaders may want to start now in preparing to deploy one effectively. In this section, we highlight some second-order or less appreciated lessons from the pandemic response so far.

Exhibit 2

Identifying and isolating those infected
Widespread, accurate, efficiently managed testing and contact-tracing programs allow countries to isolate those who have or are at high risk of contracting COVID-19. Testing and tracing have played major roles in the successful response to various phases of the pandemic in a number of countries, including Austria, Iceland, New Zealand, and South Korea.

Despite the apparent simplicity of testing and tracing, practitioners learned the hard way through early missteps. Among their many lessons are the following:

Communicate clearly with the public about the appropriate uses and limitations of different types of tests, including antigen, molecular, and antibody testing.
Address supply-chain and logistical challenges to keep expanding testing access until most cases are being detected. Test-positivity rates above 5 percent suggest that too many cases are being missed.
Make use of pooled testing to boost capacity where needed, especially in low-prevalence settings. Combine testing for surveillance with testing for positive-case identification.
Accelerate testing turnaround time by ensuring that those performing tests are compensated based on speed and accuracy, not just volume. Accelerate the application of test results by integrating data platforms for testing with those for contact tracing, shortening the time to quarantine.
Staff enough personnel, as the core of contact-tracing programs is human-to-human conversation. Overinvest in community sensitization to the value of tracing and importance of contact quarantine. Digital contact-tracing tools with high adoption can also accelerate contact identification and shorten the time to quarantine.
Don’t expect contact tracing to work perfectly initially; take a data-centric approach to improving operations and effectiveness over time.
Recognize that isolating for ten to 14 days is onerous, especially for low-income individuals. Social services and options for out-of-home isolation, such as in converted hotels, can improve the effectiveness of quarantine and make it more tolerable.
Managing risk in encounters between people
COVID-19 is spread primarily from person to person, including from those not showing symptoms, through the air (either on droplets or by truly being airborne). Close proximity and poor airflow increase the risk of transmission, while the use of facial coverings decreases it. Specific considerations for risk reduction vary depending on risk, context, and other conditions.6
Limited evidence from US states suggests that mask mandates are correlated with greater reductions in new cases than mask recommendations are. Different masks offer varying levels of protection. N95 respirators fitted to users provide the greatest protection,7 protecting both the wearer and those around them. Supply constraints, cost, and user comfort mean that universal N95 use is not practical in many settings. Three-layer surgical masks provide the next greatest protection. Goggles and other eye protection may provide incremental protection to the wearer relative to a mask alone.8
Frequent hand washing and environmental cleaning reduce the transmission of COVID-19. However, the relative emphasis on environmental cleaning has decreased, as evidence suggests that transmission primarily occurs from person to person rather than via objects in the environment.

Reducing case migration
Across the world, countries are taking different approaches to restricting importation of COVID-19 cases. They range from complete bans on international travel to targeted bans on travel from locations with high caseloads to screening and quarantine requirements for arriving travelers. In some countries, including Australia and the United States, some of those measures also apply for travel within countries. In many cases, companies and other institutions are implementing their own policies beyond those required by governments. Measures that are based on consistent, easily understood criteria are more likely to maintain high levels of public buy-in and participation.

Changing behaviors
A successful response to the COVID-19 pandemic requires convincing large numbers of people to change their behaviors. Some countries have seen significant resistance to such changes, particularly those around physical-distancing measures and facial-covering mandates. A lack of trust in governments, information overload, and inconsistent messaging over time have all contributed to that opposition. Effective public-health communication can accelerate the adoption of new behaviors.

Effective communication includes segmenting populations based on the combination of channels, influencers, and messages most likely to resonate with individual groups. While there are positive examples from the response to the COVID-19 pandemic, the public-health community could learn more from experts in targeting and tailoring political and consumer-marketing messages. The influence model can help. It suggests that people are most likely to change behaviors when four elements are in place:

Understanding and conviction in what is being asked. “I believe that wearing a face mask will help protect me and my community from COVID-19.”
Reinforcement with formal mechanisms, which may include both ‘carrots’ and ‘sticks.’ “The grocery store has both a sign, which I can see as I approach, saying that masks are required and a greeter handing them out.”
Confidence and skill building in the new behavior. “I’ve worn a mask enough times that I’ve stopped worrying about looking silly.”
Role modeling the new behavior. “The mayor of my town and almost everyone around me are wearing masks. Those not doing so look like the exceptions.”
Applying insights from the influence model to COVID-19-related communications is an area in which collaboration might help. Many jurisdictions are enlisting the help of partners, celebrities, and influencers to amplify their messages. For example, in the United States, basketball star Stephen Curry asked questions of infectious-disease expert Anthony Fauci live on Instagram. That served to bring evidence-based public-health information to audiences less likely to access official sources.

Protecting vulnerable populations
The COVID-19 pandemic has a disproportionate impact on a number of vulnerable populations. Such groups include people whose age or health puts them at increased risk and those at greater risk because of socioeconomic factors (Exhibit 3). Communities with severe housing problems, unemployment rates, incarceration rates, poverty levels, and food insecurity suffer 1.4 to 4.0 times as many COVID-19-related deaths as other communities. Vulnerable populations are less likely to have access to healthcare in most countries and are more likely to have underlying health conditions.

Exhibit 3

In addition to measuring and tracking the impact of the COVID-19 pandemic on vulnerable populations, designing protective interventions requires identifying what makes those groups more vulnerable to infection. Approaches might include prioritizing access to testing, targeting communications, and providing additional support for quarantine and isolation. Interventions will likely need to be multipronged, since the most vulnerable communities are often vulnerable for multiple reasons. Furthermore, the stakeholders best positioned to implement interventions effectively will need resources, which would ideally be allocated proportionately to the outsize impact of COVID-19 infection on vulnerable communities.

Planning for a vaccine
It is reasonable to hope that Emergency Use Authorization (or its equivalent) may be granted to one or more COVID-19-vaccine candidates before the end of 2020.9 While vaccines will be valuable new tools, their approval will bring a whole new set of questions for leaders.

Planning now will increase the chance of a successful vaccine rollout. Those on point will need to monitor closely the technical characteristics of the most promising vaccine candidates. Such monitoring includes understanding the likely dosing regimen, potential efficacy, and side effects. From there, they will need to develop a clear, scenario-based strategy for prioritizing vaccine access, recognizing the range of potential vaccine outcomes and combinations available.

Every jurisdiction is likely to be vaccine-supply constrained in the short term, so agreeing on grounding principles in advance will make allocation decisions easier down the road. So will designing the end-to-end supply and delivery systems that will be needed. The plan should include systems for ensuring series completion in the case of a multidose vaccine and data systems for tracking those who have been vaccinated. It may include temporarily expanding the roles of medical practitioners—for example, by allowing those with lower levels of qualification to administer vaccines, after training, in uncomplicated cases.

Finally, planners may need to overinvest in addressing vaccine hesitancy in areas where surveys suggest it is a significant concern. Communications around vaccines will be both challenging and important given the likely complexity of information around efficacy, safety, and dosing across multiple vaccine candidates.

Limiting mortality
In addition to limiting case numbers, reducing the mortality associated with COVID-19 is a key element of the fight against the disease. Clinicians and health-system leaders have learned much about both the specific clinical management of COVID-19 and how to prepare health systems to manage surges in cases while maintaining essential services.

Health-system preparedness
In the early days of the COVID-19 pandemic, the world anxiously witnessed many countries’ health systems strain under the exponential onslaught of cases. Critical-care capacity was a bottleneck, given that one in five patients, initially, were dependent on ventilators. Healthcare supply chains, especially for personal protective equipment, were overwhelmed.

To create surge capacity, health systems and consumers ceased elective care—seemingly overnight. That resulted in an imbalance of capacity, with overloaded health systems in COVID-19 epicenters transformed into disaster-response hubs. In areas where the disease had not yet spread, care centers sat empty, waiting for an outbreak they were unsure would ever arrive.

We know now that health systems in any developed country should be able to anticipate, plan for, manage, and successfully navigate the pandemic adequately both for patients with COVID-19 and for patients with other diseases. Some require focused action, especially surge capacity, supply availability, workforce readiness, clinical-operations processes, structure for COVID-19-case governance, and financial resiliency.10
Use of therapeutics and clinical management
The search for effective therapies for COVID-19 has yielded two important advances, so far, but no breakthrough transformative enough to obviate the need to limit cases. Dexamethasone, an injected corticosteroid, was shown to reduce mortality by 35 percent in patients requiring mechanical ventilation and by 18 percent in those requiring oxygen only.11 Remdesivir has been shown to reduce recovery time by an average of four days.12
Both drugs emerged from the medical community’s initial focus on repurposing drugs that were already approved or in late-stage development for the treatment of other diseases. The focus is now shifting to new R&D. In the months ahead, additional evidence may support therapies based on other antivirals and monoclonal antibodies. In addition to specific therapeutics for COVID-19, there have been advances in the nonpharmaceutical management of the disease. For example, there is some evidence to support the use of “proning”—placing patients face down—to reduce the need for mechanical ventilation.13
Policy makers can continue to keep a close eye on both the evidence for new therapeutics and the standards of clinical practice. Over time and with further advances, strong health systems may succeed in reducing COVID-19-related mortality to the point at which the disease is far less feared.

Public-health measures to control the COVID-19 pandemic will be relevant for as long as its risk continues. Many countries and regions have risen to the challenge by combining multiple public-health measures that work for them, although almost all have some room to improve. As we consider what it will take to respond to current and future waves of COVID-19, we can take some comfort from the fact that far more is known about controlling SARS-CoV-2 than was understood seven months ago. It is up to all of us to learn, adapt, and apply those lessons effectively.

Download the article (PDF–435KB).

About the authors
Sarun Charumilind is a partner in McKinsey’s Philadelphia office, where Jessica Lamb is a partner; Matt Craven is a partner in the Silicon Valley office; and Matt Wilson is a senior partner in the New York office.

The authors wish to thank Damien Bruce, Penny Dash, Pooja Kumar, and Taylor Ray for their contributions to this article.

This article was edited by Dennis Swinford, a senior editor in the Seattle office.



COVID-19 and the great reset: Briefing note #20, August 27, 2020
Amid one of the greatest bull markets ever for technology, semiconductor fabs must find ways to keep up. And all advanced-industry companies should organize for speed to sustain their current pace.
This week, McKinsey healthcare researchers documented the shortage of medical oxygen in developing countries, a long-standing problem made worse by COVID-19. New ideas can help these regions meet short-term needs and set the foundation for a better long-term future.

Our industry research focused on semiconductors and the industries that make and use advanced electronics. Chips control everything from toys and smartphones to laptops and thermostats. In the pandemic, demand has soared for many of these products—even as supply chains have faltered and geopolitical tensions have risen. But will the boom last? Such questions have semiconductor companies thinking about their manufacturing plants. In a winner-take-all industry, even a slight edge in manufacturing can help a company capture an outsize portion of revenues (exhibit). Our new report outlines the essential ingredients of tomorrow’s successful fab.

Exhibit

Chipmakers and other advanced manufacturers have been running hot for six months now, with some notable notches in their belts. One factory recently ran at more than 90 percent capacity with only about 40 percent of the typical workforce. But few leaders think the pace is sustainable. Our new report lays out what it will mean for companies to switch from running on adrenaline to making organizational speed a permanent part of their cultures.

The pace is unlikely to slacken soon. As our new global survey suggests, the appetite for automation has not dimmed. Instead, the factors for success are shifting. More and more, successful organizations are finding ways for people to work in concert with new technologies.

In fact, automation is among the key themes that can lift India to prosperity. That’s the conclusion of a new report from McKinsey Global Institute published this week. The pandemic has sounded a clarion call for India to accelerate growth. Our analysis suggests that a program of targeted reforms, including greater productivity in several sectors, can help the country produce the 90 million nonfarm jobs it needs to create by 2030.

This week, McKinsey researchers also examined cash management at privately owned companies and reviewed lessons from the past for US governors and mayors planning a second term.

Download the full briefing materials.

Download the article.

We are in the thick of August, the time of year when many people take a break, or at least slow down—even in a pandemic. With that in mind, McKinsey broadened its annual summer reading list and asked 60 diverse leaders to share books that have inspired them, that have provided a much-needed respite, or that they look forward to reading. We hope you draw some inspiration from this list and find ways to restore yourself during these unusual times.

Speaking of reading, our special collection, The Next Normal: The recovery Will Be Digital, has a 172-page curated volume that you can download—the first of five edited collections produced to accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #19, August 20, 2020
McKinsey’s latest research looks at restoring economic activity, today and tomorrow.
This week, we returned to the overarching story of the pandemic: the twin imperatives of saving lives and livelihoods. Our latest research builds on reports we published in April and May 2020, and on recent academic findings that the stringency of national lockdowns is not well correlated with changes in GDP. In our new report, we find that successful control of the virus is the key to unlocking the economy, by restoring the confidence consumers need to reengage in economic activity. In countries that have successfully controlled the coronavirus (“near zero” countries), economic activity (in the form of discretionary mobility) has returned to normal; in those that have not (“balancing act”), it is still about 40 percent lower than before the pandemic (exhibit).

Exhibit

It seems that controlling the virus can get countries back to where they were at the start of the year. But where do we go from there? A companion report outlines the future of economic growth in the United States, by looking back at what worked well in the years after the 2008–09 recession. Federal, state, and local governments can take a range of actions to both improve productivity and stimulate demand. Among the most powerful is investment in inclusive growth and unlocking the maximum productive potential of all people in communities. For example, achieving gender equality could add $4 trillion to the US economy, and closing the Black–white wealth gap could add a further $1.5 trillion.

Businesses, too, are eager to boost productivity and demand. Consumer companies may feel these needs more acutely than most, as two reports published this week demonstrate. McKinsey experts outlined the five bold moves that consumer companies should make to adapt their organizations to the exigencies of the crisis. Another requirement is to meet the next-normal consumer. Seemingly every consumer behavior has been altered by the crisis; companies need to adapt to big changes in how people get their information, what and where they buy, and how they experience shopping.

This week, we also surveyed executives at cell and gene therapy companies about the effects of the pandemic; reviewed the challenges of securing digitally savvy talent at aerospace and defense companies; calculated the significant impact of COVID-19 on mining operations; considered the prospects of upstream oil and gas operations; and surveyed liquefied natural gas buyers on their changing preferences.

We are in the thick of August, the time of year when many people take a break, or at least slow down—even in a pandemic. With that in mind, McKinsey broadened its annual summer reading list and asked 60 diverse leaders to share books that have inspired them, that have provided a much-needed respite, or that they look forward to reading. We hope you draw some inspiration from this list and find ways to restore yourself during these unusual times.

Speaking of reading, our special collection, The Next Normal: The recovery Will Be Digital, has a 172-page curated volume that you can download—the first of five edited collections produced to accompany Our New Future, a multimedia series we created with CNBC.


 .



COVID-19 and the great reset: Briefing note #18, August 13, 2020
As consumer needs change with the ups and downs of the pandemic, and companies look for signs of recovery, McKinsey continues to explore ways to approach the next normal from leadership and operational perspectives.
The abrupt halt of global travel during the COVID-19 crisis, aside from delaying personal trips and vacations, has had a major impact on businesses across sectors. Companies with workforces used to frequent travel—along with the airlines and hotels that depend on revenue from that travel—have been particularly affected. As companies continue to enforce travel restrictions and workers resort to virtual meetings, travel-industry players are looking to rebound from the crisis, but it may be a years-long road to recovery. Our latest research shows that, historically, business travel rebounds from crises at a slower pace than leisure travel (exhibit). As outbreaks in some regions stabilize and travel resumes, travel providers can work to accommodate changing needs and, in turn, boost customer confidence.

Exhibit

Our research this week explores how business operations may change as the travel industry and other sectors reimagine the next normal in a world of physical distancing and evolving consumer behaviors. For operationally intensive sectors, our analysis suggests that the COVID-19 crisis has accelerated automation and digitization. Upskilling and reskilling the workforce will become even more of a priority. For consumer-goods leaders, reshaping the sales function and fostering collaboration between retailers and manufacturers will be critical.

More broadly, our conversations with executives this week demonstrate that successfully weathering the pandemic will also require a people-centered approach to internal and remote leadership. Our interview with Steve Collis, CEO of AmerisourceBergen, considers how a daily executive meeting can be used not only for decision making but also as an opportunity to extend empathy to colleagues. Mike Henry, CEO of BHP, similarly tells us that prioritizing people and building strong relationships has boosted the company’s resilience. He says, “Against the backdrop of COVID-19, there’s a premium on getting out, demonstrating empathy, and engaging with people to understand what their concerns are.”

This week we also analyzed how companies can mitigate risks in industrial supply chains and utilize their procurement functions and spend analytics to bolster resilience; explored ways behavioral-health leaders can build on the current momentum for change in the industry; and looked at how Australia can gauge the scope of the pandemic’s effects on different industries and workforces and address cautious consumers.

Also this week: we’ve added a special collection, The Next Normal: The recovery Will Be Digital. This 172-page volume is the first of five edited collections produced to accompany Our New Future, a multimedia series we created with CNBC. You can download it here.

We’ve now reached August, the time of year when many people take a break, or at least slow down—even in a pandemic. With that in mind, McKinsey broadened its annual summer reading list and asked 60 diverse leaders to share books that have inspired them, that have provided a much-needed respite, or that they look forward to reading. We hope you draw some inspiration from this list and find ways to restore yourself during these unusual times.


We continue to track economic and epidemiological developments around the world. For an overview, read our latest briefing materials (July 6, 2020). In 54 pages, we document the current situation, the economic outlook, the forces shaping the next normal, and the new organizational structures that can help companies keep pace sustainably.

 was edited by Dana Sand, an assistant managing editor in the Atlanta office.



COVID-19 and the great reset: Briefing note #17, August 6, 2020
Banks are using new techniques to find out who’s ‘swimming naked.’ And new MGI research looks at the cost of disruption in global supply chains.
Millions of employees have lost their jobs and cannot pay their credit cards. Restaurants and shops are only slowly reopening; many cannot pay their rent. Industrial companies can’t make payments on their equipment leases. Landlords have less income and cannot keep up with their mortgages. Suddenly, the world is awash in credit risk. Our new research shows how banks are tending to a radical surge in demand for one of their most ancient practices: measuring and monitoring credit risk. Leading banks are deploying a new configuration of sector analysis, borrower resilience, and high-frequency analytics. They are moving past sectoral analysis to take subsector views of the probability of default (exhibit). Some are going even deeper, to understand what’s happening in the financial life of their borrowers.

Exhibit

Like credit risk, supply chains have experienced intense disruption. This week, the McKinsey Global Institute looked at the effects not only of COVID-19 but of all manner of disruptions, including natural disasters, geopolitical uncertainty, climate risk, cyberattacks, and more. A key finding: over the course of a decade, companies can expect disruptions to erase half a year’s worth of profits.

This week, McKinsey also had the privilege of speaking with three CEOs about what is shaping up to be the defining moment in their careers. Alain Bejjani, CEO of Majid Al Futtaim, told us about the resilience needed to keep this Dubai-based operator of shopping malls and other consumer real-estate businesses vital and relevant during the crisis. Lance Fritz, CEO of Union Pacific Railroad, talked with us about tactics to stay present in video calls and keep the board informed. Kristin Peck, the brand-new CEO of animal-health company Zoetis, reflected on the core beliefs that have kept her company on track through the crisis.

Also this week: a new report documents the disproportionate effect of the pandemic on Asian-American communities. And McKinsey’s industry research examined the potential for greater collaboration with government in global tourism, outlined the moves that European restaurants are taking to thrive in the next normal, considered how life insurers can use artificial intelligence to better underwrite risk, and reviewed the nascent Tech for Good movement in the United Kingdom.


 .



COVID-19 and the great reset: Briefing note #16, July 30, 2020
Our latest executive survey reveals a darkening outlook. But our review of vaccine research provides a ray of hope. This week, McKinsey covers the yin and yang of the pandemic.
In North America and in developing markets, executives have become less hopeful about their countries’ economies and more cautious in their views on potential scenarios for COVID-19 recovery. That’s a key finding from our latest poll of more than 2,000 global executives. Leaders in China and India, on the other hand, are growing more upbeat (exhibit).

Exhibit

One thing that will certainly improve expectations in every country is news of a safe and broadly available vaccine. Our latest research looks at global vaccine development and finds that early data on safety and immunogenicity in Phase I and II trials are promising, though limited. Our review of historical attrition rates suggests that the current pipeline may yield more than seven approved products over the next few years, with some available for emergency use late this year or early in the next. A new interview with Microsoft’s chief technology officer explains how artificial intelligence is aiding vaccine development. After development, it’s on to production, where we argue that tech transfer may be critical to beating the disease.

Our new research on leadership in the crisis turned up several intriguing developments this week, starting with the idea of creating a “to be” list. Our interview with the CEO of Cincinnati Children’s Hospital talks about how leaders can choose to be generous and genuine with some colleagues, and collaborative and catalytic with others. Leaders can also acquaint their teams with lessons from the past. We looked at the post–World War II era, when countries rebuilt from the ashes, to extrapolate ideas that are just as relevant now. Finally, we identified the ways that leaders can shift mindsets and behaviors to reopen safely.

In the COVID-19 crisis, many companies are finding new leaders in unexpected places, well down the org chart. Some young middle managers are defying the problems and frustrations of this difficult period to achieve far more than others. Leading companies are capitalizing on this by installing four talent-management practices to thrive beyond the pandemic. These companies are also revisiting the playbook of chief HR officers, to understand how the crisis has changed the game.

McKinsey’s industry-research teams were active this week, publishing new reports on the resiliency of national banking systems; the future of US freight and logistics; the lessons of past crises for mining companies; the recovery of Germany’s travel industry; and the next normal for European bancassurance. Readers interested in banking should also see our interview with the chairman of the State Bank of India, India’s largest lender and the world’s largest digital bank. And bankers, retailers, and others should consult our must-see guide on how to understand and shape consumer behavior.


 .



COVID-19 and the great reset: Briefing note #15, July 23, 2020
Economic recovery depends on the return of the consumer—but shopping will never be the same. New McKinsey research considers the possibilities.
McKinsey continues to track economic and epidemiological developments around the world. For an overview, read our latest briefing materials (July 6, 2020). In 51 pages, we document the current situation, the economic outlook, the forces shaping the next normal, and the new organizational structures that can help companies keep pace sustainably.

The ubiquitous face mask does more than protect against viral spread; it also changes the way we look at one another—and thus symbolizes the mystery of customer behavior in the pandemic. Several new McKinsey research efforts analyze the changes taking place in the homes of consumers, on their phones, and in stores. “Reimagining marketing in the next normal,” for example, documents six of the biggest shifts emerging from COVID-19. One of the most intriguing is the rising importance of neighborhoods: with travel largely shut down, marketers must figure out how to localize their outreach.

Another momentous shift: customers care more about sustainability: our survey finds that European consumers want fashion firms to act responsibly by considering their social and environmental impact. This survey is part of McKinsey’s comprehensive effort to document customer sentiment across dozens of countries throughout the pandemic.

Both consumer-goods makers and retailers have everything at stake in understanding these shifts. For consumer companies, the future is about three things: getting better at predicting demand, being alive to all the ways they might increase their sales, and using agile techniques to sustain the hard-won momentum. For retailers—particularly grocers, apparel companies, and restaurants—the way forward starts with new ideas about revenue management (a fundamental rethink of products, pricing, and promotions might be in order) and about operating models (especially store footprints, which will depend on how soon cities reopen). Such operational issues are paramount in Africa and Asia; our latest research collects useful innovations from those regions and around the world.

B2B customers too are changing, and their providers must adapt. Our latest insights, based on a detailed survey, suggest that B2B companies may be too focused on the here and now. In times like these, first movers do better than the competition by finding new pockets of growth and reshaping go-to-market approaches to serve them.

Chief executives can help their marketing chiefs meet these goals, and much more besides. As some of our most senior colleagues argue, this may be the CEO moment of our times. Companies can reset themselves and their potential by embracing four shifts: unlocking bolder (“10x”) aspirations, making their “to be” lists just as important as their “to do” lists, fully embracing stakeholder capitalism, and harnessing the full power of CEO peer networks.

This week, McKinsey researchers also considered the future of mobility in India, reviewed the changes underway in Europe’s private banks, surveyed physicians about their employment prospects, looked at the reset that supply chains need, and explored two hot topics in tech: how to get value from cloud computing and the shifting priorities of cybersecurity.


 . 



COVID-19 and the great reset: Briefing note #14, July 16, 2020
As many countries struggle to control the pandemic, McKinsey remains tightly focused on the global healthcare response to it.
McKinsey continues to track economic and epidemiological developments around the world. For an overview, read our latest briefing materials (July 6, 2020). In 51 pages, we document the current situation, the economic outlook, the forces shaping the next normal, and the new organizational structures that can help companies keep pace sustainably.

This week, we reviewed the potential for South Africa’s small businesses to survive during the pandemic and to thrive after it, considered the case for more M&A as corporate India seeks to recover from the crisis, looked at the ways shared mobility might come back after it ends, offered recommendations on pricing for property and casualty insurers, and pondered the future of packaging design (including an interview with the CEO of Sealed Air).

But we focused mainly on healthcare systems. Testing is critical for containing COVID-19, yet many countries still struggle with shortages of the necessary materials. Our new article looks at five parts of the testing process and examines the bottlenecks in each. Some US laboratories, for example, have reported unused capacity to conduct tests, even as patients and healthcare workers report difficulty securing them. Similar mismatches have arisen in the United Kingdom, and they are also showing up in supplies of reagents, test kits, and other consumables. To fix the problems, countries will have to make capacity more visible by establishing information nerve centers.

Another focus of research is airborne transmission of the coronavirus. World Health Organization guidelines now state that it may be possible indoors, especially for people who spend significant amounts of time in crowded, poorly ventilated rooms. Our new article not only offers a primer on air purification, air filtration, and airflow management but also examines the steps that building managers, safety experts, and others might take to optimize airflows and ventilation indoors and to limit the spread of the virus.

This week also saw news about a successful vaccine trial. Thanks to that, the world may be able to look ahead to the pandemic’s end. But as a McKinsey team writes, this is not the last pandemic. To correct deficiencies in the surveillance of and response to infectious diseases, governments will have to make substantial investments—but they will be well worth the money (exhibit). Our research outlines the shifts needed in healthcare systems.

Exhibit

Ara Darzi, director of the United Kingdom’s Institute of Global Health Innovation, has similar aspirations: he is simultaneously focusing on new ideas that can help tame COVID-19 and on the longer term beyond it. In an interview with McKinsey’s Rodney Zemmel, Lord Darzi explains how healthcare can transition from a “sickness service” to a “health and well-being service.” One critical step is to recognize that “we have many pandemics—only we don’t call them pandemics. We have the pandemics of obesity, cardiovascular disease, and diabetes.” McKinsey Global Institute covered the substantial upside of addressing these chronic conditions in a new report published last week.


 .



COVID-19 and the great reset: Briefing note #13, July 9, 2020
As lockdowns lift, businesses are thinking about their next moves. McKinsey research offers insights into the near future.

Around the world, economies are cautiously reopening. Businesses are keeping one eye firmly on the here and now but also tentatively looking ahead to what’s shaping up as a great reset. Our new research this week offers several takes on this theme.

Start with the global pandemic’s front line: the healthcare sector. This week, the McKinsey Global Institute published a new report, Prioritizing health: A prescription for prosperity, which measures the potential of proven interventions to reduce the global burden of disease. Taking advantage of them would not only alleviate a problem exposed by COVID-19—people with diabetes, hypertension, chronic obstructive pulmonary disorder, and obesity have been hit hardest—but also add, in our estimate, $12 trillion to global GDP in 2040.

Reimagining the workforce is another pressing task. Executives everywhere wonder how to bring people back to the workplace and how they will do their jobs. Our new research takes a look at the challenges of creating a sense of belonging, common purpose, and shared identity when some people work in their homes and some in offices and factories. Another article considers the great reset’s tactical challenges, such as guarding against cyberattacks on remote workers.

Small businesses confront some of these problems. But much as Ginger Rogers danced the same steps as Fred Astaire—only backward and wearing high heels—small businesses must make the necessary changes at a greater relative cost and with less working capital. Our new research examines the struggles of US small businesses in three sectors (restaurants, manufacturing, and retailing) that could be facing a long, hard recovery (exhibit).

Exhibit

Another sector thinking hard about its future is infrastructure. In the United States, two scenarios are possible: a boom spurred by a government stimulus or a bust as tax revenues and user fees dry up. Agencies and investors alike must prepare for both outcomes. One key to generating a rapid impact from infrastructure spending is to repair existing assets.

This week we also look at global freight flows (down 13 to 22 percent this year) and the varied potential for recovery, reviewed the implications of COVID-19 for the US food supply chain, and considered the challenges of pricing in a pandemic.

 .



COVID-19: Briefing note #12: July 2, 2020
One step forward, two steps back: the pandemic is giving new depth of meaning to that well-worn expression. Our new research explores both parts of it.

In a week when the global pandemic seemed to gather strength, our new research both shows the grim economic news and reveals a streak of optimism that many are starting to feel. Our monthly global economic conditions snapshot indicates that 52 percent of executives now say that their national economies are doing substantially worse, up from 10 percent in March 2020. Yet the proportion of executives who expect profits to rise within six months rose by four percentage points, and leaders in retail, high tech, and telecom are increasingly optimistic about the return of customer demand. In June, many more executives around the world said that the economies of their home countries would soon be doing better than had said so in May.

Another new global survey examined sentiment among people who make financial decisions for their households. Across the globe, they are reporting lower income, savings, and spending. In most countries, 20 to 60 percent of these decision makers say they fear for their own jobs. Roughly half have no more than four months of savings.

These grim statistics present a challenge for banks and other consumer-facing businesses, such as telecom companies, retailers, health systems, and utilities. A delicate balancing act awaits these organizations as they work to ensure that customers receive the necessary support—and that lenders can continue to cultivate relationships with their borrowers—while preserving shareholder value in the longer term. A detailed perspective on utilities considers this and other conundrums. So does a new look at African banks.

The virus’s spread is accelerating, but businesses everywhere are both coping with their urgent needs and looking ahead to the time when their employees can safely return to work. As that moment comes closer—let us hope—three new research efforts show, first, how leaders can seize the moment to support their employees by building on the trust their early efforts have engendered and, second, how they can engage employees through clear and inspiring communication. And our survey of US companies shows that same insistent streak of optimism: respondents expect most employees to be working onsite by December.

One lesson of the crisis is the need for speed: the pandemic obeys no speed limits, so businesses have had to adapt through quick fixes and workarounds. How can they keep these successful innovations going over the long term? Our new research suggests nine ways to reinvent the organization for speed.

This week, we also looked at how companies can reset their capital spending, demystified the role of quantitative models, and talked with two McKinsey experts about how to choose the right path to unlock the economy.

 .



COVID-19: Briefing note #11: June 25, 2020
Every industry is adapting to life during a pandemic. New McKinsey research examines the implications for six sectors.

This week we zeroed in on critical developments in six major industries, starting with consumer goods. Research we published last year (here and here) documented the recent trends in consumer M&A and the ways that successful companies used acquisitions to accelerate revenues and profits at a time when growth was elusive. Our latest research reveals that COVID-19 has accelerated some of these trends and created new realities. One critical finding: consumers are returning to big brands they know and trust. While these companies accounted for only 16 percent of the industry’s growth in 2015–18, that figure rose to 39 percent in 2018–19—and reached 55 percent in the first three weeks of April 2020.

In times of crisis, all eyes focus on the insurance sector. This week, we surveyed insurance agents in China, the first country to reopen. The outlook there is complex: some lines, such as health insurance, fared well, while others, such as property and casualty, suffered significant declines and are just now recovering.

At semiconductors companies, the pandemic has posed questions for every aspect of the business model. Our April 2020 research outlined the potential shifts in demand. This week’s update offers scenarios in which demand might revive, and the ways that companies can adapt while also preparing the enterprise to emerge stronger in the next normal.

Software makers are also in the midst of (yet another) disruption. For the past ten years, the rise of software as a service (SaaS) has reshaped the enterprise-software industry. Growth accelerated, but industry profitability tumbled. Our research finds that the next ten years will be just as tumultuous. SaaS companies are at a crossroads: COVID-19 will accelerate the footprint of SaaS, given the growth of remote working, the rapid deployment of digital solutions, and the lower up-front costs.

Like many other industries, engineering and construction has had to reimagine how work gets done. This week, we spoke with an industry leader, who revealed the ways that his company has adapted.

Finally, while small business might not be an industry, it is a mighty economic sector that employs tens of millions of people in the United States. Our new research finds that between 1.4 million and 2.1 million US small businesses could close permanently as a result of the first four months of the pandemic. Certain sectors are particularly at risk (exhibit).

Exhibit

This week, we also examined the priorities for companies in India to thrive in the next normal; reviewed the early returns on post-COVID-19 discretionary spending in China, India, and Indonesia; and considered the lessons of the past that might prove helpful as policy makers seek to revive the US economy. Finally, we were privileged to speak with two remarkable leaders, Mellody Hobson of Ariel Investments and Hubert Joly of Best Buy, about the challenges of leadership in extraordinary times.

 .



COVID-19: Briefing note #10: June 18, 2020
Our latest research focuses on recovery in Europe.
Governments worldwide have already allocated more than $13 trillion to stabilize economies in freefall and restart growth. These measures, written and delivered at speed, have succeeded in many ways. But as the crisis drags on, new questions are arising. Is the money directed in the best possible way? And is more needed?

This week, McKinsey researchers looked at ways to fill the gaps that COVID-19 has created in US state budgets. Worldwide, we estimate that government deficits could reach $30 trillion by 2023. That’s a sobering figure. But we believe that if governments and the private sector work together as never before, they can avoid the disastrous consequences of massive deficits, lay the foundations for a new social contract, and begin to shape a postcrisis era of shared, sustainable prosperity.

Our new research on Europe suggests that governments can start by distinguishing between sectors that can navigate the crisis safely, and others, such as those that were already in decline and were then badly hit by the crisis, that may need structural change. In Germany, for example, you will find both types of sectors in abundance (exhibit).

Exhibit


This week we documented many of the COVID-19-related shifts taking place in Europe. A critical issue now coming to a head concerns privacy: How do companies comply with the European General Data Protection Regulation and also support contact tracing and testing measures? Another is mobility: transportation systems may be permanently altered in the crisis. Our new research on the United Kingdom outlines the implications.

Some of those concern the many UK start-ups offering novel transport solutions. This week, our new research found that small and medium-size businesses in the United Kingdom face dire prospects: one in five may not survive past August 2020. In the recovery, European governments cannot do all the heavy lifting; our analysis suggests that European foundations have a window of opportunity to step up their actions and play an essential role in national rebuilding and recovery efforts. And we interviewed a leading UK dealmaker on the potential to restart major capital projects through standby agreements and other moves that keep projects on track.

This week, we also presented ideas for retailers to redefine value and affordability for newly strapped consumers, addressed big shifts in physicians’ behavior, explained why insurers need to revamp their distribution models, and considered a safer, better future for travel.

 .



COVID-19: Briefing note #9: June 11, 2020
Our latest research examines the social risks of COVID-19.
Even as societies and businesses race to reopen, the global pandemic still poses significant problems. This week, we documented an accelerating one: loneliness. In many parts of the world, the social fabric was already fraying before the pandemic. Now, as former US surgeon general Vivek H. Murthy, MD, points out, COVID-19 is disconnecting us further from our human relationships. That might cause a “social recession, with profound consequences for our health, for our productivity in the workplace, for how our kids do in school.”


This week, we investigated loneliness and other effects of the lockdown and physical distancing in Europe. Across the Continent, the proportion of people who say that they feel lonely “most or all of the time” has nearly tripled. Loneliness is higher in countries, such as Bulgaria and Greece, where trust and satisfaction with relationships were already at low levels in 2018.

For many, the cure for loneliness might be a return to the office, the subject of some of our latest research. Many people say they are happy working from home. But could their happiness be running on fumes of the social capital built up through years of water-cooler conversations, meetings, and social engagements? Has working from home succeeded only because it is viewed as temporary, not permanent? Hundreds of billions in real-estate investment are riding on these questions.

Most industries are engaged in similarly momentous discussions. This week we published new perspectives on the fashion, hospitality, infrastructure, institutional-investing, nursing, and public-transport sectors. We also reviewed developments in Spain and Africa (payments and food supplies), as well as trillion-dollar ideas for governments around the world.

Although the news is bleak, Vivek Murthy sees cause for optimism: “I think that this could be an extraordinary opportunity for us to step back and ask ourselves if we’re leading the kind of lives that we really want to lead. This is our chance to ask ourselves where people fit in our priority list and whether there’s a gap between our stated priorities and our lived priorities.”

 .



COVID-19: Briefing note #8: June 4, 2020
This has been an extremely painful time for communities across the United States and beyond, even as the pandemic continues to take its toll. We are amplifying our commitment to do our part to ensure that black lives are spoken for and valued, both inside our firm and beyond. Our ongoing research on the US racial-wealth gap and on diversity and inclusion is intended to clarify some of the underlying issues and potential paths forward.

Research we published in April called out the disproportionate effects of COVID-19 on black Americans, who are almost twice as likely to live in the counties where the risk to health and economic activity is highest if and when contagion strikes (exhibit). Many of these places were the scene of this week’s anguished protests.

Exhibit

Our newest research looks at the pandemic’s effects on US minority-owned small businesses. Vulnerable even before the pandemic, it has struck them disproportionately hard. Many of them are in the industries most susceptible to health and economic problems, such as accommodations and food services, retail, and healthcare. Owners are innovating and staying flexible, helping their communities to cope with the crisis. But these businesses are highly vulnerable; they need help from the private, public, and social sectors.

Similar dynamics afflict US minority students. Previous McKinsey research has demonstrated the costs of a sizable achievement gap between white students and black and Hispanic ones. Our latest research, published this week, finds that the pandemic not only threatens to widen the achievement gap but also poses problems for all learners. The hurt could last a lifetime.

People of color are vulnerable to yet another effect of the COVID-19 crisis as it affects large companies. Previous crises show there is a very real risk that as companies adapt to new ways of working, inclusion and diversity may unintentionally recede as strategic priorities. Yet as our latest report on inclusion and diversity argues, that would place companies at a disadvantage: they could not only face a backlash from customers and talent now but also, down the line, fail to better position themselves for growth and renewal.

McKinsey continues to research many aspects of leadership through the crisis. This week, we reported on dozens of our new research efforts, including the emerging themes dominating boardrooms; the post-COVID-19 future for US rail and trucking companies; the lessons learned from Asia’s manufacturing and supply chains; a new approach to tracking demand for travel; the potential for telehealth; and the safety protocols that hospitals, grocery stores, and others have used to stay open.


 .



COVID-19: Briefing note #7, May 27, 2020
New insights on consumer sentiment and the return to work.
The Memorial Day weekend in the United States, always a somber occasion and never more so than this year, seemed to mark a turning point in the COVID-19 crisis. As spring turned to summer, many US regions started to reopen, as did others in Europe, Latin America, and Asia. Despite ongoing public-health concerns, the desire to spend and shop is palpable. This week, McKinsey published new surveys of consumers in Argentina, Australia, Brazil, Central America, and the United States, detailing the strength of the consumer urge in each country. The outlook is brighter. Consumers are less anxious and depressed about health concerns. Business executives are a bit more optimistic this month than last. And our new surveys of global B2B buyers and those in Asia and Europe suggest that confidence is holding firm.

That said, the picture these surveys paint is complex. In a sense, the world is turning from “resilience” to “return”—the third of the five pandemic elements we sketched out in late March. To get back to business, many companies are running spreadsheets to see how many people spaced six feet apart will fit in an office, planning one-way paths through the workplace, and figuring out adaptations to restrooms, lunchrooms, and entrances. All of those are critical tasks, but they are not enough. What’s needed is a return “muscle”: an enterprise-wide ability to absorb uncertainty and incorporate lessons into the operating model quickly.

One of the skills that will help with that urgent need is surely analytics, widely recognized for its problem-solving and predictive prowess, which is becoming a modern-day sextant to navigate the COVID-19 crisis. Analytics can help tackle numerous urgent tasks facing businesses today: forecasting demand, identifying potential supply-chain disruptions, targeting support services to at-risk workers, and determining the effectiveness of crisis-intervention strategies, to name a few.

Also on that list: improving the experience of customers, many of them frightened, some jobless, and all of them deeply uncertain about the next normal. To renew and refresh their connections to the people they serve, companies need to recognize what’s happening now, and respond in three ways: digital excellence, safe and contactless engagement, and dynamic customer insights.


 .



COVID-19: Briefing note #6, May 13, 2020
Emerging evidence provides some tantalizing glimpses into the epidemiology of the global pandemic.
By Matt Craven, Mihir Mysore, and Matthew Wilson
As the reopening of economies continues across much of Europe and North America, it’s worth taking stock of the epidemiological situation and trends that will define the months ahead. At the time of this writing, the official counts of cases and deaths from COVID-19 have passed four million and 280,000, respectively. Recent studies have made increasingly clear that each of these figures is a significant underestimate. Population antibody surveys suggest that official counts are underestimating the true number of cases by a factor of five or more (although in several cases the methodology has been called into question) (Exhibit 1).

Exhibit 1

Comparisons of 2020 and 2019 mortality rates show that substantially more people are dying this year, although we don’t know how much of this is due to missed deaths from COVID-19 rather than excess mortality from other causes (Exhibit 2).

Exhibit 2

The pandemic and public health—five trends to watch
With lives at stake, a thoughtful approach is paramount. Here are the five emerging trends that private-sector leaders need to monitor.

There are still many places where the epidemic is getting worse
While much of the media narrative is about reopening, many countries, including several of the largest emerging economies, are still on the “upslope” of the epidemic, with daily case counts increasing (Exhibit 3). While an increasing number of countries and regions have proven that they can use lockdowns to drive a reduction in cases, to date, we have few examples of success outside higher-income countries. The next few weeks will be critical tests of our ability to “bend the curve” in more countries with varying contexts and healthcare capacity. In some of these countries, the absolute number of deaths is relatively low; interventions against COVID-19 will need to be viewed through the lens of both lives and livelihoods.

Exhibit 3

Reopening is a massive natural experiment—make sure you learn from it
We have never before attempted to shut down the modern global economy, much less reopen it in the setting of an ongoing pandemic. We have a few examples of strategies that seem to work better, or worse, but none of us know with any certainty the best actions. Even places with strong initial responses like Hong Kong and Singapore have faced challenges as they reopen.14 China has also seen an increase in cases in the past few days.15
In the United States, there is only a loose correlation between disease prevalence and plans for reopening. States with more cases generally plan to reopen later, but there are exceptions.

A similar point can be made about businesses’ plans to reopen. Companies are planning different approaches, even based on the same underlying fact base. This implies that leaders across the public and private sectors should build learning and adaptation into their reopening plans from the start. Relevant lessons might come from other geographies, other sectors, or from peers and competitors. Leaders should be prepared to incorporate new information and alter their approaches, either incrementally or radically, as new information becomes available.

Resurgence seems to be not a question of if but when, where, and how bad. Many experts are focused on a potential second wave of COVID-19 in the northern hemisphere this autumn.16 This is certainly possible. But focusing on the risks of autumn and winter causes us to look past the summer, which is risky because it is sooner and because it is when many jurisdictions will be reopening and testing.

R is important, but so is the absolute number of new cases
Over the past few months, many have become more familiar with epidemiological concepts like the reproduction number (R) of a virus. R defines the transmissibility of a pathogen, as measured by the average number of people to whom each infected person transmits. R is a measure of change; it tells us how fast the epidemic will expand or shrink. Values greater than one define a growing epidemic, while those less than one define a shrinking one.

R has been getting a lot of attention, for example, in defining the packages of interventions that can yield R<1 in a given setting. But the absolute number of cases is also important. Imagine two cities, each with an R of 0.9, implying a slightly declining epidemic. But one of the cities has 1,000 new cases per day and the other has ten. The former faces a far higher risk in reopening than the latter.

In practice, we are seeing countries and regions take divergent approaches to this question (Exhibit 4). Hubei Province in China waited until reported cases were near zero to reopen, whereas Italy and Spain took the first steps to reopening with daily case counts at more than 1,000. Every location needs to balance public-health and economic imperatives; we can’t say which approach is better, but we are likely to learn more about what works in the weeks and months ahead.

Exhibit 4

It’s (still) all about testing, tracing, and targeted quarantine
Significant resources are required to run a program of testing, contact tracing, isolation, and quarantine at the required scale, but relative to the economics of lockdowns or global recession, these costs are trivial. Many countries are still far short of where they need to be on testing, and contact-tracing programs remain a patchwork. Our recent article provides more details on contact tracing. Strengthening these programs remains an urgent priority for many geographies. This point is no less important for having been made frequently.

In any country, here are the four metrics to watch in assessing the strength of test, trace, and quarantine efforts:

Test positivity rate, which measures (imperfectly) the extent to which testing systems are capturing all cases. The World Health Organization recommends a target of less than 10 percent positivity.
Tests per million population, a measure of the depth of testing.
Average number of contacts identified per case, which measures how effective contact-tracing systems are at identifying and isolating the likely next generation of cases. The figure will tend to be lower in lockdown settings than when people are moving and interacting freely.
Fraction of cases arising from contact lists, a measure of the portion of cases arising from known sources versus undetected community transmission.
An element of transmission dynamics now beginning to receive more attention is transmission within households.17 We may need to rethink the current model of home isolation and develop modified strategies for mild and asymptomatic cases given that isolation can prove difficult for many. Any new model should of course ensure a comfortable experience for those who test positive, so that they’re strongly inclined to follow the recommended approach.

Innovation—and clinical evidence—leads to hope
The speed and scale of the R&D response to the COVID-19 outbreak is unprecedented in human history, with billions of dollars being spent and committed in pursuit of drugs, vaccines, and diagnostics for the virus. Today, there are more than 150 vaccines in the pipeline, and 200 drug candidates. On diagnostics, beyond the RT-PCR18 and classic lateral-flow immunoassays already in use for many viral and antibody tests, new technologies such as CRISPR19 have already been granted emergency-use authorization by the US Food and Drug Administration.20
The past few months have seen the launch of numerous trials in an effort to find therapies and vaccines—with some challenges from studies that are too small in size, or not randomized or controlled. As of early May, more than 1,700 trials are in progress targeting COVID-19 and related complications. More of these are randomized and controlled clinical studies—and some are starting read out results, providing evidence to support new approaches to prevent and manage COVID-19 infection and associated complications. The expert consensus is that enhanced treatments for COVID-19 will likely be available by the end of 2020; and only 12 to 18 months will likely be needed21 to bring a vaccine to market at sufficient scale for widespread immunization, compared with the typical five or more years. Some developers have even indicated a vaccine may be available sooner for limited use, with an emergency-use authorization for health workers issued as early as this fall.22
Here are five areas to watch:

The great vaccine-platform race. At the time of writing, 13 vaccines are already in clinical trials, and the full pipeline spans a massive range of platforms, including RNA, DNA, inactivated viruses, protein subunits, and virus-like particles (VLPs). The virus and viral-vector approaches are traditional; others are nascent. Each platform will start to produce data in the months ahead, starting with evidence of vaccine safety and then potentially demonstrations of immunogenicity (and even efficacy) toward the end of the year, though we still need to better understand the link between immunogenicity and correlates of protection. While having multiple platforms in development increases the likelihood of a successful vaccine, each platform has different competitors, ranging from smaller biotech companies to multinationals, as well as distinct manufacturing requirements, with implications for the scale-up of capacity.
A more nuanced understanding of the uses of different therapeutics. The initial discussion on drugs has focused almost exclusively on repurposed antivirals and antimalarials for treatment. The 200-plus candidates currently in development cover a broad range of use cases—from postexposure to prophylaxis, and from mild and moderate to severe cases. The more than 1,700 active trials are expanding the focus from drugs that directly attack the virus to those that confer immunity and to those that target complications of COVID-19 such as cytokine-release syndrome (CRS) and, more recently, acute respiratory distress syndrome (ARDS). Labs are deploying a wide array of platforms, from repurposed antivirals (as mentioned above) to monoclonal and polyclonal antibodies to neutralize the virus to immune modulators for ARDS/cytokine storms, and even cell-therapy approaches for late-stage disease. The emergency-use authorization for remdesivir is an important milestone for COVID-19 drug development as well. In the coming months, we anticipate that a more nuanced understanding of the different use cases and the types of approaches being tested will help reduce the mortality rate of COVID-19 and also change the standard of care.
A new normal and unrealized opportunity for data sharing. Unlike the experience with prior epidemics (including Ebola), COVID-19 has been characterized by unprecedented sharing of prepublication data, analyses, and results via medRxiv, a collaborative platform. This proliferation of information can support innovation and has been rapidly integrated into both the media and policy discussions—sometimes, however, to unfortunate effect. Looking forward, as the scientific community seeks to make meaningful interpretations of the thousands of running studies, we need to bring together the patient-level data from the hundreds of small, undersize, not-well-controlled, compassionate-use, and observational studies, in a responsible way. Meta-analyses of such studies will help us know if therapies actually work, at what dosing and clinical regimen. There are efforts underway in the ecosystem to address this—and hopefully a collaborative model emerges that could remain with us postpandemic.
The impact of novel R&D models. Competitors are collaborating in ways never expected.23 Companies are banding together in multilateral collaborations, some formal and some informal, to advance innovation. For example, leading plasma manufacturers are partnering in novel ways to produce a single unbranded immunoglobulin product; more than 15 pharmacos are collaborating in a COVID-19 R&D forum to advance, individually and collectively, the most promising drugs and vaccines; and decades-long competitors Sanofi and GSK are partnering on COVID-19 vaccine development. Novel master protocols, often with inspired names (such as Solidarity, Recovery, and ACTT), are being used to simultaneously test multiple drugs.24 Innovators are deploying novel development plans and trial designs as well; for example, Pfizer and BioNTech are simultaneously testing four vaccines in their combined Phase I/II study. These approaches are not without risk given the parallel work in traditionally sequential stage-gated processes.
The challenge of separating the signal from the noise. With Ebola, a substantial R&D mobilization ran into difficulties recruiting patients to test all of the approaches being considered. Some of these same challenges are happening with COVID-19. Ensuring that studies are well controlled and appropriately powered will be critical to understanding what actually works. Further, data sharing will hold the key to advance our understanding and interrogation of the benefit/risk trade-off. Multiple prioritization efforts are attempting to do this but are still in the early stages. In some ways, the scale of the mobilization may be the biggest challenge.
About the authors
Matt Craven, MD, is a partner in McKinsey’s Silicon Valley office. Mihir Mysore is a partner in the Houston office. Matt Wilson is a senior partner in the New York office.

This article .



COVID-19: Briefing note #5, April 13, 2020
Our latest perspectives on the coronavirus pandemic.
By Matt Craven, Mihir Mysore, Shubham Singhal, and Matt Wilson
In this note, we offer some of our latest insights on the COVID-19 pandemic, starting with a survey of the current epidemiology and the five dynamics leaders need to watch: the efficacy of the surge in critical care, the expansion of testing and other traditional approaches, the development of antibody testing, the unknown nature of immunity, and a wave of innovation that might produce treatments and vaccines.

We then highlight four of our many recently published articles, each designed to help senior executives think through the challenges of restarting economies. These and many more articles are available in our collection of coronavirus thinking.

The outbreak is moving quickly, and some perspectives here may soon be out of date. This article reflects our perspective as of April 13, 2020. We will update it regularly as the crisis evolves.

COVID-19: Where we are, and where we might be heading
COVID-19 continues to spread rapidly around the world. Almost every country has reported cases, but the burden is asymmetrically distributed. In the past seven days (April 6–12), 46 percent of new confirmed cases have been reported in Europe and 39 percent in the United States. To an extent, that’s because countries are at different stages of the pandemic. Some that were effective at initial containment, such as Singapore and Hong Kong, have seen resurgence and are implementing additional measures to address it. Others, such as many countries in Western Europe, have seen the number of new cases plateau or begin to decline and are debating the right approach to reopening their economies. Some countries appear to be at the peak of infection and are urgently building surge capacity in their health systems. In other parts of the world, the number of cases is rising rapidly. Countries such as Russia and Turkey are seeing a recent acceleration. India too has experienced a significant increase in the number of cases since the beginning of April and has evolved its response strategy, including extending the nationwide lockdown.

The public-health tools and approaches to be deployed vary considerably based on this status (Exhibit 1). Measures including physical distancing, travel restrictions, effective use of personal protective equipment (PPE), testing and tracing, and healthcare surge capacity require more or less emphasis, depending on epidemic phase and local context. Local use of these measures varies considerably—physical distancing may be near-impossible in crowded urban settings, for example, and the apps and digital tools for contact tracing like those used in China may not be acceptable in other parts of the world. Another challenge is the dependencies among these measures: to take one example, the timeliness and stringency of physical distancing measures substantially influences how other tools should be deployed.

Exhibit 1

Although a consensus has emerged around the use of physical distancing to slow transmission in many high-prevalence settings, a few countries, such as Sweden, are pursuing an alternative “herd immunity” strategy focused on protecting the most vulnerable populations while using only limited distancing measures to flatten the curve for others. The goals are to maintain many aspects of economic and social life today and, over time, to develop a large enough pool of exposed people (about 70 to 80 percent) to “protect the herd.” Other countries are closely watching the outcome of this approach.

The months ahead will probably be quite volatile and dynamic. It now appears likely that some places will experience a local resurgence as restrictions are lifted and economies reopen. That will influence countries at the earliest stages. For example, Singapore has seen a resurgence mainly from imported cases, which have led to local transmission; this suggests that restrictions on international travel may continue. As China gradually reopens, the tactics it used (including group-based isolation models and setting a norm of wearing masks in the workplace) and their efficacy will inform approaches around the world. Western Europe’s experience in relaxing restrictions, and the most successful approaches there, will inform the approaches deployed in the United States.

Considering the variety of approaches in use, public understanding and consensus will evolve day by day. We will continue to find out more about the coronavirus—how it is mutating, the duration of immunity, its transmission dynamics, and so on. For example, it now appears that the virus probably won’t be highly seasonal, given the recent rapid growth in a number of hot spots in the Southern Hemisphere. But it is still possible that the arrival of summer in the Northern Hemisphere will slow transmission somewhat, as some studies in both labs and natural contexts suggest.25
With all this in mind, we believe that leaders should closely watch five health-response dynamics in the coming weeks:

The efficacy of the health-system surge and how it is maintained over time. Countries with rapidly increasing numbers of cases are finding ways to expand their critical-care capacity massively. Their ability to do so, and to push mortality from COVID-19 to lower levels, will not only save lives but also engender confidence in their health systems’ ability to manage a resurgence. Over time, as cases plateau and then decline, there will be questions about how long to maintain surge capacity while also guarding against resurgence. Providers will be under pressure to consider the broader context of a capacity surge; for example, in the United States, the mass cancellation of elective medical procedures and the associated financial hardship for many providers is likely to force difficult discussions about which procedures should be allowed to restart, and when. Other effects of surge capacity, on vaccine-preventable diseases and maternal and child health, will also be critical to monitor.
The scaling of traditional public-health approaches. In parallel with the surge in critical care, countries also need to think about building surge capacity in traditional public-health approaches—disease surveillance, contact tracing, and targeted quarantines. Such a surge must build on current efforts to scale viral testing rapidly, mostly through RT-PCR26 machines. Exhibit 2 shows the somewhat surprising relationship between testing and the number of cases—generally, countries that have tested more people have diagnosed fewer cases per thousand people. Moreover, to detect and control flare-ups quickly, widespread access to viral testing will become increasingly important as countries and cities prepare to relax distancing measures. In some countries, this testing capacity could be paired with at-scale contact tracing, with privacy-by-design embedded; and quarantine facilities to help localize hot spots and prevent a broader resurgence.
Exhibit 2

The development of antibody testing and understanding of sero-prevalence. We have little idea how many people have been exposed to this coronavirus. One recent study in a hard-hit area of Germany showed that about 14 percent of the population has been—far from the levels of exposure required for herd immunity to emerge but higher than many had expected.27 A lot of other studies are underway to assess the portion of the population exposed to COVID-19. If individual or herd immunity is to play a meaningful role in reopening, antibody tests to measure exposure must be widely available. While many such tests are being developed, their accuracy and availability have been challenged. The arrival of accurate, widely available antibody tests will help countries understand how close they are to achieving herd immunity and whether they can use immunity as a meaningful signal to start reopening.
The nature of immunity. People exposed to other coronaviruses have exhibited durable immunity for several years after exposure. Everyone hopes the same holds true for the novel coronavirus, but we don’t know for sure. Emerging reports of recovered patients testing positive again on RT-PCR acute-infection tests raise questions about reactivation, as do studies in China showing very low levels of antibodies among some infected people. While it is unlikely that the duration of immunity is short, any new information about this issue would require a significant shift in strategy.
Innovation. There has been an unprecedented burst of global pharmaceutical R&D related to COVID-19. Today, more than 130 therapeutic candidates and 80 vaccine candidates are under consideration across a range of modalities and use cases, such as treatment of severe disease and pre-exposure prophylaxis. If drugs already approved for other indications prove effective in treating COVID-19, they could be deployed most quickly, but in coming months readouts on experimental new drugs will also arrive. The massive scale-up of clinical trials—especially randomized placebo-controlled studies—will provide evidence to guide clinical decisions. Similarly, the unprecedented consortium of plasma companies generates hope that hyperimmune immunoglobulin can be developed quickly. Seven vaccines are already being tested in humans.28 Although at-scale production and distribution is not likely for 12 to 18 months after a successful trial, these vaccines would provide a critical element in the armamentarium against COVID-19. For all these innovations, a central challenge will be rapidly scaling up production to meet global needs.
Getting back to work: Four insights
The pandemic’s economic challenges are unprecedented. Since the crisis began, McKinsey has published more than 70 articles on the extraordinary public-health and economic impact, as well as the ideas that government and business leaders need to safeguard lives and livelihoods. In the past week, four articles have captured the attention of leaders around the world. We summarize these articles here and invite you to take in the full collection.

‘How to restart national economies during the coronavirus crisis’
by Andres Cadena, Felipe Child, Matt Craven, Fernando Ferrari, David Fine, Juan Franco, and Matthew Wilson

The threat of COVID-19 to lives and livelihoods will fully resolve only when enough people are immune to the disease to blunt transmission, either from a vaccine or direct exposure. Until then, governments that want to restart their economies must have public-health systems that are strong enough to detect and respond to cases.

The first and most obvious factor in determining readiness is the number of new cases in a given area. Regions with significant ongoing transmission should expect that restarting economic activity will only lead to more transmission. Case numbers and, more importantly, hospitalizations need to be low enough for a health system to manage individually rather than through mass measures. A second factor in thinking about this is the strength of the systems in place for detecting, managing, and preventing new cases, including adequate medical capacity, especially of intensive care units (ICUs), for those with severe disease; the ability to perform a diagnostic test for COVID-19 with a fast turnaround time; and several other elements.

If we combine a system’s level of strength with an assessment of the intensity of virus transmission, we can evaluate any region’s readiness to restart activity (Exhibit 3). These two dimensions determine four stages of readiness to reopen the economy, with Stage 4 the least ready and Stage 1 the most.

Exhibit 3

‘Europe needs to prepare now to get back to work—safely’
by David Chinn, Hauke Engel, Daniel Härtl, Milena Quittnat, Pal Erik Sjatil, Marja Seidel, Sven Smit, Sebastian Stern, and Eckart Windhagen

As European countries begin to consider how to exit lockdowns, local leaders are often the people best placed to evaluate conditions and impose measures that maximize economic recovery while protecting public health. Decisions about which measures to deploy, when and where, should be made locally—if possible, district by district—because there are material differences in the severity of the crisis and economic circumstances (Exhibit 4).

Exhibit 4

Authorities will need three essential elements to ensure robust implementation. First, leaders will require effective, ready-to-act local-authority structures. In Italy, regional governments collaborated with Rome to establish a national lockdown that allowed regions to apply more stringent rules as necessary.

Second, solutions and directives must be clear and simple, so that the public and businesses can understand them. This might require using new communication channels, such as mobile messaging.

Third, measures must be consistent. If the guidance one day is that shops can admit five people at a time for six hours a day and in the next week that rule changes to two people for eight hours, the results will be irritation, noncompliance, and the erosion of trust in public authorities.

‘Winning the (local) COVID-19 war’
by Tom Latkovic, Leah Pollack, and Jordan VanLare, MD

Local US leaders, such as mayors and governors, have an outsized role in the fight against COVID-19. We see six domains for engagement:

Foundational public health. We assessed 23 public-health interventions and identified the most fundamental ones.
Societal compliance. We monitored different approaches to ensure compliance and found a steeper decline in infections where communities enforced policies tightly (through arrests, for example) than in those that used only fines.
Health-system capacity. To prevent demand for healthcare services from outstripping supply, we found that at least a doubling of critical-care capacity is probably possible and necessary, at least temporarily, across most parts of the United States.
Industry safeguards. If the risk of contagion continues for 12 to 18 months, public- and private-sector leaders should promote the most effective adaptations and safeguards to economic activity, including physical barriers, face guards, physical distancing, health screenings before entry, and generous and flexible sick leave. Sectors will vary in how critical they are and their ability to safeguard.
Protection of the vulnerable. COVID-19 is especially destabilizing for vulnerable populations, including people with chronic physical- or behavioral-health conditions, limited mobility, advanced age, and unmet health-related social needs, such as food and housing insecurity. Each will require targeted interventions.
Economic health. Local leaders need to develop a fact base on their economies and then ensure that money from new and current programs gets into the hands of citizens quickly and easily.
‘Could the next normal emerge from Asia?’
by Oliver Tonby and Jonathan Woetzel

The COVID-19 outbreak began in Asia—but so have early indications of containment, new protocols, and the resumption of economic activity. Although the risk of another outbreak remains, economic-activity indicators in China suggest that urban activities are returning to pre-outbreak levels. Traffic congestion and residential-property sales are close to where they stood in early January 2020. Air pollution and coal consumption have returned to 74 and 85 percent, respectively, of their January 1 levels. A recent McKinsey survey of 2,500 Chinese consumers indicates “cautious optimism”—a gradual regaining of confidence, which should increase spending. At this moment, strong public-health responses in China, Singapore, and South Korea appear to have been successful. Significant evidence indicates that the curve of cumulative confirmed COVID-19 patients in Asia is becoming flatter.

As companies in the region resume activity, they may be the world’s first to shape the “next normal.” What will that look like? Four dimensions could define it:

Rethinking social contracts. In crises, the state plays an essential and expanded role, protecting people and organizing the response. This power shift transforms long-held expectations about the roles of individuals and institutions.
Defining the future of work and consumption. The crisis has propelled new technology across all aspects of Asian life, from e-commerce to remote-working and -learning tools, including Alibaba’s DingTalk, WeChat Work, and Tencent Meeting. New working and shopping practices will probably become a permanent fixture of the next normal.
Mobilizing resources at speed and scale. Within weeks, China added tens of thousands of doctors and hospital beds. Several governments invested in new tools to map transmission and rolled out huge economic-stimulus plans. Asia has a proven ability to mobilize resources in a crisis.
Moving from globalization to regionalization. The pandemic has exposed the world’s risky dependence on vulnerable nodes in global supply chains. China, for example, accounts for about 50 to 70 percent of global demand for copper, iron ore, metallurgical coal, and nickel. We could see a massive restructuring as production and sourcing move closer to end users and companies localize or regionalize their supply chains.
About the authors
Matt Craven, MD, is a partner in McKinsey’s Silicon Valley office. Mihir Mysore is a partner in the Houston office. Shubham Singhal is a senior partner in the Detroit office. Matt Wilson is a senior partner in the New York office.

This article .



COVID-19: Briefing note #4, March 30, 2020
Our latest perspectives on the coronavirus pandemic.
By Matt Craven, Mihir Mysore, Shubham Singhal, Sven Smit, and Matt Wilson
The pandemic continues to expand. More than 175 countries and territories have reported cases of COVID-19, the disease caused by the coronavirus (SARS-CoV-2). Case growth has accelerated to more than 735,000 cases and 35,000 deaths as of March 30. Some geographies have a handful of cases, others with early community transmission have a few hundred, and those with uncontrolled, widespread transmission have tens of thousands. Governments have launched unprecedented public-health and economic responses. The situation evolves by the day.

In this note, we offer some of our latest insights, starting with five likely epidemiologic swing factors that will largely determine the contours of the pandemic in the next year. We then summarize two new articles designed to help senior executives lead through the crisis. In “Beyond coronavirus: The path to the next normal,” we outline five time frames to help leaders organize their thinking and responses. And in “Safeguarding our lives and our livelihoods: The imperative of our time,” we explain how business and society can and must take on both spheres of action, right away. These and many more are available in our collection of coronavirus thinking. We conclude with a short list of the areas in which executives should be concentrating their thought and attention.

Epidemiological swing factors for COVID-19
Every country is looking to join the few that have controlled the epidemic for now and are focusing on preventing a resurgence. The next stages in every country are unknowable (Exhibit 1). But in our view, the spread or control of the virus in the next year comes down to five factors:

Growth of new transmission complexes and evidence of seasonality. While most countries in the world have at least one case, most counts are relatively low. The extent to which these countries follow the path of countries such as Singapore that have achieved rapid control, versus that of western Europe and the United States, will be a major driver of outcomes. Moreover, these geographies also skew to more tropical climates and will provide some evidence on how much of a mitigating effect heat and humidity will have on the coronavirus. If the virus proves to be seasonal, this has the potential to shape both emerging and existing transmission complexes.
Impact of physical-distancing measures. We know that rigorous, at-scale physical-distancing measures can drive a significant reduction in the number of new COVID-19 cases. However, given the range of approaches in use—and the varying stringency with which they are being applied—there’s much still to learn about what exactly works and how long it takes. In the next one to two weeks, we will learn much more, as we begin to see evidence of the impact of physical distancing in Europe and the United States.
Efficacy of health-system surge. As the world has awakened to the potential risks of COVID-19, there has been a massive effort to add capacity to the healthcare system rapidly. This has rightly focused on adding acute-care capacity, providing ventilators, and building stocks of other critical medical supplies, such as personal protective equipment. If this surge (combined with efforts to reduce the demand on the health system) can prevent health systems from being overwhelmed, mortality from COVID-19 will be significantly lower. The development of clinically validated treatments could be a similar boon, but the emerging evidence on that front is mixed, thus far.
Readiness of the health system to navigate recurrence. As authorities begin to think about what’s needed to navigate a postpeak environment, the public-health tools deployed will have a different emphasis from today’s focus in Europe and the United States. They will include at-scale testing, sophisticated real-time surveillance, rigorous contact tracing, and rapid, targeted quarantine to isolate cases and contacts. This mix of tools is how Korea, Singapore, and Taiwan have rapidly contained COVID-19. An antibody test would be a powerful tool in this arsenal, since it would show which people are at risk and which aren’t. Even as public-health authorities negotiate an unprecedented period of demand on the health system, they will need to design and build systems to prevent resurgence of the disease as we pass the peak.
Emergence of herd immunity. Herd immunity occurs when a sufficient portion of the population isn’t susceptible to an infectious disease; at that point, transmission doesn’t propagate, for lack of available hosts. It typically occurs through either widespread exposure or immunization. With a disease as infectious as COVID-19, experts believe that more than two-thirds of the population would need to be immune to create herd immunity.29 But there’s much that we don’t know about the possibility of multiple strains of the virus—and about the duration of human immunity. Answering those questions will have important implications for the course of the pandemic.
Exhibit 1

Two new insights
We have recently published several new articles on the pandemic. Two have captured the attention of leaders worldwide. We summarize them here and invite you to take in the full case in our collection.

‘Beyond coronavirus: The path to the next normal’
By Kevin Sneader and Shubham Singhal

What will it take to navigate this crisis, now that our traditional metrics and assumptions have been rendered irrelevant? More simply put, it’s our turn to answer a question that many of us once asked of our grandparents: What did you do during the war?

Our answer is a call to act across five stages, leading from the crisis of today to the next normal that will emerge after the battle against coronavirus has been won: Resolve, Resilience, Return, Reimagination, and Reform (Exhibit 2).

Exhibit 2

Collectively, these five stages represent the imperative of our time: the battle against COVID-19 is one that leaders today must win if we are to find an economically and socially viable path to the next normal.

‘Safeguarding our lives and our livelihoods: The imperative of our time’
By Sven Smit, Martin Hirt, Kevin Buehler, Susan Lund, Ezra Greenberg, and Arvind Govindarajan

We see enormous energy invested in suppressing the coronavirus, while many urge even faster and more rigorous measures. We also see enormous energy expended on stabilizing the economy through public-policy responses. However, to avoid permanent damage to our livelihoods, we need to find ways to “timebox” this event: we must think about how to suppress the virus and shorten the duration of the economic shock.

To aid decision makers, we have developed scenarios, based on three likely paths for the spread of the virus and the public health response, and three potential levels of effectiveness for governmental economic response (Exhibit 3).

Exhibit 3

Many leaders currently expect one of the scenarios shaded in Exhibit 3 (A1–A4) to materialize. In each of these, the COVID-19 spread is eventually controlled, and catastrophic structural economic damage is avoided. These scenarios describe a global average, while situations will inevitably vary by country and region. But all four of these scenarios lead to V- or U-shaped recoveries.

Other, more extreme scenarios can also be conceived, and some of them are already being discussed (B1–B5 in Exhibit 3). One can’t exclude the possibility of a “black swan of black swans”: structural damage to the economy, caused by a yearlong spread of the virus until a vaccine is widely available, combined with the lack of policy response to prevent widescale bankruptcies, unemployment, and a financial crisis.

Steps to take now
Amid the chaos and all the incoming advice, it’s hard to know exactly what leaders should do today. We suggest they focus their time on four areas:

Support and protect employees in this brave new world. Many institutions have put basic protections in place for their employees and customers. Companies have activated no-travel and work-from-home policies for some workers and physical-distancing-at-work measures for others. The challenge is evolving. For remote workers, interruptions are more frequent than in the office. Making a mental separation from a sometimes-chaotic home life is tough. Workers are finding that they don’t have the skills to be successful in an extended remote environment, from networking to creating routines that drive productivity. They worry that staying remote could make them less valuable, especially in a recessionary environment.

As our colleagues recently explained, three goals are essential. Companies need to increase communication, balancing the needs of the business with expectation setting and morale building, so employees know that their well-being is top of mind. They also need to change working norms, making remote work practical and simple whenever possible. And of course, they must protect people’s health, with whatever measures are appropriate to the workplace: positive hygiene habits, personal protective equipment, amended sick-leave policies—whatever it takes to ensure health and safety.

Monitor leading indicators of how and where the pandemic is evolving and conduct scenario planning using both epidemiological and economic inputs. Earlier, we sketched out the swing factors to watch to understand how the coronavirus pandemic might develop. As companies develop scenarios, they might want to consider the article “Safeguarding our lives and our livelihoods: The imperative of our time,” which details McKinsey’s nine epidemiologic and economic scenarios.
Think about the next horizons of COVID-19. In the urgency of the moment, it’s easy to lose sight of the actions that might be needed tomorrow—and the day after that. The article “Beyond coronavirus: The path to the next normal,” explains the five horizons that every executive should use to ensure an organization’s rapid response, adaptation to change, and reemergence in a position of strength.
Evolve the nerve center to plan for the next phase. Every assumption underpinning a business is open to question. To take one example, we might be in the midst of the largest drawdown in demand since the Second World War. The pendulum might not swing back fully once the outbreak has relented. Having experienced a new way of living, consumers are recalibrating their spending, increasing the likelihood that spending may permanently shift between categories and that online services could get adopted far faster. Decoding this new normal—and ensuring that the company has a strategy to navigate it—is an important part of the work of a nerve center. Approaches such as using a portfolio of initiatives and planning for decision making under uncertainty can go a long way toward creating a compass for business leaders to follow.
The next normal will look unlike any in the years preceding the coronavirus, the pandemic that changed everything. In these briefing notes, we aim to provide leaders with an integrated perspective on the unfolding crisis and insight into the coming weeks and months.

About the authors
Matt Craven is a partner in McKinsey’s Silicon Valley office, Mihir Mysore is a partner in the Houston office, Shubham Singhal is a senior partner in the Detroit office, Sven Smit is a senior partner in the Amsterdam office, and Matt Wilson is a senior partner in the New York office.

This article .



COVID-19: Briefing note #3, March 16, 2020
Current perspectives on the coronavirus outbreak.
At the time of writing, there have been more than 160,000 confirmed cases of COVID-19 and more than 6,000 deaths from the disease. Older people, especially, are at risk (Exhibit 1). More than 140 countries and territories have reported cases; more than 80 have confirmed local transmission. Even as the number of new cases in China is falling (to less than 20, on some days), it is increasing exponentially in Italy (doubling approximately every four days). China’s share of new cases has dropped from more than 90 percent a month ago to less than 1 percent today.

Exhibit 1

WHO declared COVID-19 a pandemic on March 11, 2020. In its message, it balanced the certainty that the coronavirus (SARS-CoV-2) will inevitably spread to all parts of the world, with the observation that governments, businesses, and individuals still have substantial ability to change the disease’s trajectory. In this note, we describe emerging archetypes of epidemic progressions; outline two scenarios for the pandemic and its economic effects; and observe some of the ways that business can improve on its early responses.

Our perspective is based on our analysis of past emergencies and our industry expertise. It is only one view, however. Others could review the same facts and emerge with a different view. Our scenarios should be considered only as two among many possibilities. This perspective is current as of March 16, 2020. We will update it regularly as the outbreak evolves.

Archetypes for epidemic progression
Many countries now face the need to bring widespread community transmission of coronavirus under control. While every country’s response is unique, there are three archetypes emerging—two successful and one not—that offer valuable lessons. We present these archetypes while acknowledging that there is much still to be learned about local transmission dynamics and that other outcomes are possible:

Extraordinary measures to limit spread. After the devastating impact of COVID-19 became evident in the Hubei province, China imposed unprecedented measures—building hospitals in ten days, instituting a “lockdown” for almost 60 million people and significant restrictions for hundreds of millions of others, and using broad-based surveillance to ensure compliance—in an attempt to combat the spread. These measures have been successful in rapidly reducing transmission of the virus, even as the economy has been restarting.
Gradual control through effective use of public-health best practices. South Korea experienced rapid case-count growth in the first two weeks of its outbreak, from about 100 total cases on February 19 to more than 800 new cases on February 29. Since then, the number of new cases has dropped steadily, though not as steeply as in China. This was achieved through rigorous implementation of classic public-health tools, often integrating technology. Examples include rapid and widespread deployment of testing (including the drive-through model) (Exhibit 2), rigorous contact tracing informed by technology, a focus on healthcare-provider safety, and real-time integrated tracking and analytics. Singapore and Taiwan appear to have applied a similar approach, also with broadly successful results.
Exhibit 2

Unsuccessful initial control, leading to overwhelmed health systems. In some outbreaks where case growth has not been contained, hospital capacity has been overwhelmed. The disproportionate impact on healthcare workers and lack of flexibility in the system create a vicious cycle that makes it harder to bring the epidemic under control.
There are also other approaches being considered (such as a focus on reaching herd immunity); the impact of these is unclear.

Two scenarios
Based on new information that emerged last week, we have significantly updated and simplified our earlier scenarios. A number of respected institutions are now projecting very high case counts. The most pessimistic projections typically give the virus full credit for exponential growth but assume that humans will not respond effectively—that is, they assume that many countries will fall into the third archetype described earlier. We believe this is possible but by no means certain. The scenarios below outline two ways that the interplay between the virus and society’s response might unfold and the implications on the economy in each case. Exhibit 3 lays out a number of critical indicators that may provide early notice of which scenario is unfolding.

Exhibit 3

Delayed recovery
Epidemiology. In this scenario, new case counts in the Americas and Europe rise until mid-April. Asian countries peak earlier; epidemics in Africa and Oceania are limited. Growth in case counts is slowed by effective social distancing through a combination of national and local quarantines, employers choosing to restrict travel and implement work-from-home policies, and individual choices. Testing capacity catches up to need, allowing an accurate picture of the epidemic. The virus proves to be seasonal, further limiting its spread. By mid-May, public sentiment is significantly more optimistic about the epidemic. The Southern Hemisphere winter sees an uptick in cases, but by that point, countries have a better-developed playbook for response. While the autumn of 2020 sees a resurgence of infections, better preparedness enables continued economic activity.

Economic impact. Large-scale quarantines, travel restrictions, and social-distancing measures drive a sharp fall in consumer and business spending until the end of Q2, producing a recession. Although the outbreak comes under control in most parts of the world by late in Q2, the self-reinforcing dynamics of a recession kick in and prolong the slump until the end of Q3. Consumers stay home, businesses lose revenue and lay off workers, and unemployment levels rise sharply. Business investment contracts, and corporate bankruptcies soar, putting significant pressure on the banking and financial system.

Monetary policy is further eased in Q1 but has limited impact, given the prevailing low interest rates. Modest fiscal responses prove insufficient to overcome economic damage in Q2 and Q3. It takes until Q4 for European and US economies to see a genuine recovery. Global GDP in 2020 falls slightly.

Prolonged contraction
Epidemiology. In this scenario, the epidemic does not peak in the Americas and Europe until May, as delayed testing and weak adoption of social distancing stymie the public-health response. The virus does not prove to be seasonal, leading to a long tail of cases through the rest of the year. Africa, Oceania, and some Asian countries also experience widespread epidemics, though countries with younger populations experience fewer deaths in percentage terms. Even countries that have been successful in controlling the epidemic (such as China) are forced to keep some public-health measures in place to prevent resurgence.

Economic impact. Demand suffers as consumers cut spending throughout the year. In the most affected sectors, the number of corporate layoffs and bankruptcies rises throughout 2020, feeding a self-reinforcing downward spiral.

The financial system suffers significant distress, but a full-scale banking crisis is averted because of banks’ strong capitalization and the macroprudential supervision now in place. Fiscal and monetary-policy responses prove insufficient to break the downward spiral.

The global economic impact is severe, approaching the global financial crisis of 2008–09. GDP contracts significantly in most major economies in 2020, and recovery begins only in Q2 2021.

Responding to COVID-19: What companies are missing
Our conversations with hundreds of companies around the world on COVID-19 challenges have allowed us to compile a view of the major work streams that companies are pursuing (Exhibit 4).

Exhibit 4

While this list is fairly comprehensive, some companies are taking other steps. However, we have seen evidence that many companies are finding it hard to get the major actions right. We have consistently heard about five challenges.

Having an intellectual understanding isn’t the same as internalizing the reality
Exponential case-count growth is hard to internalize unless you have experienced it before. Managers who haven’t experienced this or been through a “tabletop” simulation are finding it difficult to respond correctly. In particular, escalation mechanisms may be understood in theory, but companies are finding them hard to execute in reality, as the facts on the ground don’t always conform to what it says in the manual. Crisis case studies are replete with examples of managers who chose not to escalate, creating worse issues for their institutions.

Employee safety is paramount, but mechanisms are ineffective
Policy making at many companies is scattershot, especially at those that haven’t yet seen the coronavirus directly. Many, such as professional-services and tech companies, lean very conservative: their protection mechanisms often add to a perception of safety without actually keeping people safer. For instance, temperature checks may not be the most effective form of screening, given that the virus may transmit asymptomatically. Asking employees to stay at home if they are unwell may do more to reduce transmissibility. Such policies are more effective if employees receive compensation protection—and insulation from other consequences too.

Some companies aren’t thinking through the second-order effects of their policies. For example, a ban on travel without a concomitant work-from-home policy can make the office very crowded, leading to higher risk of transmission. Others are adopting company-wide policies without thinking through the needs of each location and each employee segment.

Optimism about the return of demand is dangerous
Being optimistic about demand recovery is a real problem, especially for companies with working-capital or liquidity shortages and those veering toward bankruptcy. Troubled organizations are more likely to believe in a faster recovery—or a shallower downturn. Facing up to the possibility of a deeper, more protracted downturn is essential, since the options available now, before a recession sets in, may be more palatable than those available later. For example, divestments to provide needed cash can be completed at a higher price today than in a few weeks or months.

Assumptions across the enterprise are misaligned
Some companies are pursuing their coronavirus responses strictly within organizational silos (for example, the procurement team is driving supply-chain efforts, sales and marketing teams are working on customer communications, and so on). But these teams have different assumptions and tend to get highly tactical, going deep in their own particular patch of weeds rather than thinking about what other parts of the company are doing—or about what might come next.

The near term is essential, but don’t lose focus on the longer term (which might be worse)
Immediate and effective response is, of course, vital. We think that companies are by and large pursuing the right set of responses, as shown in Exhibit 4. But on many of these work streams, the longer-term dimensions are even more critical. Recession may set in. The disruption of the current outbreak is shifting industry structures. Credit markets may seize up, in spite of stimulus. Supply-chain resilience will be at a premium. It may sound impossible for management teams that are already working 18-hour days, but too few are dedicating the needed time and effort to responses focused on the longer term.

The coronavirus crisis is a story with an unclear ending. What is clear is that the human impact is already tragic, and that companies have an imperative to act immediately to protect their employees, address business challenges and risks, and help to mitigate the outbreak in whatever ways they can.

For the full set of our latest perspectives, please see the attached full briefing materials, which we will update regularly. We welcome your comments and questions at coronavirus_client_response@mckinsey.com.

For more of the latest information on COVID-19, please see reports from the European Centre for Disease Control and Prevention, the US Centers for Disease Control and Prevention, and WHO; and the live tracker of global cases from Johns Hopkins University.

 .



COVID-19: Briefing note #2, March 9, 2020
A range of outcomes is possible. Decision makers should not assume the worst.
Less than ten weeks have passed since China reported the existence of a new virus to the World Health Organization. This virus, now known as SARS-CoV-2, causing COVID-19 disease, spread quickly in the city of Wuhan and throughout China. The country has experienced a deep humanitarian challenge, with more than 80,000 cases and more than 3,000 deaths. COVID-19 progressed quickly beyond China’s borders. Four other major transmission complexes are now established across the world: East Asia (especially South Korea, with more than 7,000 cases, as well as Singapore and Japan), the Middle East (centered in Iran, with more than 6,500 cases), Europe (especially the Lombardy region in northern Italy, with more than 7,300 cases, but with widespread transmission across the continent), and the United States, with more than 200 cases. Each of these transmission complexes has sprung up in a region where millions of people travel every day for social and economic reasons, making it difficult to prevent the spread of the disease. In addition to these major complexes, many other countries have been affected. Exhibit 1 (see an updated version of the exhibit here) offers a snapshot of the current progress of the disease and its economic impact.

The next phases of the outbreak are profoundly uncertain. In our view, the prevalent narrative, focused on pandemic, to which both markets and policy makers have gravitated as they respond to the virus, is possible but underweights the possibility of a more optimistic outcome. In , we attempt to distinguish the things we know from those we don’t, and the potential implications of both sets of factors. We then outline three potential economic scenarios, to illustrate the range of possibilities, and conclude with some discussion of the implications for companies’ supply chains, and seven steps businesses can take now to prepare.

Our perspective is based on our analysis of past emergencies and on our industry expertise. It is only one view, however. Others could review the same facts and emerge with a different view. Our scenarios should be considered only as three among many possibilities. This perspective is current as of March 9, 2020. We will update it regularly as the outbreak evolves.

What we know, and what we are discovering
What we know. Epidemiologists are in general agreement on two characteristics of COVID-19:

The virus is highly transmissible. Both observed experience and emerging scientific evidence show that the virus causing COVID-19 is easily transmitted from person to person. The US Centers for Disease Control and Prevention estimates that the virus’s reproduction number (the number of additional cases that likely result from an initial case) is between 1.6 and 2.4, making COVID-19 significantly more transmissible than seasonal flu (whose reproduction number is estimated at 1.2 to 1.4) (Exhibit 2).
Exhibit 2

The virus disproportionately affects older people with underlying conditions. Epidemiologists Zunyou Wu and Jennifer McGoogan analyzed a report from China Centers for Disease Control and Prevention that looked at more than 72,000 cases and concluded that the fatality rate for patients 80 and older was seven times the average, and three to four times the average for patients in their 70s.30 Other reports describe fatality rates for people under 40 to be 0.2 percent.
What we are still discovering. Three characteristics of the virus are not fully understood, but are key variables that will affect how the disease progresses, and the economic scenario that evolves:

The extent of undetected milder cases. We know that those infected often display only mild symptoms (or no symptoms at all), so it is easy for public-health systems to miss such cases. For example, 55 percent of the cases on board the Diamond Princess cruise ship did not exhibit significant symptoms (even though many passengers were middle-aged or older). But we don’t know for sure whether official statistics are capturing 80 percent, 50 percent, or 20 percent of cases.
Seasonality. There is no evidence so far about the virus’s seasonality (that is, a tendency to subside in the northern hemisphere as spring progresses). Coronaviruses in animals are not always seasonal but have historically been so in humans for reasons that are not fully understood. In the current outbreak, regions with higher temperatures (such as Singapore, India, and Africa) have not yet seen a broad, rapid propagation of the disease.
Asymptomatic transmission. The evidence is mixed about whether asymptomatic people can transmit the virus, and about the length of the incubation period. If asymptomatic transfer is a major driver of the epidemic, then different public-health measures will be needed.
These factors notwithstanding, we have seen that robust public-health responses, like those in China outside Hubei and in Singapore, can help stem the epidemic. But it remains to be seen how these factors will play out and the direct impact they will have. The economic impact too will vary considerably.

Economic impact
In our analysis, three broad economic scenarios might unfold: a quick recovery, a global slowdown, and a pandemic-driven recession. Here, we outline all three. We believe that the prevalent pessimistic narrative (which both markets and policy makers seem to favor as they respond to the virus) underweights the possibility of a more optimistic outcome to COVID-19 evolution.

Quick recovery
In this scenario, case count continues to grow, given the virus’s high transmissibility. While this inevitably causes a strong public reaction and drop in demand, other countries are able to achieve the same rapid control seen in China, so that the peak in public concern comes relatively soon (within one to two weeks). Given the low fatality rates in children and working-age adults, we might also see levels of concern start to ebb even as the disease continues to spread. Working-age adults remain concerned about their parents and older friends, neighbors, and colleagues, and take steps to ensure their safety. Older people, especially those with underlying conditions, pull back from many activities. Most people outside the transmission complexes continue their normal daily lives.

The scenario assumes that younger people are affected enough to change some daily habits (for example, they wash hands more frequently) but not so much that they shift to survival mode and take steps that come at a higher cost, such as staying home from work and keeping children home from school. A complicating factor, not yet analyzed, is that workers in the gig economy, such as rideshare drivers, may continue to report to work despite requests to stay home, lest they lose income. This scenario also presumes that the virus is seasonal.

In this scenario, our model developed in partnership with Oxford Economics suggests that global GDP growth for 2020 falls from previous consensus estimates of about 2.5 percent to about 2.0 percent. The biggest factors are a fall in China’s GDP from nearly 6 percent growth to about 4.7 percent; a one-percentage-point drop in GDP growth for East Asia; and drops of up to 0.5 percentage points for other large economies around the world. The US economy recovers by the end of Q1. By that point, China resumes most of its factory output; but consumer confidence there does not fully recover until end Q2. These are estimates, based on a particular scenario. They should not be considered predictions.

Global slowdown
This scenario assumes that most countries are not able to achieve the same rapid control that China managed. In Europe and the United States, transmission is high but remains localized, partly because individuals, firms, and governments take strong countermeasures (including school closings and cancellation of public events). For the United States, the scenario assumes between 10,000 and 500,000 total cases. It assumes one major epicenter with 40 to 50 percent of all cases, two or three smaller centers with 10 to 15 percent of all cases, and a “long tail” of towns with a handful or a few dozen cases. This scenario sees some spread in Africa, India, and other densely populated areas, but the transmissibility of the virus declines naturally with the northern hemisphere spring.

This scenario sees much greater shifts in people’s daily behaviors. This reaction lasts for six to eight weeks in towns and cities with active transmission, and three to four weeks in neighboring towns. The resulting demand shock cuts global GDP growth for 2020 in half, to between 1 percent and 1.5 percent, and pulls the global economy into a slowdown, though not recession.

In this scenario, a global slowdown would affect small and mid-size companies more acutely. Less developed economies would suffer more than advanced economies. And not all sectors are equally affected in this scenario. Service sectors, including aviation, travel, and tourism, are likely to be hardest hit. Airlines have already experienced a steep fall in traffic on their highest-profit international routes (especially in Asia–Pacific). In this scenario, airlines miss out on the summer peak travel season, leading to bankruptcies (FlyBe, the UK regional carrier, is an early example) and consolidation across the sector. A wave of consolidation was already possible in some parts of the industry; COVID-19 would serve as an accelerant.

In consumer goods, the steep drop in consumer demand will likely mean delayed demand. This has implications for the many consumer companies (and their suppliers) that operate on thin working-capital margins. But demand returns in May–June as concern about the virus diminishes. For most other sectors, the impact is a function primarily of the drop in national and global GDP, rather than a direct impact of changed behaviors. Oil and gas, for instance, will be adversely affected as oil prices stay lower than expected until Q3.

Pandemic and recession
This scenario is similar to the global slowdown, except it assumes that the virus is not seasonal (unaffected by spring in the northern hemisphere). Case growth continues throughout Q2 and Q3, potentially overwhelming healthcare systems around the world and pushing out a recovery in consumer confidence to Q3 or beyond. This scenario results in a recession, with global growth in 2020 falling to between –1.5 percent and 0.5 percent.

Supply-chain challenges
For many companies around the world, the most important consideration from the first ten weeks of the COVID-19 outbreak has been the effect on supply chains that begin in or go through China. As a result of the factory shutdowns in China during Q1, many disruptions have been felt across the supply chain, though the full effects are of course still unclear.

Hubei is still in the early phases of its recovery; case count is down, but fatality rates remain high, and many restrictions remain that will prevent a resumption of normal activity until early Q2. In the rest of China, however, many large companies report that they are running at more than 90 percent capacity as of March 1. While some real challenges remain, such as lower than usual availability of migrant labor, there is little question that plants are returning back to work quickly.

Trucking capacity to ship goods from factories to ports is at about 60 to 80 percent of normal capacity. Goods are facing delays of between eight and ten days on their journey to ports.

The Baltic Dry Index (which measures freight rates for grains and other dry goods around the world) dropped by about 15 percent at the onset of the outbreak but has increased by nearly 30 percent since then. The TAC index, which measures air-freight prices, has also risen by about 15 percent since early February.

In the next few months, the phased restart of plants outside Hubei (and the slower progress of plants within Hubei) is likely to lead to challenges in securing critical parts. As inventories are run down faster, parts shortages are likely to become the new reason why plants in China cannot operate at full capacity. Moreover, plants that depend on Chinese output (which is to say, most factories around the world) have not yet experienced the brunt of the initial Chinese shutdown and are likely to experience inventory “whiplash” in the coming weeks.

Perhaps the biggest uncertainty for supply-chain managers and production heads is customer demand. Customers that have prebooked logistics capacity may not use it; customers may compete for prioritization in receiving a factory’s output; and the unpredictability of the timing and extent of demand rebound will mean confusing signals for several weeks.

Responding to COVID-19
In our experience, seven actions can help businesses of all kinds. We outline them here as an aid to leaders as they think through crisis management for their companies. These are only guidelines; they are by no means exhaustive or detailed enough to substitute for a thorough analysis of a company’s particular situation.

Protect your employees. The COVID-19 crisis has been emotionally challenging for many people, changing day-to-day life in unprecedented ways. For companies, business as usual is not an option. They can start by drawing up and executing a plan to support employees that is consistent with the most conservative guidelines that might apply and has trigger points for policy changes. Some companies are actively benchmarking their efforts against others to determine the right policies and levels of support for their people. Some of the more interesting models we have seen involve providing clear, simple language to local managers on how to deal with COVID-19 (consistent with WHO, CDC, and other health-agency guidelines) while providing autonomy to them so they feel empowered to deal with any quickly evolving situation. This autonomy is combined with establishing two-way communications that provide a safe space for employees to express if they are feeling unsafe for any reason, as well as monitoring adherence to updated policies.

Set up a cross-functional COVID-19 response team. Companies should nominate a direct report of the CEO to lead the effort and should appoint members from every function and discipline to assist. Further, in most cases, team members will need to step out of their day-to-day roles and dedicate most of their time to virus response. A few workstreams will be common for most companies: a) employees’ health, welfare, and ability to perform their roles; b) financial stress-testing and development of a contingency plan; c) supply-chain monitoring, rapid response, and long-term resiliency (see below for more); d) marketing and sales responses to demand shocks; and e) coordination and communication with relevant constituencies. These subteams should define specific goals for the next 48 hours, adjusted continually, as well as weekly goals, all based on the company’s agreed-on planning scenario. The response team should install a simple operating cadence and discipline that focuses on output and decisions, and does not tolerate meetings that achieve neither.

Ensure that liquidity is sufficient to weather the storm. Businesses need to define scenarios tailored to the company’s context. For the critical variables that will affect revenue and cost, they can define input numbers through analytics and expert input. Companies should model their financials (cash flow, P&L, balance sheet) in each scenario and identify triggers that might significantly impair liquidity. For each such trigger, companies should define moves to stabilize the organization in each scenario (optimizing accounts payable and receivable; cost reduction; divestments and M&A).

Stabilize the supply chain. Companies need to define the extent and likely duration of their supply-chain exposure to areas that are experiencing community transmission, including tier-1, -2, and -3 suppliers, and inventory levels. Most companies are primarily focused on immediate stabilization, given that most Chinese plants are currently in restart mode. They also need to consider rationing critical parts, prebooking rail/air-freight capacity, using after-sales stock as a bridge until production restarts, gaining higher priority from their suppliers, and, of course, supporting supplier restarts. Companies should start planning how to manage supply for products that may, as supply comes back on line, see unusual spikes in demand due to hoarding. In some cases, medium or longer-term stabilization may be warranted, which calls for updates to demand planning, further network optimization, and searching for and accelerating qualification of new suppliers. Some of this may be advisable anyway, absent the current crisis, to ensure resilience in their supply chain—an ongoing challenge that the COVID-19 situation has clearly highlighted.

Stay close to your customers. Companies that navigate disruptions better often succeed because they invest in their core customer segments and anticipate their behaviors. In China, for example, while consumer demand is down, it has not disappeared—people have dramatically shifted toward online shopping for all types of goods, including food and produce delivery. Companies should invest in online as part of their push for omnichannel distribution; this includes ensuring the quality of goods sold online. Customers’ changing preferences are not likely to go back to pre-outbreak norms.

Practice the plan. Many top teams do not invest time in understanding what it takes to plan for disruptions until they are in one. This is where roundtables or simulations are invaluable. Companies can use tabletop simulations to define and verify their activation protocols for different phases of response (contingency planning only, full-scale response, other). Simulations should clarify decision owners, ensure that roles for each top-team member are clear, call out the “elephants in the room” that may slow down the response, and ensure that, in the event, the actions needed to carry out the plan are fully understood and the required investment readily available.

Demonstrate purpose. Businesses are only as strong as the communities of which they are a part. Companies need to figure out how to support response efforts—such as by providing money, equipment, or expertise. For example, a few companies have shifted production to create medical masks and clothing.

The checklist in Exhibit 3 can help companies make sure they are doing everything necessary.

Exhibit 3

 .



COVID-19: Briefing note #1, March 2, 2020
The following is McKinsey’s perspective as of March 2, 2020.
What we know about the outbreak
COVID-19 crossed an inflection point during the week of February 24, 2020. Cases outside China exceeded those within China for the first time, with 54 countries reporting cases as of February 29. The outbreak is most concentrated in four transmission complexes—China (centered in Hubei), East Asia (centered in South Korea and Japan), the Middle East (centered in Iran), and Western Europe (centered in Italy). In total, the most-affected countries represent nearly 40 percent of the global economy. The daily movements of people and the sheer number of personal connections within these transmission complexes make it unlikely that COVID-19 can be contained. And while the situation in China has stabilized with the implementation of extraordinary public-health measures, new cases are also rising elsewhere, including Latin America (Brazil), the United States (California, Oregon, and Washington), and Africa (Algeria and Nigeria). The US Centers for Disease Control and Prevention has set clear expectations that the United States will experience community transmission, and evidence is emerging that it may be happening already.

While the future is uncertain, it is likely that countries in the four mature transmission complexes will see continued case growth; new complexes may emerge. This could contribute to a perception of “leakage,” as the public comes to believe that the infections aren’t contained. Consumer confidence, especially in those complexes, may erode, and could be further weakened by restrictions on travel and limits on mass gatherings. China will mostly likely recover first, but the global impact will be felt much longer. We expect a slowdown in global growth for 2020. In what follows, we review the two most likely scenarios for economic impact and recovery and provide insights and best practices on how business leaders can navigate this uncertain and fast-changing situation.

Economic impact
In our base-case scenario, continued spread within established complexes, as well as community transmission in new complexes, drives a 0.3- to 0.7-percentage-point reduction in global GDP growth for 2020. China, meanwhile, continues on its path to recovery, achieving a near-complete economic restart by mid-Q2 (in spite of the current challenges of slow permissions and lack of migrant-worker capacity). As other geographies experience continued case growth, it is likely that movement restrictions will be imposed to attempt to stop or slow the progression of the disease. This will almost certainly drive a sharp reduction in demand, which in turn lowers economic growth through Q2 and early Q3. Demand recovery will depend on a slowing of case growth, the most likely cause of which would be “seasonality”—a reduction in transmissions similar to that seen with influenza in the northern hemisphere as the weather warms. Demand may also return if the disease’s fatality ratio proves to be much lower than we are currently seeing.

Regions that have not yet seen rapid case growth (such as the Americas) are increasingly likely to see more sustained community transmission (for example, expansion of the emergency clusters in the western United States). Greater awareness of COVID-19, plus additional time to prepare, may help these complexes manage case growth. However, complexes with less robust health systems could see more general transmission. Lower demand could slow growth of the global economy between 1.8 percent and 2.2 percent instead of the 2.5 percent envisioned at the start of the year.

Unsurprisingly, sectors will be affected to different degrees. Some sectors, like aviation, tourism, and hospitality, will see lost demand (once customers choose not to eat at a restaurant, those meals stay uneaten). This demand is largely irrecoverable. Other sectors will see delayed demand. In consumer goods, for example, customers may put off discretionary spending because of worry about the pandemic but will eventually purchase such items later, once the fear subsides and confidence returns. These demand shocks—extended for some time in regions that are unable to contain the virus—can mean significantly lower annual growth. Some sectors, such as aviation, will be more deeply affected.

In the pessimistic scenario, case numbers grow rapidly in current complexes and new centers of sustained community transmission erupt in North America, South America, and Africa. Our pessimistic scenario assumes that the virus is not highly seasonal, and that cases continue to grow throughout 2020. This scenario would see significant impact on economic growth throughout 2020, resulting in a global recession.

In both the base-case and pessimistic scenarios, in addition to facing consumer-demand headwinds, companies will need to navigate supply-chain challenges. Currently, we see that companies with strong, centralized procurement teams and good relationships with suppliers in China are feeling more confident about their understanding of the risks these suppliers face (including tier-2 and tier-3 suppliers). Others are still grappling with their exposure in China and other transmission complexes. Given the relatively quick economic restart in China, many companies are focused on temporary stabilization measures rather than moving supply chains out of China. COVID-19 is also serving as an accelerant for companies to make strategic, longer-term changes to supply chains—changes that had often already been under consideration.

To better understand which scenario may prevail, planning teams can consider a set of leading indicators like those in the exhibit (see an updated version of the exhibit here).
"""

article11 = """
Strengthening institutional risk and integrity culture

Many of the costliest risk and integrity failures have cultural weaknesses at their core. Here is how leading institutions are strengthening their culture and sustaining the change.
DOWNLOADS
Open interactive popup
 Article (8 pages)
The COVID-19 pandemic has created a time of unprecedented change for both public and private organizations across the globe. Executives and boards have had to move quickly to address threats and seize opportunities, all while continuing to protect employee and customer health and safety and evolving to adopt new digital and work-from-home norms.

Sidebar
About the authors

Risk and integrity culture refers to the mindsets and behavioral norms that determine how an organization identifies and manages risk. In this challenging and highly uncertain moment, risk culture is more important than ever. Companies cannot rely on reflexive muscles for predicting and controlling risks. A good risk culture allows an organization to move with speed without breaking things. It is an organization’s best cross-cutting defense.

MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
Beyond today’s travails, a strong risk culture is a critical element to institutional resilience in the face of any challenge. In our experience, those organizations that have developed a mature risk and integrity culture outperform peers through economic cycles and in the face of challenging external shocks. At the same time, companies with strong risk cultures are less likely to suffer from self-inflicted wounds, in the form of operational mistakes or reputational difficulties, and have more engaged and satisfied customers and employees.

This article explores the steps involved in setting up an effective risk-culture program, when to launch such a program, and the factors we have found to be critical for long-term success.

Understanding and measuring risk culture
The starting point for most organizations looking to improve their risk culture is to diagnose the current state. Organizations that have built strong risk and integrity cultures seek to understand (and then address) three mutually reinforcing drivers: risk mindsets, risk practices, and contributing behavior.

Risk mindsets can be understood as the set of assumptions about risk that individuals hold within the organization; risk practices are the daily actions that determine the effectiveness of risk management; contributing behavior comprises the collective actions that build risk attitudes. Ideally, these actions will be systematic and deliberately intended to strengthen individuals’ risk attitudes, with desired risk behavior built into everyday functioning.

Concrete definition
Companies that seek to understand risk culture can best begin by establishing concrete, detailed definitions. They should clearly spell out the specific elements of risk culture to set aspirations and measure progress. For example, we define ten dimensions of risk culture, based on a wide range of experiences with companies across all major industries, and incorporating close study of a range of real-world risk-culture failings (Exhibit 1).

Exhibit 1

Systematic measurement
Once risk and integrity culture is defined, measurement can begin. Leading companies assess themselves systematically, looking at mindsets, practices, and behavior.

Companies with strong risk cultures have more engaged and satisfied customers and employees.

This assessment is often based on interviews among units and functions, then followed by a more comprehensive organization-wide survey.

The survey will typically include 20 to 30 questions that measure performance against the elements of risk culture (covering mindsets, practices, and behavior) and will set the organization-wide baseline. The team can complement results with qualitative insights gleaned from follow-up interviews to provide further detail on the particular strengths or weaknesses revealed, and help uncover their root causes.

Leading companies take proactive steps to maintain strong risk cultures in normal times, in times of stress, and when they are undergoing transformations.

Instead of using a dedicated risk and integrity survey, many organizations falter by relying on a combination of employee-engagement surveys, focus groups, and analyses of incidents and near-misses to measure their risk culture. Each of these tools can bring useful results when used with sufficient rigor. However, typical employee-engagement surveys contain only a few relevant questions and therefore do not usually uncover enough insight to create an effective measure. These approaches, furthermore, do not provide a view over time or ready comparisons between organizational units.

We believe that a dedicated survey is an indispensable tool for obtaining a broad measure of a company’s risk culture. It is the only way to set a true initial baseline. A comprehensive survey creates hard data, comparable across divisions, geographies, and roles; with repeated use, it traces trends through time. The results allow fact-based conversations about risk culture, fostering engagement while deepening executive-level understanding.

Sharing results
Once an initial baseline is developed, the results should be shared with leadership teams and the broader organization. Transparent results are an important first step in increasing the focus on risk culture. While maturity levels across different dimensions matter, outliers (both strengths and weaknesses) or areas of change where a survey is repeated over time tend to drive the greatest insights for an organization. Differences among units, functions, geographies, and tenure levels can also be illuminating.

In one example of this process, a government-owned corporation held a series of town-hall meetings to share the results of its risk-culture survey. The town halls were the first active communications on risk culture and demonstrated to employees a new openness. The comparative data shared showed divergent strengths and weaknesses, which stimulated strong interdepartmental conversations in what was a traditionally siloed organization.

As a second example, a high-performing financial institution created tailored readout packs for a series of thoughtful discussions between the chief risk officer and the leader of each major line of business and function. The readout materials highlighted areas of opportunity for each business and function, including dimensions where their risk culture was weaker than the organization as a whole or where results were at odds with stated strengths or goals of the leader. For instance, with one leader who had taken pride in his organization’s openness to sharing bad news, the conversation centered around weak scores in this area in some geographies.

Addressing risk-culture shortcomings
With the help of measured risk-culture results, companies can act to address weaknesses in risk culture. The leadership team, with support from the team coordinating risk-culture efforts, can use the strengths, weakness, and cultural differences identified to agree on a set of prioritized interventions or intervention areas based on enterprise-wide and divisional aspirations.

Some interventions will affect the entire organization—for example, certain compensation or recruiting changes. These warrant group-led approaches, and a dedicated team should be created or assigned to take charge of them.

Many, however, will be specific to and driven by particular parts of the organization. For instance, affected business units would take charge of work to redesign problematic product-approval processes; likewise, business-unit leaders might “localize” a groupwide focus on a topic like accountability. Where possible, interventions or their application should be driven, and owned, by the front line to ensure that cultural change is truly lived locally and linked to day-to-day business activities and outcomes. Successes and lessons from these localized efforts can be shared across the organization by a central coordinating team.

The process of developing interventions end to end is well illustrated by the experience of one insurance company. The company explored the results of an initial risk-culture survey at a top-team offsite. The survey data allowed leaders to move from discussions based on intuition to those based on evidence. The leaders discovered that the organization was universally strong in some dimensions and universally weak in others. Clear differences also emerged among business units. The CEO probed the comparative differences, challenged executives to understand the causes of low scores, and explored ways for everyone to learn and apply lessons from higher-performing business units. Coming out of the discussions, the team agreed on focus areas and assigned responsibility for carrying out the improvements.

Designing and deploying tailored interventions
To lift risk culture, organizations move from measuring and planning to taking action. A broad range of techniques can be summoned to inspire change. Successful efforts are usually the result of several kinds of actions taken together. In thinking about how to generate meaningful, lasting changes in risk and integrity culture, leaders can be guided by the “influence model” schematized in Exhibit 2. This model has proven useful in ensuring that change programs draw upon a breadth of approaches, and its use increases the chance of success for a transformation by three or four times.

Exhibit 2

The effort to address risk-culture gaps usually involves a balance of short- and long-term interventions. Targeted short-term interventions allow organizations to respond flexibly to changing needs while longer-term programs constantly reinforce core elements of desired risk culture. Long-term interventions are often formal programs like speak-up hotlines or training and compensation standards (based on risk criteria) that continually reinforce desired behaviors.

In an effective example of a long-term intervention, one bank developed a program that both encouraged employees to speak up on risk issues and increased the level of responsive actions. The program includes an externally managed channel for employees to register concerns, with the option of confidential help from internal speak-up champions on navigating the process. The board receives regular reports on both internal and external complaints, with resolution rates and common themes and trends.

The following short-term initiatives are just a few examples of how organizations have addressed gaps in risk culture:

A government agency developed a short-term program to increase its speed of response, which was identified as a major weakness. This was done with walk-throughs of key processes, which identified bottlenecks; components were then redesigned as needed to speed up the process and ensure future clarity on escalation and resolution.
A bank discovered weaknesses in its approval process for new products. Its investigation led to the creation of a dedicated challenger role, filled by rotating members of the approval committee. The role is charged with taking deliberately contrarian positions and pressure-testing proposed products on how well they served the long-term interests of the customer and the bank.
A pharmaceutical company sought to address a weak culture of challenge by training new and junior colleagues on how to constructively question leadership decisions. To encourage the best results, senior leaders acted as role models, visibly promoting nonhierarchical decision making.
Launching a risk-culture program
Risk-culture programs can have multiple triggers. Leading companies take proactive steps to maintain strong risk cultures in normal times, in times of stress (such as under the COVID-19 crisis), and when they are undergoing transformations.

Proactively shaping risk culture
Building and sustaining strong risk culture requires proactive attention. In normal times, this means addressing risk culture before issues arise. Under the stress of the COVID-19 pandemic, which has disrupted the traditional mechanisms that reinforce an organization’s risk culture, this includes understanding how risk culture is evolving and then taking action to protect or improve it. Because of the pandemic, people are working together differently, often from home. In addition, many individuals and organizations are under added stress (including financial stress), increasing the risk of nearsighted decision making and cultural problems.

Once a crisis with roots in risk culture hits, existing leadership, including boards, will find it difficult to lead change as they themselves become increasingly associated with the cultural problems. The problems tend to be seen as leadership failings in the eyes of the public, investors, and regulators.

By taking a preemptive look, leaders might see early signs of concern or inadequate processes for understanding the state of risk culture. An initial deep dive into the root causes of seemingly isolated incidents or complaints can be a starting point, eventually expanded into a broader risk-culture review to build a comprehensive picture. Today, the preemptive look should also seek to understand the impact the COVID-19 crisis is having on employees and develop interventions to strengthen the culture by filling the gaps created by remote working.

The effort might be triggered by the need to understand whether an organization is vulnerable to incidents experienced by peers, either before or during the pandemic. By proactively driving this topic, leaders can avoid larger problems and demonstrate that they are part of the solution and not the problem. For example, a company in the advanced industries sector built a speak-up program after leadership recognized the devastating impact of other failures in the industry. The leaders methodically created formal mechanisms to support desired behavior, helping to ward off potential crises before the point of no return was reached.

Maintaining risk culture under company transformation
Many organizations are transforming their operations, particularly to become more digital and more efficient. The COVID-19 crisis has served to accelerate many planned change programs. Large transformations can themselves raise risk levels, as risk-management practices are disrupted, core processes are redesigned, and teams and organizational structures shift. “Change fatigue,” a species of anxiety that comes with a transformation, can contribute its own share of risk. But transformations also afford organizations the opportunity to reset their model to their desired risk-management culture. They must include programs to promote desired behaviors, in transparent, organization-wide efforts, as opposed to siloed, business-as-usual approaches.

For example, one global manufacturing company undertook a major transformation in response to a series of product- and regulatory-compliance incidents. Front and center were issues of culture, integrity, and compliance, which became the core focus of the groupwide transformation.

As a second example, a bank undertook a major transformation and restructuring effort, partly in response to COVID-19-triggered considerations. The program included a dedicated cultural component with a specific risk-culture stream. As the transformation progressed, business units incorporated risk-culture initiatives into their broader program of activities, ensuring risk-culture changes became core elements of the new ways of working.

Getting started
Whatever the original motivation for a risk-culture program, a one- or two-year plan covering a range of intervention types can begin with a small set of priority initiatives targeting key weaknesses. In addition to achieving progress in important areas, these initiatives will create visibility and momentum for the entire plan. An example campaign would be one to encourage employees to speak up where they see risk concerns. The initiative might include a confidential speak-up line, communications from the top to set the tone on the importance of speaking up, and, for a dedicated period, an explicit focus on speaking up in team meetings. Results would be conveyed to the board, in a report covering internal and external complaints, whistleblower activity, overarching themes, and resolutions. This would serve as a first step and a gesture of commitment to the larger effort of changing risk culture.

Setting yourself up for risk-culture success
Careful risk-culture definition, measurement, and initiative work plans are not enough. Successful risk-culture programs share five essential characteristics that leaders should put in place as part of their focus on risk culture:

True ownership and responsibility for risk culture sits with the front line. To be truly lived, culture must be linked with the day-to-day business activities and outcomes of an institution. First-line leaders must feel accountability for their role in supporting the company’s risk culture.
Dedicated ownership is assigned for coordinating the definition, measurement, reporting, and reinforcement of risk culture. These responsibilities should sit centrally—either within enterprise risk management, with a risk chief operating officer or an enterprise chief operating officer, or within HR. It is helpful to have a central point, as too often varying language is used to discuss culture within a bank. Without an enterprise-wide view and vocabulary, it is not possible to effect true, coordinated cultural change.
The case for change is visible and compelling. The strengths and weaknesses of the prevailing risk and integrity culture need to be spelled out, supported by data. The vision for an enhanced culture and how it will benefit the organization and individuals can then be articulated.
The effort is sustained over time. Cultural change takes time, and gains must be regularly reinforced. Successful programs combine periodic measurement of organizational risk culture with a multiyear change program encompassing short- and long-term initiatives. Too often organizations bring a burst of energy to the initial diagnostic but then fail to implement initiatives or sustain the changes needed to drive long-term improvement.
The C-suite holds leaders accountable for success. Risk-culture programs need someone to provide overarching direction and drive, but to succeed, leadership across the organization should be actively engaged. Business-unit owners in particular should champion initiatives. Leaders need to show they are serious about change if they want their people to adopt new risk behaviors, which may themselves be perceived as risky—for example, speaking up.
As senior leaders navigate the complexity of the current crisis, they must ensure the organization as a whole maintains its cultural health. Organizations that nurture their risk and integrity culture will be better positioned to serve their clients, team members, and society effectively, and to avert risks that could potentially prove catastrophic. By taking the steps outlined above, institutions can prepare, reap near-term rewards, and be ready for future uncertainties and challenges.
"""

article12 = """
When nothing is normal: Managing in extreme uncertainty

In this uniquely severe global crisis, leaders need new operating models to respond quickly to the rapidly shifting environment and sustain their organizations through the trials ahead.
By Patrick Finn, Mihir Mysore, and Ophelia Usher
Open interactive popup
When nothing is normal: Managing in extreme uncertainty
Open interactive popup
DOWNLOADS
Open interactive popup
 Article (8 pages)
In normal times organizations face numerous uncertainties of varying consequence. Managers deal with challenges by relying on established structures and processes. These are designed to reduce uncertainty and support calculated bets to manage the residual risks. In a serious crisis, however, uncertainty can reach extreme levels, and the normal way of working becomes overstrained. At such times traditional management operating models rarely prove adequate, and organizations with inadequate processes can quickly find themselves facing existential threats.

  22:56
Audio
Listen to this article
Uncertainty can be measured in magnitude and duration. By both measures, the extreme uncertainty accompanying the public-health and economic damage created by the COVID-19 pandemic is unprecedented in modern memory. It should not be surprising, therefore, that organizations need a new management model to sustain operations under such conditions. The magnitude of the uncertainty organizations face in this crisis—defined partly by the frequency and extent of changes in information about it—means that this operating model must enable continuous learning and flexible responses as situations evolve. The duration of the crisis, furthermore, has already exceeded the early predictions of many analysts; business planners are now expecting to operate in crisis mode for an extended period. Leaders should therefore begin assembling the foundational elements of this operating model so that they can steer their organizations under conditions of extreme uncertainty.

Understanding extreme uncertainty
MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
Due to the severity of this crisis, many organizations are in a struggle for their existence. An existential crisis puts at stake the organization’s survival in recognizable form. Readers can probably call to mind numerous individual companies that faced such crises in the recent past. The crises may have been touched off by single catastrophic incidents or by series of failures; the sources are familiar—cyber breaches, financial malfeasance, improper business practices, safety failures, and natural or human-caused disasters. Effective action saved many; others spiraled downward.

Existential crises subject organizations to both extreme uncertainty and severe material consequences; they are often new and unfamiliar and can unfold quickly. In business terms, the present crisis more closely resembles economic crises of the past. In the financial crisis of 2008–09, for example, many organizations were simultaneously affected. Qualitatively, however, the present crisis is far more severe.

The COVID-19 pandemic and the resulting economic recession have affected most large organizations around the world. Managers continue to scramble to address rapidly developing changes in the public-health environment, public policy, and customer behavior. And then there is the economic uncertainty. The severity and speed of the crisis is reflected in the International Monetary Fund’s (IMF) projections for US GDP growth. After an estimated GDP expansion of 2.2 percent in 2019 (year-on-year), the US economy, in the IMF’s view, was expected to grow at a rate of 2.1 percent in 2020 (forecast of October 2019). With the onset of the pandemic, the IMF quickly shifted its estimate into contraction, of –5.9 percent in April 2020, revised to –8.0 percent in June. The latest estimate (October 2020) is less severe at –4.3 percent, but this would still be the worst result in many decades. The forecasting institution foresees the world economy shrinking at a rate of –4.4 percent in 2020, after having grown 2.8 percent in 2019 (estimate).1
Exhibit

Uncertainty levels from recent global shocks do not approach those of the present COVID-19-triggered crisis. The IMF’s GDP contraction forecast for 2020 is more than double the estimated contraction that took place in 2009, the worst year of the earlier global financial crisis. As measured by the Economic Policy Uncertainty Index, a metric developed jointly by researchers at several US business schools, uncertainty on a daily basis has been elevated for nearly 200 days’ running. By contrast, commensurate uncertainty was experienced during the 2008–09 financial crisis a few times for a maximum of 27 consecutive days. The COVID-19 outbreak already accounts for seven of the ten highest-ever daily readings.2 Crises such as Hurricane Katrina or the Fukushima Daiichi nuclear disaster cause high levels of uncertainty for individual communities or particular industries. Since the uncertainty is confined by industry or geography, the magnitude decreases steadily with time. In the present crisis, however, elevated uncertainty is globally pervasive, and events trigger compounding effects. The following exhibit conveys a range of crises and their corresponding levels of uncertainty.

Why existing operating models fail
Extreme uncertainty on a global scale is rare; however, existential crises at the organizational or community level are more frequent and thus provide lessons concerning which operating models succeed and fail during periods of uncertainty. Many organizations, including publicly traded companies, operate on an annual-planning cycle. Managers collectively decide on strategies, budgets, and operating plans once a year and then manage operations in accordance with those goals and cost limits. Between annual-planning cycles, amendments are few and usually minor. The assumptions shape how managers engage with each other: from the content of status reports to interdepartmental information sharing to the timing and structure of management meetings. Recently, some organizations have adopted more agile techniques to make planning more flexible and responsive to outcomes from pilots or trials. However, the approach is rarely deployed in the C-suite to manage the whole organization.

The COVID-19 crisis has undermined most of the assumptions of the traditional planning cycle. Existing management operating models are no longer supporting managers effectively in addressing the challenges this crisis presents. The revenue assumptions managers relied on for 2020, often worked out to two decimal points, are not relevant in an economy suddenly expected to suffer a historic contraction. Meticulously prepared status reports are now outdated before they reach senior managers. Managers seeking more up-to-date information discover that existing processes are too rigid for a timely response.

Managers thus find themselves working in ways unsuited to a highly uncertain environment. They know what they need: flexibility, the capability to act collectively, quickly, and across the whole organization as challenges arise. They need also to be able to work in this way over an extended period. Some organizations have therefore begun to experiment with new operating models that allow managers to work together. Some of the changes have been successful and others have failed.

The COVID-19 operating environment requires that managers reexamine their collective thought processes and challenge their own assumptions.

Overcoming challenges
To increase the odds that a new operating model will be effective today, managers must ensure that it addresses the problems of operating under highly uncertain conditions. The COVID-19 operating environment requires that managers reexamine their collective thought processes and challenge their own assumptions. Failure to do so will create the risk of serious errors. Here are some of the pitfalls managers will likely encounter:

Optimism bias. Since managers and their organizations have never seen anything like this crisis, existing heuristics learned from years of management might not apply. One common problem is that managers experience optimism bias, both individually and collectively. They will be inclined to bring forward the date of an expected revenue rebound or minimize the duration of expected business closure. Simply, managers cannot or will not believe how bad the situation could get, and the organization ends up planning for a much milder scenario than transpires.
Informational instability. Information is unstable in the COVID-19 pandemic. Epidemiological data are constantly shifting: infection and mortality rates, the proportion of asymptomatic cases, the intensity and effectiveness of testing, the length of the infectious period, and the extent and duration of immunity after infection. The problem extends to poor or missing economic data whose reliability has been affected by the speed and severity of change. Conventional business strategy is most often based on assumptions about a probable course of events. In today’s crisis, a single “most likely” planning scenario is unachievable. The sensitivity of statistical models to relatively small changes in assumptions on key variables creates even greater hazard. For example, projections of the rate of transmission of COVID-19 (R0) are central to forming a view on the likely impact of the disease: even a tiny uptick in the reproduction number can create a dramatic increase in the expected infection and mortality rates and radically change expectations of likely government measures and consumer behavior.
Wrong answer. In addition to the instability of information, leaders must also be sensitive to the possibility that information they thought was clear and certain could turn out to be wrong. Managers cannot take their own assumptions as facts, since new information could emerge that invalidates them. Assumptions and understanding need to be regularly revisited and revised as necessary, as part of the organization’s practice of continuous learning. The operating model must be able to absorb initial wrong answers and override them quickly; organizations can even encourage managers to look for opportunities to update assumptions.
Paralysis by analysis. Confusing and ever-changing data can cause managers to delay decisions as they search for more analytical rigor. They may never find it, given the extent of the crisis we are in. Delayed decision making is not advisable in a crisis as fast moving and severe as the COVID-19 pandemic. Delay is in itself a decision, since taking no action has consequences—for example, a continued, unchecked spread of the virus. Managers should rather act on what they do know, and adapt their strategy as new information becomes available.
Organizational exhaustion. In extreme uncertainty, organizations are usually unable to return to business as usual for a long time, sometimes years. This exposes managers and their teams to the risk of exhaustion in the face of constant and apparently never-ending change. A crisis may galvanize a company’s senior managers and employees in its initial phase. But once that adrenaline fades, continuing uncertainty becomes enervating. At worst it can take a toll on managers’ mental and physical health, causing major harm to organizational effectiveness, from a decline in responsiveness to a deterioration in the overall quality of work.

A suitable organizational structure
When determining how their organization should respond to extreme uncertainty, managers need to estimate the magnitude and expected duration of the crisis. At the onset, a timely and centralized organizational response—“crisis mode”—should be activated. Then leaders need to switch to an operating model that will be sustainable but appropriately reactive to continuing uncertainty over months or even years. A celebrated example is the way the New York City Fire Department handled the aftermath of the September 11 attacks. It had to shift its operating model from one based on immediate response to one that could handle continuing fires at the World Trade Center site and sustain recovery activities for months.

Activating crisis response
The earlier managers determine that they are in a crisis, the faster and more effectively organizations can respond. Effective response is enabled by several fundamental elements.

Early warning system. A fundamental operating principle in normal times is for senior managers to develop an understanding of the kinds of events that might trigger a crisis. This will allow them to establish appropriate monitoring and early warning systems. Such systems can be likened to the Intergovernmental Oceanographic Commission’s early warning systems, which rapidly relay data of approaching tsunamis to potentially affected communities.
Integrated nerve center. Once an alarm has been triggered, leaders must have an organizational structure in which a common understanding of the crisis can be developed quickly and decisive actions taken with authority. Such a structure could be part of the organization’s ready-made crisis-management plan, but leaders must prepare for the possibility that preconceived structures may be unsuitable in an existential crisis. They must therefore create a new operating model if the situation requires one. The organization needs an integrated nerve center to oversee a holistic crisis response. Within that structure, leadership must identify an inner core: a small group of managers who have the judgment and internal credibility to lead the response. Once identified, these leaders need to be given decision-making authority throughout the crisis, including the top-level support needed to make the “big bets.” A recent example of rapid and radical response was the National Basketball Association’s decision on March 11 to suspend play for the season. This action was one of the earliest high-profile operational changes taken in the United States in response to COVID-19.
Transparent operating principles. At the outset managers need to define the high-level approach that will guide their actions during the crisis. The approach should be spelled out in a set of operating principles made available throughout the organization. These transparent principles will guide decision making throughout the crisis and provide standards against which management actions can be measured. One example of such transparency can be seen in Airbnb’s response to the consequences of the pandemic for the company—a massive drop in revenue and significant layoffs. CEO Brian Chesky wrote an honest letter to the staff explaining in detail the measures being taken to ensure the company’s survival and the ways in which the travel business was being reshaped in the crisis.
Operating in crisis mode: discover, design, execute
Rapidly moving events demand speedy decisions but also a wholesale change in the organization’s managerial modus operandi. The operating cadence in which managers meet, discuss, and take action needs to match the evolution of the crisis. This does not imply a simple speedup of existing processes to accommodate the information needs of managers. Rather, it means creating entirely new procedures.

Extreme uncertainty turns an organization’s operating imperatives on their heads. It demands continuous learning and constant review of assumptions. Instead of establishing a plan and ensuring the organization sticks to it, as in more normal times, managers must understand and respond continuously to dynamic and wrenching change. Rather than making periodic reviews of a static plan, they need to meet for iterative decision-making sessions structured around three imperatives: discover, design, execute. Managers must work together to diagnose the current situation, consider its practical implications, explore how it might evolve, and establish and execute appropriate actions.

The cycle of learning and redesign must recur with frequency sufficient to ensure that responses reflect the evolving situation. Managers must doggedly question established assumptions, especially the ideas adopted under conditions of extreme uncertainty. The organization cannot treat any assumptions as sacrosanct. Organizations should accept that they will be wrong and celebrate learning quickly from experience.

To make informed decisions, managers need specialized knowledge and should actively seek expert advice. Experts can contribute to better decisions by filling gaps in existing management knowledge. For example, managers need external advice—from epidemiologists—to assess the course of the COVID-19 pandemic. Likewise, civil society organizations can have experts who can provide valuable alternative perspectives on such important matters as racial bias, diversity, and the importance of female leaders. Internal expertise is also valuable in crisis times. Managers should reach deep into their own organization for frontline insights—such as those that a customer-service representative could provide on customer experience.

The organization should also systematically challenge proposed solutions. One established way to do this is to create a “red team” of experts to pressure test managers’ decisions, identifying potential weaknesses or overly optimistic assumptions. This type of exercise has been very successful in enabling more robust solutions. Leading companies, including Microsoft and IBM, perform regular exercises in which red teams test cybersecurity infrastructure, for example.

Unprecedented crises frequently require leadership to take unprecedented actions—bold, speedy actions that would feel risky in normal times. A historic case in point is Johnson & Johnson’s 1982 decision to recall 31 million bottles of the painkiller Tylenol after some product samples were found to have been laced with cyanide. The swift, decisive action saved this valuable product and enhanced the company’s reputation.

As they focus intensely on making fast practical decisions, managers must also be prepared to shift course if the situation changes. Actions, furthermore, need to be prioritized. First must come actions to mitigate the “worst case” scenarios for the organization. Low-cost (“no regrets”) actions can also be taken quickly, to address issues that could arise in any of several potential scenarios. In an existential crisis, managers must feel comfortable making conscious decisions and taking deliberate action. Otherwise, events will take their course, decisions will be made by default, and organizational control will be lost.

Instead of establishing a plan and ensuring the organization sticks to it, managers must understand and respond continuously to dynamic and wrenching change.

A sustainable model
The global COVID-19 pandemic is approaching its tenth month, a protracted period defined by extreme uncertainty. Depending on their industrial sector and geography, organizations have experienced different forms of uncertainty at different times over the course of the crisis—with falling consumer demand, supply-chain disruptions, inventory shortages, and shifting demand across channels. Today companies face economic instability as well as secondary incidents created by extreme uncertainty. To manage an extended recovery period, management structures and processes have to shift to a long-term, sustainable operating model.

One way of thinking about this problem is to imagine that a major fire strikes a company’s headquarters. Once the fire itself is extinguished, a different set of challenges emerges, from damage assessment to restarting operations. The shift from crisis mode to recovery of sustainable operations is more an evolution than a transformation. As it reshapes its overall strategy and goals, the organization needs to maintain its integrated nerve center, as crisis circumstances may require reactivation. However, the nerve center would no longer own day-to-day activities. Decisions and actions can increasingly return to their traditional owners such as business units. The operating cadence established in crisis mode will not return to normal, but it will likely moderate. Teams might scale back to meeting weekly from daily but need to maintain the flexibility to ramp back up as needed if something occurs.

The issues to monitor will change, but the importance of monitoring and early warning remains critical. In the COVID-19 crisis, for example, employees continue to work from home in many countries. For this reason, IT departments must remain extraordinarily vigilant in monitoring for cyberattacks. Furthermore, when the time comes for employees to return to their offices, testing and monitoring processes will have to be in place. When infection is detected, quarantine and treatment can thereby quickly follow. The experiences of Korea and China well illustrate the importance of country-level monitoring and quick response in the recovery of public health and the economy.

Whether operating in crisis mode or in recovery mode, leaders still need to prioritize actions. Resilient organizations should be able to begin looking for opportunities once the worst of the crisis is past. Our research indicates, for example, that more resilient companies shifted to M&A quickly after the 2008–09 financial crisis, using the cash saved during the crisis to purchase new assets.

Extreme uncertainty—defined in terms of novelty, magnitude, duration, and the rapid pace of change—generates a difficult operating environment for managers and organizations. The radically changed circumstances call for new forms of leadership, new ways of working, and new operating models. Crisis-tested managers will develop a tolerance of ambiguity, a quickened operating cadence, and a culture of constant refinement, review, and revision. Management structure and processes need to be adapted, too, as the crisis unfolds, to ensure the organization is sustainable and can take advantage of new opportunities.
"""

article13 = """
Applying machine learning in capital markets: Pricing, valuation adjustments, and market risk

By enhancing crisis-challenged financial models with machine-learning techniques such as neural networks, banks can emerge stronger from the present crisis.
DOWNLOADS
Open interactive popup
 Article (5 pages)
When the COVID-19 outbreak became a global pandemic, financial-markets volatility hit its highest level in more than a decade, amid pervasive uncertainty over the long-term economic impact. Calm has returned to markets in recent months, but volatility continues to trend above its long-term average. Amid persistent uncertainty, financial institutions are seeking to develop more advanced quantitative capabilities to support faster and more accurate decision making.

Sidebar
About the authors
As financial markets gyrated in recent months, banks faced particular problems calculating value at risk (VAR) across asset classes. Many institutions experienced elevated levels of VAR back-testing exceptions, leading to higher regulatory-capital multipliers. Increases of as much as 30 percent were reported, prompting regulators to apply exemptions in some cases. There were also challenges with valuation adjustments, as derivatives faced snowballing collateral calls and increasing funding costs. Where credit-value-adjustment (CVA) risks were excluded from market risk models, CVA hedges sat “naked” on the balance sheet, leading to significant uplifts in exposures, and therefore in risk-weighted assets (RWAs). One large US dealer was hit with a loss of $950 million stemming from a valuation adjustment (XVA) in the first quarter of 2020. Elsewhere, rising gap risk in illiquid securities catalyzed painful fair-value losses—as high as $200 million in the case of a major Europe-based bank.

In an unpredictable environment, financial modelers were required to come up with solutions, but were often stymied by inadequate models or the need for huge computational power that was not always available. Given the speed of response required, models in some cases were rendered unusable. The inevitable result was an increase in risk exposures and opacity from valuations, sometimes in absolute value and other times relating to the reason for specific model outputs.

An imperative to act
MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
Since forecasting institutions expect the global economy to have contracted by about 5 percent in 2020, banks should aim to optimize their trading books and risk positions. This ambition requires more accurate and timely valuations. With those priorities in mind, more advanced models and sufficient computational power are imperatives. Indeed, “speed” is of the essence.

In response, some leading institutions have started to incorporate advanced techniques into their quantitative armories. In pricing, an area that has experienced a spike in recent activity, several banks are applying machine learning (ML) to enhance traditional models—for example, by calibrating parameters more efficiently. In particular, banks have used neural networks, a type of ML focused on nonlinear and complex data relationships. Advanced machine-learning techniques can do the following:

speed up calculations, reducing operational costs and allowing real-time risk management of complex products
animate more complex models that may currently be unusable in practice, and unlock more accurate valuations
generate high volumes of synthetic but market-consistent data, helping, for example, to offset the disruptive impact of COVID-19-related market moves
One way to implement neural networks is to apply them to pricing, where they can “learn” how to price vanilla calibration instruments under a given (possibly complex) model, and then act as pricing engines for new model calibration. The approach obviates one of the most significant challenges associated with ML, which is parameter interpretability. In this case, there is no interpretability issue because the network uses the original model’s parameters. This means that there is no ML “black box,” and the key calibrated parameters can be interpreted in the original model’s context.

Neural networks can also support future exposure modeling for valuation adjustments (Exhibit 1).

Exhibit 1

The network can be trained on established samples, such as those relating to the evolution of risk factors and corresponding cash flows for the products being modeled. The additional efficiency provided by the network makes for improved accuracy and faster processing (Exhibit 2). That saves banks from using time-consuming nested Monte Carlo approaches and less accurate analytical approximations or “least squares”—style regressions.

Exhibit 2

There are equally promising applications in real-time portfolio valuation, risk assessment, and margining.

Three steps to deepening ML engagement
Machine learning offers significant enhancement for conventional quantitative approaches through its ability to interpolate across large data sets and streamline model calibration. Banks would benefit by deepening their ML engagement and testing new use cases. The uncertain macroeconomic environment should act as a catalyzer to this process and trigger banks to act. The emphasis initially should be on discrete applications rather than wholesale transformation. Use cases can later be extended and expanded across the business.

There is no blueprint for model development, and individual businesses must solve for their own pressing needs. However, the experience of early movers suggests that reliable options for establishing a track record encompass three key steps:

1. Identify quick wins
While ML can help to improve numerous calculation processes, it is more useful in some contexts than others. The task for decision makers is to identify potentially winning applications that will help create a positive track record. Likely candidates are models that consume large amounts of time or computing power. ML can both speed the work of these models and lay the groundwork for scaling their application. Among the applications that have begun to attract attention are valuations of level-3 assets, XVA calculations, profit-and-loss attributions (“P&L explains”), adaptations for Fundamental Review of the Trading Book (FRTB), and stress testing.

A “discovery phase” of an ML transformation could proceed as follows:

Identify concrete cases based on accepted criteria, such as the complexity of models, exposure in books, or computational bottlenecks. For example, complex, hard-to-value derivatives such as structured callable trades could be good targets.
Size the estimated impact and align various stakeholder groups.
Create an action plan, including the effort and time required for implementing the identified use cases.
2. Build capabilities to embrace a culture enabled by machine learning
Machine learning has the potential to create significant efficiencies in a range of activities. However, financial institutions cannot maximize the ML opportunity without acquiring the necessary capabilities to build, maintain, and apply ML-enabled models. They must also take steps to help employees understand and exploit potential benefits so that ML is embedded in the culture of the organization.

This could be achieved by following through with the earlier approach and establishing and executing pilot programs to implement prioritized use cases. During these pilots, the following practices can be applied:

build capabilities via learning on the job
understand typical challenges and pitfalls and how to solve them
acquire continuous feedback on how new applications can fit into the wider organization
Financial institutions cannot maximize the machine-learning opportunity without acquiring the capabilities to build, maintain, and apply ML-enabled models.

3. Roll out at scale
Over time, sprints, prototypes, and quick wins will have accumulated sufficiently to create the conditions for a more sustained machine-learning rollout. Assuming a critical mass of use cases, quant teams should move to integrate ML into a wider range of activities. They may begin with the front office and extend into risk, finance, compliance, and research.

A plan to scale up the machine-learning program could include the following activities:

strategic execution of identified priority use cases
continuous exploration of additional areas where ML could be relevant, such as anti–money laundering, know your customer, or cybersecurity
updating risk-management practices, such as model governance and risk assessment, to monitor and control new risks introduced by ML
Machine learning has the potential to enable institutions to do more in capital markets, to move faster, and to move with greater accuracy. The working conditions created during the pandemic have accelerated reliance on digital access and the data-driven environment. Given these factors, machine learning could easily begin to migrate into mainstream operations. With this in mind, firms must not delay in building their capabilities. They must experiment, develop use cases, and move quickly to the production of machine-learning-enhanced models. Those that create and execute a sensible implementation strategy are likely to emerge from the current crisis stronger, more assured of risk exposures, and better prepared for what lies ahead.
"""

article14 = """
Risk, resilience, and rebalancing in global value chains

Companies need an understanding of their exposure, vulnerabilities, and potential losses to inform resilience strategies.
By Susan Lund, James Manyika, Jonathan Woetzel, Edward Barriball, Mekala Krishnan, Knut Alicke, Michael Birshan, Katy George, Sven Smit, Daniel Swan, and Kyle Hutzler
Open interactive popup
Risk, resilience, and rebalancing in global value chains
Open interactive popup
DOWNLOADS
Open interactive popup

Special Report
 Full Report (112 pages)
 Executive Summary (28 pages)
In recent decades, value chains have grown in length and complexity as companies expanded around the world in pursuit of margin improvements. Since 2000, the value of intermediate goods traded globally has tripled to more than $10 trillion annually. Businesses that successfully implemented a lean, global model of manufacturing achieved improvements in indicators such as inventory levels, on-time-in-full deliveries, and shorter lead times.

However, these operating model choices sometimes led to unintended consequences if they were not calibrated to risk exposure. Intricate production networks were designed for efficiency, cost, and proximity to markets but not necessarily for transparency or resilience. Now they are operating in a world where disruptions are regular occurrences. Averaging across industries, companies can now expect supply chain disruptions lasting a month or longer to occur every 3.7 years, and the most severe events take a major financial toll.

Play Video
Video
Making supply chains more resilient in the post-COVID world
Supply chain disruptions lasting a month or longer now happen every 3.7 years on average.
The risk facing any particular industry value chain reflects its level of exposure to different types of shocks, plus the underlying vulnerabilities of a particular company or in the value chain as a whole. New research from the McKinsey Global Institute explores the rebalancing act facing many companies in goods-producing value chains as they seek to get a handle on risk—not ongoing business challenges but more profound shocks such as financial crises, terrorism, extreme weather, and, yes, pandemics.

Today technology is challenging old assumptions that resilience can be purchased only at the cost of efficiency. The latest advances offer new solutions for running scenarios, monitoring many layers of supplier networks, accelerating response times, and even changing the economics of production. Some manufacturing companies will no doubt use these tools and devise other strategies to come out on the other side of the pandemic as more agile and innovative organizations.

Section 1

With shocks growing more frequent and severe, industry value chains vary in their level of exposure
The COVID pandemic has delivered the biggest and broadest value chain shock in recent memory. But it is only the latest in a series of disruptions. In 2011, a major earthquake and tsunami in Japan shut down factories that produce electronic components for cars, halting assembly lines worldwide. The disaster also knocked out the world’s top producer of advanced silicon wafers, on which semiconductor companies rely. Just a few months later, flooding swamped factories in Thailand that produced roughly a quarter of the world’s hard drives, leaving the makers of personal computers scrambling. In 2017, Hurricane Harvey, a Category 4 storm, smashed into Texas and Louisiana. It disrupted some of the largest US oil refineries and petrochemical plants, creating shortages of key plastics and resins for a range of industries.

This is more than just a run of bad luck. Changes in the environment and in the global economy are increasing the frequency and magnitude of shocks. Forty weather disasters in 2019 caused damages exceeding $1 billion each—and in recent years, the economic toll caused by the most extreme events has been escalating.1 As a new multipolar world takes shape, we are seeing more trade disputes, higher tariffs, and broader geopolitical uncertainty. The share of global trade conducted with countries ranked in the bottom half of the world for political stability, as assessed by the World Bank, rose from 16 percent in 2000 to 29 percent in 2018. Just as telling, almost 80 percent of trade involves nations with declining political stability scores.2 Increased reliance on digital systems increases exposure to a wide variety of cyberattacks; the number of new ransomware variations alone doubled from 2018 to 2019.3 Interconnected supply chains and global flows of data, finance, and people offer more “surface area” for risk to penetrate, and ripple effects can travel across these network structures rapidly.

Exhibit 1 classifies different types of shocks based on their impact, lead time, and frequency of occurrence. In a few cases, we also show hypothetical shocks like a global military conflict or a systemic cyberattack that would dwarf the most severe shocks experienced to date. While these may be only remote possibilities, these scenarios are in fact studied and planned for by governments and security experts. The impact of a shock can be influenced by how long it lasts, the ripple effects it has across geographies and industries, and whether a shock hits the supply side alone or also hits demand.

Exhibit 1

This analysis reveals four broad categories of shocks. Catastrophes are historically remarkable events that cause trillions of dollars in losses. Some are foreseeable and have relatively long lead times, while others are unanticipated. Larger patterns and probabilities can guide general preparedness; hurricanes strike in the Gulf of Mexico every year, for example. But the manifestation of a specific event can strike with little to no warning. This includes some calamities that the world has avoided to date, such as a cyberattack on foundational global systems.

Disruptions are serious and costly events, although on a smaller scale than catastrophes. They, too, can be split into those that telegraph their arrival in advance (such as the recent US–China trade disputes and the United Kingdom’s exit from the European Union) and unanticipated events such as data breaches, product recalls, logistics disruptions, and industrial accidents. Disruptions do not cause the same cumulative economic toll as catastrophes.

Companies tend to focus much of their attention on managing the types of shocks they encounter most often, which we classify as “unanticipated disruptions.” Some other shocks such as trade disputes have made headlines in recent years and, as a result, companies have started to factor them into their planning. But other types of shocks that occur less frequently could inflict bigger losses and also need to be on companies’ radar. The COVID pandemic is a reminder that outliers may be rare—but they are real possibilities that companies need to consider in their decision making.

All four types of shocks can disrupt operations and supply chains, often for prolonged periods. We surveyed dozens of experts in four industries (automotive, pharmaceuticals, aerospace, and computers and electronics) to understand how often they occur. Respondents report that their industries have experienced material disruptions lasting a month or longer every 3.7 years on average. Shorter disruptions happen even more frequently.

We analyzed 23 industry value chains to assess their exposure to specific types of shocks. The resulting index (Exhibit 2) combines multiple factors, including how much of the industry’s current geographic footprint is found in areas prone to each type of event, the factors of production affected by those disruptions and their importance to that value chain, and other measures that increase or reduce susceptibility.

Interactive Exhibit 2

Exposure to different types of shocks varies sharply by value chain. Aerospace and semiconductors, for example, are susceptible to cyberattacks and trade disputes, because of their high level of digitization, R&D, capital intensity, and exposure to digital data flows. However, both value chains have relatively low exposure to the climate-related events we have assessed here (heat stress and flooding) because of the footprint of their production.

Specific types of shocks are more likely to touch certain industries. Pandemics, for example, have a major impact on labor-intensive value chains. In addition, this is the one type of shock for which we assess the effects on demand as well as supply. As we are seeing in the current crisis, demand has plummeted for nonessential goods and travel, hitting companies in apparel, petroleum products, and aerospace. By contrast, while production has been affected in value chains like agriculture and food and beverage, they have continued to see strong demand because of the essential nature of their products.

In general, heat stress is more likely to strike labor-intensive value chains (and some resource-intensive value chains) because of their relatively high reliance on manual labor or outdoor work. Perhaps surprisingly, these same value chains are relatively less susceptible to trade disputes, which are increasingly focused on value chains with a high degree of knowledge intensity and high-value industries.

Overall, value chains that are heavily traded relative to their output are more exposed than those with lower trade intensity. Some of these include value chains that are the most sought after by countries: communication equipment, computers and electronics, and semiconductors and components. These value chains have the further distinction of being high value and relatively concentrated, underscoring potential risks for the global economy. Heavily traded labor-intensive value chains, such as apparel, are highly exposed to pandemic risk, heat stress (because of their reliance on labor), and flood risk. In contrast, the value chains including glass and cement, food and beverage, rubber and plastics, and fabricated metals have much lower exposure to shocks; these are among the least traded and most regionally oriented value chains.

All in all, the five value chains most exposed to our assessed set of six shocks collectively represent $4.4 trillion in annual exports, or roughly a quarter of global goods trade (led by petroleum products, ranked third overall, with $2.4 trillion in exports). The five least exposed value chains account for $2.6 trillion in exports. Of the five most exposed value chains, apparel accounts for the largest share of employment, with at least 25 million jobs globally, according to the International Labor Organization.4
Even value chains with limited exposure to all types of shocks we assessed are not immune to them. Despite recent headlines, we find that pharmaceuticals are relatively less exposed than most other industries. But the industry has been disrupted by a hurricane that struck Puerto Rico, and cyberattacks are a growing concern. In the future, the industry may be subject to greater trade tensions as well as regulatory and policy shifts if governments take action with the intent of safeguarding public health. The food and beverage industry and agriculture similarly have relatively low exposure overall, as they are globally dispersed. Yet these value chains are subject to climate-related stresses that are likely to grow over time. In addition to disrupting the lives and livelihoods of millions, this could cause the industries to become more dependent on trade or force them to undertake expensive adaptations.

Section 2

Shocks exploit vulnerabilities within companies and value chains
Shocks inevitably seem to exploit the weak spots within broader value chains and specific companies. An organization’s supply chain operations can be a source of vulnerability or resilience, depending on its effectiveness in monitoring risk, implementing mitigation strategies, and establishing business continuity plans.

Some of these vulnerabilities are inherent to a given industry; the perishability of food and agricultural products, for example, means that the associated value chains are highly vulnerable to delivery delays and spoilage. Industries with unpredictable, seasonal, and cyclical demand also face particular challenges. Makers of electronics must adapt to relatively short product life cycles, and they cannot afford to miss spikes in consumer spending during limited holiday windows.

Other vulnerabilities are the consequence of intentional decisions, such as how much inventory a company chooses to carry, the complexity of its product portfolio, the number of unique SKUs in its supply chain, and the amount of debt or insurance it carries.5 Changing these decisions can reduce—or increase—vulnerability to shocks.

Weaknesses often stem from the structure of supplier networks in a given value chain. Complexity itself is not necessarily a weakness to the extent that it provides companies with redundancies and flexibility. But sometimes the balance can tip. Complex networks may become opaque, obscuring vulnerabilities and interdependencies. A large multinational company can have hundreds of tier-one suppliers from which it directly purchases components. Each of those tier-one suppliers in turn can rely on hundreds of tier-two suppliers. The entire supplier ecosystem associated with a large company can encompass tens of thousands of companies around the world when the deepest tiers are included.

Exhibit 3 applies network analytics to illustrate the complexity of the first- and second-tier supply ecosystems for two Fortune 500 companies in the computer and electronics industry. This is based on publicly available data and may therefore not be exhaustive.6 These multitiered, multinational networks span thousands of companies and extend to deeper tiers that are not shown here. This illustration also underscores the fact that even within the same industry, companies may make materially different decisions about how to structure their supply ecosystems, with implications for risk.

Interactive Exhibit 3

Companies’ supplier networks vary in ways that can shape their vulnerability. Spending concentrated among just a few suppliers may make it easier to manage them, but it also heightens vulnerability should anything happen to them. Suppliers frequently supply each other; one form of structural vulnerability is a subtier supplier that accounts for relatively little in spending but is collectively important to all participants. The number of tiers of participating suppliers can hinder visibility and make it difficult to spot emergent risks. Suppliers that are dependent on a single customer can cause issues when demand shocks cascade through a value chain. The absence of substitute suppliers is another structural vulnerability.

In some cases, suppliers may be concentrated in a single geography due to that country’s specialization and economies of scale. A natural disaster or localized conflict in that part of the world can cause critical shortages that snarl the entire network. Some industries, such as mobile phones and communication equipment, have become more concentrated in recent years, while others, including medical devices and aerospace, have become less so (Exhibit 4). The aerospace value chain, for example, has diversified in part due to secure market access.

Exhibit 4

Even in value chains that are generally more geographically diversified, production of certain key products may be disproportionately concentrated. Many low-value or basic ingredients in pharmaceuticals are predominantly produced in China and India, for instance. In total, we find 180 products across value chains for which one country accounts for 70 percent or more of exports, creating the potential for bottlenecks. The chemicals value chain has a particularly large number of such highly concentrated products, but examples exist in multiple industries. Other products may be produced across diverse geographies but have severe capacity constraints, which can create bottlenecks if production is halted. Geographic diversification is not inherently positive, particularly if production and sourcing expands into areas that are more exposed to shocks.

Section 3

Over the course of a decade, companies can expect disruptions to erase half a year’s worth of profits or more
When companies understand the magnitude of the losses they could face from supply chain disruptions, they can weigh how much to invest in mitigation. We built representative income statements and balance sheets for hypothetical companies in 13 different industries, using actual data from the 25 largest public companies in each. This enables us to see how they fare financially when under duress.

We explore two scenarios involving severe and prolonged shocks:

Scenario 1. A complete manufacturing shutdown lasting 100 days that affects raw material delivery and key inputs but not distribution channels and logistics. In this scenario, companies can still deliver goods to market. But once their safety stock is depleted, their revenue is hit.
Scenario 2. The same as above, but in this case, distribution channels are also affected, meaning that companies cannot sell their products even if they have inventory available.
Our choice to model a 100-day disruption is based on an extensive review of historical events. In 2018 alone, the five most disruptive supply chain events affected more than 2,000 sites worldwide, and factories took 22 to 29 weeks to recover.7
Our scenarios show that a single prolonged production-only shock would wipe out between 30 and 50 percent of one year’s EBITDA for companies in most industries. An event that disrupts distribution channels as well would push the losses sharply higher for some.

Industries in which companies typically hold larger inventories and have lower fixed costs tend to experience relatively smaller financial losses from shocks. If a natural disaster hits a supplier but distribution channels remain open, inventory levels become a key buffer. However, the downstream company will still face a cash drain after the fact when it is time to replenish its drawn-down safety stock. When a disruption outlasts the available safety stock, lower fixed costs become important to withstanding a decline in EBITDA.

Having calculated the damage associated with one particularly severe and prolonged disruption, we then estimated the bottom-line impact that companies can expect over the course of a decade, based on probabilities. We combined the expected frequency of value chain disruptions of different lengths with the financial impact experienced by companies in different industries. On average, companies can expect losses equal to almost 45 percent of one year’s profits over the course of a decade (Exhibit 5). This is equal to seven percentage points of decline on average. We make no assessment of the extent to which the cost of these disruptions has already been priced into valuations.

Exhibit 5

These are not distant future risks; they are current, ongoing patterns. On top of those losses, there is an additional risk of permanently losing market share to competitors that are able to sustain operations or recover faster, not to mention the cost of rebuilding damaged physical assets. However, these expected losses should be weighed in the context of the additional profits that companies are able to achieve with highly efficient and far-reaching supply chains.

Section 4

Will global value chains shift across countries?

Today much of the discussion about resilience in advanced economies revolves around the idea of increasing domestic production. But the highly interconnected nature of value chains limits the economic case for making large-scale changes in their physical location. Value chains often span thousands of interconnected companies, and their configurations reflect specialization, access to consumer markets around the world, long-standing relationships, and economies of scale.

We set out to estimate what share of global exports could move to different countries based on the business case and how much might move due to policy interventions. To determine whether industry economics alone support a future geographic shift, we considered a number of factors. One is whether some movement is already under way. Between 2015 and 2018, for instance, the share of trade produced by the three leading export countries in apparel dropped. In contrast, the top three countries in semiconductors and mobile communications increased their share of trade markedly.

Other considerations include whether the value chain is highly capital- or knowledge-intensive, or tied to geology and natural resources. All of these make relocation less feasible. Highly capital-intensive value chains are harder to move for the simple reason that they represent hundreds of billions of dollars in fixed investments. These industries have strong economies of scale, making them more costly to shift. Value chains with high knowledge intensity tend to have specialized ecosystems that have developed in specific locations, with unique suppliers and specialized talent. Deciding to move production outside of this ecosystem to a novel location is costly. Finally, value chains with comparatively high levels of extraregional trade have more scope to shorten than those that are already regionalized. We also consider overall growth, the location of major (and rising) consumer markets, trade intensity, and innovation dynamics.

With respect to noneconomic factors, we consider governments’ desire to bolster national security, national competitiveness, and self-sufficiency. Some nations are focusing on safeguarding technologies with dual-use (civilian and military) implications, which could affect value chains such as semiconductors and communication equipment, particularly as 5G networks are built out. In other cases, governments are pursuing industrial policies intended to capture leading shares of emerging technologies ranging from quantum computing and artificial intelligence to renewable energy and electric vehicles. This, too, has the potential to reroute value chains. Finally, self-sufficiency has always been a question surrounding energy. Now the COVID pandemic has driven home the importance of self-sufficiency in food, pharmaceuticals, and certain medical equipment as well.

We estimate that 16 to 26 percent of exports, worth $2.9 trillion to $4.6 trillion in 2018, could be in play—whether that involves reverting to domestic production, nearshoring, or new rounds of offshoring to new locations. It should be noted that this is not a forecast: it is a rough estimate of how much global trade could relocate in the next five years, not an assertion that it will actually move.

The value chains with the largest share of total exports potentially in play are pharmaceuticals, apparel, and communication equipment. In dollar terms, the value chains with the largest potential to move production to new geographies are petroleum, apparel, and pharmaceuticals.8 In all of these cases, more than half of their global exports could potentially move. With few exceptions, the economic and noneconomic feasibility of geographic shifts do not overlap. Thus, countries would have to be prepared to expend considerable sums to induce shifts from what are otherwise economically optimal production footprints.

In general, the economic case to move is most viable for labor-intensive value chains such as furniture, textiles, and apparel. These value chains were already experiencing shifts away from their current top producers, where the cost of labor has risen. The continuation of this trend could represent a real opportunity for some developing economies. By contrast, resource-intensive value chains, such as mining, agriculture, and energy, are generally constrained by the location of natural resources that provide crucial inputs. But policy considerations may encourage new exploration and development that can shift value chains at the margins.

The value chains in the global innovations category (semiconductors, automotive, aerospace, machinery, communication, and pharmaceuticals) are subject to the most scrutiny and possible intervention from governments, based on their high value, cutting-edge technologies as well as their perceived importance for national competitiveness. But the feasibility of moving these value chains based on the economics alone is low.

Production networks have begun to regionalize in recent years, and this trend may persist as growth in Asia continues to outpace global growth. But multinationals with production facilities in countries such as China, India, and other major emerging economies are typically there to serve local consumer markets, whether or not they also export from those places. As prosperity rises in these countries, they are key sources of global growth that companies will continue to pursue.

Section 5

Companies have a range of options for improving resilience
In a McKinsey survey of supply chain executives conducted in May 2020, an overwhelming 93 percent reported that they plan to take steps to make their supply chains more resilient, including building in redundancy across suppliers, nearshoring, reducing the number of unique parts, and regionalizing their supply chains.

Strengthen supply chain risk management and improve end-to-end transparency
Global manufacturing has only just begun to adopt a range of technologies such as analytics and artificial intelligence, the Internet of Things, advanced robotics, and digital platforms. Companies now have access to new solutions for running scenarios, assessing trade-offs, improving transparency, accelerating responses, and even changing the economics of production.

Most companies are still in the early stages of their efforts to connect the entire value chain with a seamless flow of data. Digital can deliver major benefits to efficiency and transparency that are yet to be fully realized. Consumer goods giant Procter & Gamble, for example, has a centralized control tower system that provides a company-wide view across geographies and products. It integrates real-time data, from inventory levels to road delays and weather forecasts, for its own plants as well as suppliers and distributors. When a problem occurs, the system can run scenarios to identify the most effective solution.9
Creating a comprehensive view of the supply chain through detailed subtier mapping is a critical step to identifying hidden relationships that invite vulnerability. Today most large firms have only a murky view beyond their tier-one and perhaps some large tier-two suppliers. Working with operations and production teams to review each product’s bill of materials can reveal whether critical inputs are sourced from high-risk areas and lack ready substitutes. Companies can also work with their tier-one suppliers to create transparency. But in cases where those suppliers lack visibility themselves or consider their own sourcing to be proprietary information, risk management teams may have to turn to other information sources to do detective work. After mapping upstream suppliers, downstream companies need to understand their production footprint, financial stability, and business continuity plans.

Minimize exposure to shocks
Targeted measures taken before an event occurs can mitigate the impact of a shock or speed time to recovery. As more physical assets are digitized, for example, companies will need to step up investment in cybersecurity tools and teams.

One of the most important steps is building more redundancy into supplier networks. Relying on a single source for critical components or raw materials can be a vulnerability. In fact, even if a company relies on multiple suppliers, they may be concentrated in the same place. Taking the time to identify, prequalify, and onboard backup vendors comes at a cost. But it can provide much-needed capacity if a crisis strikes. Auditing and diversifying the supply chain can have the added benefit of reducing carbon intensity, raising environmental and labor standards, and expanding opportunities for women- and minority-owned businesses.

One way to achieve supply chain resilience is to design products with common components, cutting down on the use of custom parts in different product offerings. Auto manufacturers are perhaps the most advanced in this regard, having implemented modular manufacturing platforms that share components across product lines and production sites.

Physical assets may need to be hardened to withstand natural disasters. In regions that are vulnerable to worsening hurricanes and storm surges, this may involve installing bulkheads, elevating critical machinery and utility equipment, adding more waterproof sealing, and reworking drainage and valves. Many factories that are not air-conditioned today will need cooling systems to prepare for rising temperatures and potential heat waves in some parts of the world. Plants located in earthquake-prone areas may need seismic retrofitting. Companies can also build more redundancies into transportation and logistics.

When a shock does hit, companies need the ability to respond quickly
The shift to just-in-time and lean production systems has helped companies improve efficiency and reduce their need for working capital. But now they may need to strike a different balance between just-in-time and “just in case.” Having sufficient backup inventory of key parts and safety stock is a critical buffer that can minimize the financial impact of disrupted supplies. It can also position companies to meet sudden spikes in demand.

The ability to reroute components and flex production dynamically across sites can keep production going in the wake of a shock. This requires robust digital systems as well as the analytics muscle to run scenarios based on different responses. When the COVID pandemic hit, Nike used predictive analytics to selectively mark down goods and reduce production early on to minimize impact. The company was also able to reroute products from brick-and-mortar stores to e-commerce sales, driven in part by direct-to-consumer online sales through its own training app. As a result, Nike sustained a smaller drop in sales than some of its competitors.

When disaster strikes, companies have to be laser focused on cash management. But those at the top of a value chain also have a vested interest in preserving the supplier networks on which they depend. In the aftermath of the global financial crisis, some companies accelerated payments or guaranteed bank loans to give key vendors a lifeline.

Coming on the heels of Brexit and a flare-up in US–China trade tensions, the COVID pandemic has forced businesses to focus on building resilience in their supply chains and operations. Not everything that can go wrong actually does go wrong, but businesses and governments cannot afford to be caught flat-footed when disaster strikes. Preparing for future hypotheticals has a present-day cost. But those investments can pay off over time—not only minimizing losses but also improving digital capabilities, boosting productivity, and strengthening entire industry ecosystems. Rather than a trade-off between resilience and efficiency, this rebalancing act might deliver a win-win.
"""

article15 = """
Optimizing data controls in banking

Banks need to do more in four important areas of data culture to build the risk-related data-control capabilities they will need in the coming decade.
DOWNLOADS
Open interactive popup
 Article (PDF-338KB)
MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
Over the past decade, banks across the globe have made considerable progress in building risk-related data-control capabilities, prompted in large part by regulatory demands. The starting point was the Basel Committee’s BCBS 239 principles, issued in 2013 to strengthen banks’ risk-related data-aggregation and reporting capabilities. Progress, however, has not been uniform, and most institutions are not fully compliant. In fact, many banks are still struggling with major deficiencies, particularly when it comes to data architecture and technology.

One major reason for this limited progress is that the Basel Committee called for effective implementation of BCBS 239 principles without clearly explaining what that means or how to implement them. This ambiguity has led to a wide range of interpretations, which vary from institution to institution, country to country, and even regulator to regulator. At the same time, a host of other regulations with substantial data implications have emerged, particularly those involving stress testing (CCAR in the United States), data privacy (CCPA in the US, GDPR in Europe), BSA/AML, and CECL.1 As might be expected, banks have a monumental task in analyzing the layers of data requirements across all these regulations and building common and reusable capabilities that meet regulatory expectations.

In response, the industry has adopted some common, workable solutions in a few key areas. These include data-aggregation capabilities to support regulatory reporting requirements, such as automating some of the reporting required by the Federal Reserve in the US and the European Banking Authority (EBA) in Europe,2 preparing to collect evidence for regulatory examinations, and deploying a federated data operating model with central capabilities under a chief data officer. Industry leaders are clear, however, that they struggle in four areas: the scope of data programs, data lineage, data quality, and transaction testing.3
There is considerable variation within the industry on how to address these four challenging areas, in investment, degree of risk mitigation, sustainability, and automation. A few institutions, however, are leading the way in improving their data programs and management and have made great strides toward regulatory compliance.

Scope of data programs
Banks need to define the scope of their data programs clearly enough to create a basis for easily conversing with regulators and identifying additional actions necessary for regulatory compliance. Most banks have defined the scope of their data programs to include pertinent reports, the metrics used in them, and their corresponding input-data elements. Thus a credit-risk report or a report on strategic decision making might be covered, as well as risk-weighted assets as a metric and the principal loan amounts as an input. Unfortunately, the industry has no set rules for how broadly or narrowly to define the scope of a data program or what standard metrics or data elements to include.

As a result, many banks are trying to identify industry best practices for the number of reports and types of data to include in their data programs. Our industry benchmarking indicates that the average bank’s data program includes 50 reports, 90 metrics, and 1,100 data elements. Interestingly, over time, we have seen the number of reports in data programs increase while the number of metrics and data elements decreased (Exhibit 1). We believe the increase in reports reflects the inclusion of different nonfinancial risk types, such as operational or compliance risk. The reduction in metrics and data elements is the result of banks’ attempts to reduce management costs and efforts and focus only on the most critical metrics and data.

Exhibit 1

More important than the number of reports, metrics, and data elements is a bank’s ability to demonstrate to regulators and other stakeholders that the scope of its data program covers the major risks it faces. With this in mind, leading banks have established principles to define the scope and demonstrate its suitability to regulators. Leading institutions usually define the scope of their data programs broadly (Exhibit 2).

Exhibit 2

For all banks, the application of the principles illustrated in Exhibit 2 ranges from narrow to broad. However, supervisors are increasingly advocating for a broader scope, and many banks are complying. Best-in-class institutions periodically expand the scope of their data programs as their needs shift. From purely meeting regulatory objectives, these banks seek to meet business objectives as well. After all, the same data support business decisions and client interactions as well as regulatory processes.

Data lineage
Of all data-management capabilities in banking, data lineage often generates the most debate. Data-lineage documents how data flow throughout the organization—from the point of capture or origination to consumption by an end user or application, often including the transformations performed along the way. Little guidance has been provided on how far upstream banks should go when providing documentation, nor how detailed the documentation should be for each “hop” or step in the data flow. As a result of the lack of regulatory clarity, banks have taken almost every feasible approach to data-lineage documentation.

In some organizations, data-lineage standards are overengineered, making them costly and time consuming to document and maintain. For instance, one global bank spent about $100 million in just a few months to document the data lineage for a handful of models. But increasingly, overspending is more the exception than the rule. Most banks are working hard to extract some business value from data lineage; for example, by using it as a basis to simplify their data architecture or to spot unauthorized data-access points, or even to identify inconsistencies among data in different reports.

Our benchmarking revealed that more than half of banks are opting for the strictest data-lineage standards possible, tracing back to the system of record at the data-element level (Exhibit 3). We also found that leading institutions do not take a one-size-fits-all approach to data. The data-lineage standards they apply are more or less rigorous depending on the data elements involved. For example, they capture the full end-to-end data lineage (including depth and granularity) for critical data elements, while data lineage for less critical data elements extends only as far as systems of record or provisioning points.

Exhibit 3

Most institutions are looking to reduce the expense and effort required to document data lineage by utilizing increasingly sophisticated technology. Data-lineage tools have traditionally been platform specific, obliging banks to use a tool from the same vendor that provided their data warehouse or their ETL tools (extract, transform, and load). However, newer tools are becoming available that can partly automate the data-lineage effort and operate across several platforms. They also offer autodiscovery and integration capabilities based on machine-learning techniques for creating and updating metadata and building interactive data-lineage flows. These tools are not yet widely available and have no proven market leaders, so some banks are experimenting with more than one solution or are developing proprietary solutions.

Other ways to reduce the data-lineage effort include simplifying the data architecture. For example, by establishing an enterprise data lake, a global bank reduced the number of data hops for a specific report from more than a hundred to just three. Some institutions also use random sampling to determine when full lineage is needed, especially for upstream flows that are especially manual in nature and costly to trace. Another possibility is to adjust the operating model. For instance, banking systems change quickly, so element-level lineages go out of date just as fast. To tackle this issue, some banks are embedding tollgates on change processes to ensure that the documented lineage is maintained and usable through IT upgrades. Report owners are expected to periodically review and certify the lineage documentation to identify necessary updates.

Data quality
Improving data quality is often considered one of the primary objectives of data management. Most banks have programs for measuring data quality and for analyzing, prioritizing, and remediating issues that are detected. They face two common challenges. First, thresholds and rules are specific to each bank, with little or no consistency across the industry. Although some jurisdictions have attempted to define standards for data-quality rules, these failed to gain traction. Second, remediation efforts often consume significant time and resources, creating massive backlogs at some banks. Some institutions have resorted to establishing vast data-remediation programs with hundreds of dedicated staff involved in mostly manual data-scrubbing activities.

Banks are starting to implement better processes for prioritizing and remediating issues at scale. To this end, some are setting up dedicated funds to remediate data-quality issues more rapidly, rather than relying on the standard, much slower IT prioritization processes. This approach is especially helpful for low- or medium-priority issues that might not otherwise receive enough attention or funding.

As data-quality programs mature, three levels of sophistication in data-quality controls are emerging among banks. The first and most common uses standard reconciliations to measure data quality in completeness, consistency, and validity. At the second level, banks apply statistical analysis to detect anomalies that might indicate accuracy issues. These could be values beyond three standard deviations, or values that change by more than 50 percent in a month. At the third and most sophisticated level, programs use artificial intelligence and machine learning–based techniques to identify existing and emerging data-quality issues and accelerate remediation efforts (Exhibit 4).

Exhibit 4

One institution identified accuracy issues by using machine-learning clustering algorithms to analyze a population of loans and spot contextual anomalies, such as when the value of one attribute is incongruent with that of other attributes. Another bank applied artificial intelligence and natural-language processing to hundreds of thousands of records to predict accurately a customer’s missing occupation. To do this the program used information captured in free-form text during onboarding and integrated this with third-party data sources.

Leading institutions are revising and enhancing their entire data-control framework. They are developing holistic risk taxonomies that identify all types of data risks, including for accuracy, timeliness, or completeness. They are choosing what control types to use, such as rules, reconciliation, or data-capture drop-downs, and they are also setting the minimum standards for each control type—when the control should be applied and who shall define the threshold, for example. Banks are furthermore pushing for more sophisticated controls, such as those involving machine learning, as well as greater levels of automation throughout the end-to-end data life cycle.

Banks are pushing for more sophisticated controls, such as those involving machine learning, as well as greater levels of automation throughout the end-to-end data life cycle.

Transaction testing
Transaction testing, also referred to as data tracing or account testing, involves checking whether the reported value of data at the end of the journey matches the value at the start of the journey (the source). Banks use transaction testing to assess the validity and accuracy of data used in key reports and to determine if “black box” rules have been implemented correctly. Banks utilize a spectrum of different transaction-testing approaches, with single testing cycles taking between a few weeks and nine months to complete.

Regulators are putting pressure on banks to strengthen their transaction-testing capabilities through direct regulatory feedback and by conducting their own transaction tests at several large banks. At the same time, many banks are inclined to focus more on transaction testing because they increasingly recognize that maintaining high-quality data can lead to better strategic decision making, permit more accurate modeling, and improve confidence among customers and shareholders.

Banks with distinctive transaction-testing capabilities shine in three areas. First, they have well-defined operating models that conduct transaction testing as an ongoing exercise (rather than a one-off effort), with clearly assigned roles, procedures, and governance oversight. The findings from transaction tests are funneled into existing data-governance processes that assess the impact of identified issues and remediate them.

Second, they strategically automate and expedite transaction testing, utilizing modern technology and tools. While no tools exist that span the end-to-end process, leading banks are using a combination of best-in-class solutions for critical capabilities (such as document management and retrieval), while building wraparound workflows for integration.

Finally, they apply a risk-based approach to define their transaction-testing methodology. For example, leading banks often select the population for testing by combining data criticality and materiality with other considerations. These could include the persistence or resolution of issues identified in previous tests. Similarly, the size and selection of samples from that population will be related to the population’s risk characteristics. While most leading banks opt for a minimum sample size and random sampling, some also use data profiling to inform their sampling, pulling in more samples from potentially problematic accounts. The review or testing of these samples is often done at an account level (rather than a report level) to allow for cross-report integrity checks, which examine the consistency of data across similar report disclosures.
"""

article16 = """
Demystifying modeling: How quantitative models can—and can’t—explain the world

The COVID-19 crisis has brought quantitative models to the forefront. Here are some ways that modeling helps us—as long as we avoid its pitfalls.
DOWNLOADS
Open interactive popup
 Article (PDF-286KB)
MOST POPULAR INSIGHTS
COVID-19: Implications for business
When will the COVID-19 pandemic end?
More than a mission statement: How the 5Ps embed purpose to deliver value
Women in the Workplace 2020
What’s next for remote work: An analysis of 2,000 tasks, 800 jobs, and nine countries
One of the many impacts of the COVID-19 crisis has been to highlight the role of quantitative models in our lives. Ideas associated with modeling, such as flattening the curve of disease transmission, are now regularly discussed in the media and among families and friends. Across the globe, we are trying to understand the numbers and what they mean for us.

Forward-looking models aren’t new. They have long played an important but unseen role in day-to-day life—for instance, in pricing homeowners’ insurance, anticipating the weather, and deciding how many iPhones to manufacture. However, in the COVID-19 pandemic, the scale of impact and the level of uncertainty have introduced new challenges—and notoriety—for modelers.

Used properly, models provide information that can present a framework for understanding a situation. But they aren’t crystal balls that state with certainty what will happen, and they don’t in themselves answer the difficult question of what to do. The eminent British statistician George Box summarized the point with his famous aphorism: “All models are wrong, but some are useful.” And he refined it by saying, “Since all models are wrong, the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned about mice when there are tigers abroad.”

Sidebar
What is a model?


This article explains how models can help us make sense of the world and why they behave the way they do (see sidebar “What is a model?”). We also discuss the most common modeling pitfalls and how to avoid them.

The power of models
Making decisions in the face of uncertainty is challenging, particularly during a pandemic. Quantitative models can help us understand systems and behaviors in a number of useful ways that help navigate this ambiguous environment.

Clarifying which drivers matter
Models structure data in support of reasoned decision making by restricting variables to those that matter for a particular question. For example, when developing a demographic model to help civic leaders plan future community needs, key drivers could be birth rates, death rates, and new-job creation. Models can help users understand what is known about each element and identify the areas of continuing uncertainty.


Determining how much an input can matter
Models are well suited to exposing sensitivities: they show how even small changes in key assumptions can produce large variations in outcomes, helping decision makers establish priorities. An obvious case in point related to the COVID-19 pandemic is the massive impact of even small adjustments in the transmission rate of infection. By establishing sensitivities, models pinpoint areas for investment of effort or money to reduce uncertainty.

Facilitating discussions about the future
Models expose how different assumptions lead to different outcomes. Through discussion of modeling results, decision makers can form a collective judgement on scenarios to plan for, based on the multiple variables considered, and thus reach practical decisions (see sidebar “Building a quantitative model while using it”). For example, models were used to enable policy makers to weigh the benefits of requiring seatbelts against the moral hazard of encouraging people to drive faster. Not only do models trigger discussion, but they may force a more nuanced and evidence-based approach to decision making. In many cases, that is more important than the specific output itself.

Sidebar
Building a quantitative model while using it



Pitfalls to avoid when using models
A model is simply a tool, and, as with any tool, its value highly correlates with the way it is used. Models can be broken down into three main components: raw data, assumptions that define what the model does with the data, and final output. The relative importance of assumptions and data varies by model. Google Search’s autofill, for example, is mostly data driven, while the adage about waiting an hour before swimming is driven by assumptions. Each part must be viewed with a critical lens; failure to do so can lead to poorly informed decisions.

Overlooking the fact that a model can’t fix bad data
A model is only as good as its underlying data, and data in a time of extreme uncertainty, such as a global pandemic, present a serious challenge. Just as rotten ingredients won’t produce a tasty dish no matter how good the recipe is, poor data lead to poor output from a model.

A model is only as good as its underlying data, and data in a time of extreme uncertainty, such as a global pandemic, present a serious challenge.

Data can be wanting for various reasons: too few data points, inconsistency, inaccuracy, or incorrectly generalizing from a particular data set. Modeling anything related to a novel virus entails the risk of using bad data. Virtually all the data series being collected about the COVID-19 crisis are incomplete or subject to caveats. For example, using data on the impacts of the COVID-19 pandemic in one geography to model potential impacts in another community can be problematic. Data might not be generalizable if the populations differ in important dimensions, such as age.

Taking assumptions and simplifications for granted
Assumptions aren’t facts; they should be subject to regular, searching review (see sidebar “The risks of bias in modeling”). For example, prior to the 2008 financial crisis, a key assumption in multiple models was that real-estate prices wouldn’t see major declines. Values had consistently increased in the precrisis years, so some began to take that assumption as a fact, thereby obscuring other possible scenarios.

Sidebar
The risks of bias in modeling


Assumptions aren’t static; they are subject to change as we learn more, especially in novel circumstances. Estimated rates of death from COVID-19 have been constantly revised as our understanding has expanded. Models tell you what might happen if you believe specified things about different variables. Those ifs all need to be revisited frequently if the model is to remain relevant and useful.

Expecting too much certainty
Models aren’t designed to eliminate uncertainty but to limit the range of uncertainties in a given situation by showing what might happen in a variety of defined scenarios. Uncertainty can arise from the very structure of the model, basic assumptions, and ongoing data inputs. For example, hurricane models are an attempt to gain understanding of where hurricanes might make landfall. The models start with significant uncertainties around the path the hurricane might take, and the uncertainties decrease over time as landfall nears.

Sidebar
Modeling philosophy for the COVID-19 pandemic


Usually, models provide guidance on possible futures given multiple inputs (see sidebar “Modeling philosophy for the COVID-19 pandemic”). That makes it dangerous to take a subset of a model’s outputs at a particular point in time as a singular reality. For example, in addition to a popular model for tracking COVID-19-related deaths and hospital demand, the Institute for Health Metrics and Evaluation has released a model that predicts daily infections and testing. For August 2, 2020, it predicts 80,130 infections, which seems very precise (and quotable). However, closer inspection appropriately shows a range of 45,595 to 156,889 infections.1 That is a huge range, but it doesn’t negate the usefulness of the model. It is an important indicator of the level of uncertainty that should be taken into account when making any subsequent decisions.

Sidebar
Read me: Quick hit on COVID-19 models

Ultimately, when using models to make decisions or when interpreting their outputs, there are several key questions to ask: How has this model simplified the world? What inputs does the model require, and how knowable, certain, and stable are those inputs? What are the outputs telling us, and what is the level of uncertainty? And lastly, how have users engaged with this model in the process of making decisions?

Satisfactory answers to these questions will foster a better understanding of potential future scenarios and better decisions in an evolving and uncertain situation. (For a list of suggested reading about the use of models in the current crisis, see sidebar “Read me: Quick hit on COVID-19 models.”)
"""

article_ranges = {'Deloitte': [x for x in range(1, 9)],
                  'McKinsey': [x for x in range(9, 17)]}

all_articles = [article1, article2, article3, article4, article5, article6, article7, article8, article9, article10,
                article11, article12, article13, article14, article15, article16]

for i in range(len(all_articles)):
	#print(i + 1)
	for j in range(i + 1, len(all_articles)):
		if all_articles[i] == all_articles[j]:
			print(f"Article {i + 1} is the same as Article {j + 1}!")
